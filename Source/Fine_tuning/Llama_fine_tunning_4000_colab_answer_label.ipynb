{"cells":[{"cell_type":"markdown","metadata":{"id":"dPW37ftTgz_3"},"source":["# Fine Tunning using unsloth"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25373,"status":"ok","timestamp":1731016891966,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"LyQYIl3BjK86","outputId":"f12959cd-193d-4be1-cfe2-3da2a22a63ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3581,"status":"ok","timestamp":1731016895543,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"gMV7Y_FPgz_5","outputId":"dd06874c-48ba-4e0a-fad2-b45fd16ba220"},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA Available:  True\n","CUDA Device Name:  Tesla T4\n","Usando dispositivo: cuda\n"]}],"source":["import torch\n","print(\"CUDA Available: \", torch.cuda.is_available())\n","print(\"CUDA Device Name: \", torch.cuda.get_device_name(0))\n","torch.cuda.empty_cache()\n","\n","# Verificar se CUDA est√° dispon√≠vel para acelerar o processamento\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f\"Usando dispositivo: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"1kE3erHvgz_5"},"source":["# Define Model"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":57126,"status":"ok","timestamp":1731016952665,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"Wlb23uhChYPC"},"outputs":[],"source":["%%capture\n","!pip install unsloth \"xformers==0.0.28.post2\"\n","# Also get the latest nightly Unsloth!\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47212,"status":"ok","timestamp":1731016999871,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"n-6nL5-3gz_5","outputId":"fc98a0d6-9200-4fa0-bff9-ec451f327986"},"outputs":[{"output_type":"stream","name":"stdout","text":["ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"]}],"source":["from unsloth import FastLanguageModel\n","import torch"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1731016999871,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"W2d6y7Amgz_6"},"outputs":[],"source":["max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281,"referenced_widgets":["70c0af1d8d2b43048b98a7558a196327","5693be8916cb46acb0ff8dcb02972f76","6d50d49b67cb41ab9036fd5e6bbba79c","fedef75fe8d644daa85359f6f547076b","9fa2c10760f741ce9706f43920b46455","80921a69a24d4820902bdc9d1174d30b","29122429dd534ceeb87bfc4f1d82c20c","de0115e5b2f74b219dc50590d1ba2e58","13c5613b5bf445d6b786f7387353c943","781f3988f9b044c4a46bccf715645df4","33baf9c73c05438ca162689161d0e984","a6735d3e0bdb4a8598dcc136afd91bf6","157edc5a3b6145ac8a6a5150402a4fe4","61c5801a7c094afca8cea0b53ce7ed7d","3b56f1186c0142c58894fa2ff003935a","e1a273de001a431ba2af7040d724427d","77e22a9bdffa40bc8cab3b37178527b0","e13c8fc0699a45ee961875985ef951dc","c65b12ea656f4feebf29350f525221c2","57e4cb7047f5401c8ca5b283afae4140","212522fb975143faa36fedbdb8507c37","b87314e38a1443d89925d3e2e3a2393b","566ac0227a914449980c9ebc8dda5853","70b1d46452d94c88a2704cf066593cc0","0a6edf146e6440559bcb7bb41408fa12","51abf5b57ce04275ae12f938f68afbed","2ba17ca986424d548abada440e800257","cde443acb343413faff3b6995c71ac41","6f582bc0a6384593aea63880bc72e487","b0fae63651c345d5876423a05f97f591","35075f81768645a4bce2bb5747daea70","ed8f46d017184c7aaf6fad01291ecba9","6376357f99894f2086cf43984bcc5b66","68f924b54a964a3290d27a7b58e5a9ac","434f3a37ea3f46ab98bb3c9b5c20d099","0425bacbba834fca831ffb0775a31518","c2e8e75bc55c4a7597fda906ec564f89","38003c547f60452e8da46f33a036d805","0f92821a826f42e0be1b2aeb18447253","9db75b28b6b64c3ca4acf3e1036ead05","d534bc062c2c4adeb01b7e4f55d3b83c","1df91a71450542dba3807b74400f1c75","224e593273fd4de5901fd71f7288e189","ecf4cc7c30a3486e82a543df62c98e8e","7bee969b3eaa4609b90a9f7289e4d053","476fda16ae734861ab6ca3a1d7a4e43a","83d70ad13fc24bb1981e584ec637df4f","320c3a68d6d34bd4b54705d07b88a05b","31840c13880548e49796119958c0ddc6","bd3bdc26d2e5421db8f1112d873860e6","f25d2b0422b34e00a38fff8d5510b436","418dececeea54f019e5d42707c65b59a","da473b6a9330463daebc6aaa43e47536","830020de55754b088db319d324cf1ee7","a200b3a0f6284e5cbe2bda2fbc5744f5"]},"executionInfo":{"elapsed":112055,"status":"ok","timestamp":1731017111921,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"dm7i8rlRgz_7","outputId":"7ed94d26-4a1e-407e-c019-adfb6a0c8956"},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2024.11.5: Fast Llama patching. Transformers = 4.46.2.\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.5.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70c0af1d8d2b43048b98a7558a196327"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6735d3e0bdb4a8598dcc136afd91bf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"566ac0227a914449980c9ebc8dda5853"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f924b54a964a3290d27a7b58e5a9ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bee969b3eaa4609b90a9f7289e4d053"}},"metadata":{}}],"source":["initial_model, tokenizer = FastLanguageModel.from_pretrained(\n","    # model_name=\"unsloth/Llama-3.2-3B-bnb-4bit\",\n","    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    # device_map=\"auto\"\n","    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",")\n","\n","model = initial_model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1731015987425,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"QG4-5m8Kgz_7","outputId":"6dcd1bac-fa8c-48e0-b473-119980c3fa07"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n","    (layers): ModuleList(\n","      (0-27): 28 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n","          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n","          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n","          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n","          (rotary_emb): LlamaExtendedRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n","          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n","          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n","        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",")"]},"metadata":{},"execution_count":7}],"source":["initial_model"]},{"cell_type":"markdown","metadata":{"id":"sq0pqL8Xgz_8"},"source":["# Dataset TeleQnA"]},{"cell_type":"markdown","metadata":{"id":"TskNUPaAgz_8"},"source":["## Release 17 Questions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBAExUUCgz_9","outputId":"c51f4da1-41a8-4c56-e5d5-8eaafe26a2b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["733\n"]}],"source":["import json\n","\n","# Path to the TeleQnA processed question in JSON file\n","rel17_questions_path = r\"../Files/rel17_questions.json\"\n","\n","# Load the TeleQnA data just release 17\n","with open(rel17_questions_path, \"r\", encoding=\"utf-8\") as file:\n","    rel17_questions = json.load(file)\n","print(len(rel17_questions))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1730563095640,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"cmI6A8IZgz_-","outputId":"312d8f6e-631b-4dcb-e2a3-da0948f1b69e"},"outputs":[{"name":"stdout","output_type":"stream","text":["200\n"]}],"source":["# Path to the TeleQnA processed question in JSON file\n","rel17_200_questions_path = r\"../Files/rel17_200_questions.json\"\n","\n","# Load the TeleQnA data just release 17\n","with open(rel17_200_questions_path, \"r\", encoding=\"utf-8\") as file:\n","    rel17_200_questions = json.load(file)\n","print(len(rel17_200_questions))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":23,"status":"aborted","timestamp":1730563095640,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"pZ8pQsdSgz__","outputId":"5f53b589-6663-40c4-8e29-d68bb0104e7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["533\n"]}],"source":["rel17_other_questions = [q for q in rel17_questions if q not in rel17_200_questions]\n","print(len(rel17_other_questions))\n","\n","rel17_other_questions_length = 500\n","\n","rel17_other_questions = rel17_other_questions[:rel17_other_questions_length]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1730563095640,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"t2dq4yB8gz__","outputId":"231b9007-788b-4b73-c6ef-49091b39ca3d"},"outputs":[{"data":{"text/plain":["{'question': 'In which bearer option is the S1-U connection terminated in the MeNB? [3GPP Release 17]',\n"," 'option 1': 'Split bearer',\n"," 'option 2': 'SCG bearer',\n"," 'option 3': 'MCG bearer',\n"," 'option 4': 'Primary bearer',\n"," 'answer': 'option 3: MCG bearer',\n"," 'explanation': 'In the MCG bearer option, the S1-U connection for the corresponding bearer(s) is terminated in the MeNB.',\n"," 'category': 'Standards specifications'}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["rel17_other_questions[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0mJ7nlfk8Fy"},"outputs":[],"source":["import re\n","\n","def extract_option(text):\n","    # Find all occurrences of 'option X:' followed by text, where X can be any number\n","    option_matches = re.findall(r'option\\s*\\d+:\\s*(.*)', text, re.IGNORECASE | re.DOTALL)\n","\n","    # Return the text after the last 'option X:' found\n","    return option_matches[-1].strip() if option_matches else None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sc-n7Ztdk8Fy","outputId":"25f9b24f-f028-4f86-9049-df9020d8c6d1"},"outputs":[{"data":{"text/plain":["'MCG bearer'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["extract_option(rel17_other_questions[0]['answer'])"]},{"cell_type":"markdown","metadata":{"id":"FPMq3y2Cg0AA"},"source":["## Questions without rel 17 and 18"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":23,"status":"aborted","timestamp":1730563095641,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"8cP7axaLg0AA","outputId":"79a5489d-73ff-4d77-bbaa-a16f333d40a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["8487\n"]}],"source":["# Path to the TeleQnA processed question in JSON file\n","questions_no_rel_17_18_path = r\"../Files/questions_no_rel_17_18.json\"\n","\n","# Load the TeleQnA data just release 17\n","with open(questions_no_rel_17_18_path, \"r\", encoding=\"utf-8\") as file:\n","    questions_no_rel_17_18 = json.load(file)\n","print(len(questions_no_rel_17_18))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1730563095641,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"YYqnllXlg0AB","outputId":"eb18018a-0605-4b5b-86bd-37aaa7a21637"},"outputs":[{"name":"stdout","output_type":"stream","text":["3500\n"]}],"source":["questions_no_rel_17_18_length = 3500\n","questions_no_rel_17_18 = questions_no_rel_17_18[:questions_no_rel_17_18_length]\n","print(len(questions_no_rel_17_18))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1730563095641,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"pUrgTVcjg0AB","outputId":"a6e72293-0210-4fd3-e313-d3ce256a2864"},"outputs":[{"data":{"text/plain":["{'question': 'Which non-orthogonal multiple access scheme utilizes the low-complexity message passing algorithm at the receiver for user data detection?',\n"," 'option 1': 'NOMA',\n"," 'option 2': 'PDMA',\n"," 'option 3': 'MUSA',\n"," 'option 4': 'MUST',\n"," 'option 5': 'SCMA',\n"," 'answer': 'option 5: SCMA',\n"," 'explanation': 'The SCMA scheme utilizes the low-complexity message passing algorithm at the receiver for user data detection.',\n"," 'category': 'Research publications'}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["questions_no_rel_17_18[0]"]},{"cell_type":"markdown","metadata":{"id":"PVe8hVA0g0AC"},"source":["## Train Data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21,"status":"aborted","timestamp":1730563095641,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"IOZ1e4Obg0AC","outputId":"ef1624eb-2206-4f30-e3f0-cc61c2c823c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["4000\n"]}],"source":["# train_questions = rel17_other_questions\n","train_questions = rel17_other_questions + questions_no_rel_17_18\n","print(len(train_questions))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1730563095642,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"OMBg8cmLg0AC","outputId":"3fc5998d-ca1f-4b80-93ec-9ed397147673"},"outputs":[{"data":{"text/plain":["{'question': 'In which bearer option is the S1-U connection terminated in the MeNB? [3GPP Release 17]',\n"," 'option 1': 'Split bearer',\n"," 'option 2': 'SCG bearer',\n"," 'option 3': 'MCG bearer',\n"," 'option 4': 'Primary bearer',\n"," 'answer': 'option 3: MCG bearer',\n"," 'explanation': 'In the MCG bearer option, the S1-U connection for the corresponding bearer(s) is terminated in the MeNB.',\n"," 'category': 'Standards specifications'}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["train_questions[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1730563095642,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"gwWYCuOmg0AC","outputId":"67ac048f-e82d-49cb-8c55-28472ceacf2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'from': 'human', 'value': 'In which bearer option is the S1-U connection terminated in the MeNB? [3GPP Release 17]'}, {'from': 'gpt', 'value': 'MCG bearer'}]\n"]}],"source":["from datasets import Dataset\n","\n","# Structure to store pairs of questions and explanations\n","data = []\n","\n","half_questions = len(train_questions)//2\n","\n","# Fill the dataset with (question, explanation) pairs\n","for item in train_questions[:half_questions]:\n","\n","    human_value = (\n","        f\"{item['question']}\"\n","    )\n","\n","    # Combine the answer and explanation\n","    gpt_value = (\n","        f\"{extract_option(item['answer'])}\"\n","    )\n","\n","    # Create a dictionary for each input pair\n","    pair = [\n","        {'from': 'human', 'value': human_value},  # For the question\n","        {'from': 'gpt', 'value': gpt_value}  # For the explanation\n","    ]\n","\n","    data.append(pair)  # Add the pair to the dataset\n","\n","data_no_options = data\n","print(data_no_options[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21,"status":"aborted","timestamp":1730563095642,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"B_KQKnq_g0AC","outputId":"574c94a5-a870-41e3-e814-bb42185e00cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'from': 'human', 'value': 'Question: What are distributed representations?\\nOptions:\\noption 1: Representations where single or subsets of components encode individual data objects\\noption 2: Representations where the encoded information is distributed across all components of a hypervector\\noption 3: Representations sensitive to the structure of the encoded objects\\noption 4: Representations that are robust to local noise\\noption 5: Representations that are productive and systematic\\n'}, {'from': 'gpt', 'value': 'Answer: option 2: Representations where the encoded information is distributed across all components of a hypervector\\nExplanation: Distributed representations are representations where the encoded information is distributed across all components of a hypervector.'}]\n"]}],"source":["from datasets import Dataset\n","\n","# Structure to store pairs of questions, options, answers, and explanations\n","data = []\n","\n","# Fill the dataset with (question + options, answer + explanation) pairs\n","for item in train_questions[half_questions:]:\n","\n","    # Extract options\n","    options = [f\"{key}: {value}\" for key, value in item.items() if 'option' in key]\n","    # Combine the question and options\n","    human_value = (\n","        f\"Question: {item['question']}\\n\"\n","        f\"Options:\\n\" + \"\\n\".join(options) + \"\\n\"\n","    )\n","\n","    # Combine the answer and explanation\n","    gpt_value = (\n","        f\"Answer: {item['answer']}\\n\"\n","        f\"Explanation: {item['explanation']}\"\n","    )\n","\n","    # Create a dictionary for each input pair\n","    pair = [\n","        {'from': 'human', 'value': human_value},  # Question with options\n","        {'from': 'gpt', 'value': gpt_value}       # Answer with explanation\n","    ]\n","\n","    data.append(pair)  # Add the pair to the dataset\n","\n","# Create the dataset using Hugging Face\n","data_options = data\n","print(data_options[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1730563095643,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"eyjIeHbYg0AD","outputId":"c8a99fc3-8b71-49a5-de59-e1d294985810"},"outputs":[{"name":"stdout","output_type":"stream","text":["4000\n"]}],"source":["import random\n","\n","data_total = data_no_options + data_options\n","# Shuffle the combined data\n","random.shuffle(data_total)\n","print(len(data_total))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1730563095643,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"MuBYdPIcg0AD","outputId":"becc7fd5-a395-48d7-ee88-23f3d483e91c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['conversations'],\n","    num_rows: 4000\n","})\n","{'conversations': [{'from': 'human', 'value': 'Question: What is the main characteristic of a Reconfigurable Intelligent Surface (RIS)?\\nOptions:\\noption 1: It is a fully passive device\\noption 2: It requires high power consumption\\noption 3: It amplifies and introduces noise when reflecting/transmitting/reshaping signals\\noption 4: All of the above\\noption 5: None of the above\\n'}, {'from': 'gpt', 'value': 'Answer: option 1: It is a fully passive device\\nExplanation: The main characteristic of a Reconfigurable Intelligent Surface (RIS) is that it is a fully passive device, meaning it does not require any amplification stage and does not amplify or introduce noise when reflecting, transmitting, or reshaping signals.'}]}\n"]}],"source":["# Create the dataset using Hugging Face\n","# Convert the list of pairs into the appropriate format\n","formatted_data = {'conversations': data_total}\n","\n","# Transform the data into a Dataset\n","dataset = Dataset.from_dict(formatted_data)\n","\n","# Display information about the created dataset\n","print(dataset)\n","\n","# Display the first entry of the dataset\n","print(dataset[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":23,"status":"aborted","timestamp":1730563095644,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"9uX5NJUcg0AD","colab":{"referenced_widgets":["48f317e98c2048ad80c30ca9bd480cee"]},"outputId":"37a4c0f5-6617-46f4-f4ce-82a5d3bb5dc4"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48f317e98c2048ad80c30ca9bd480cee","version_major":2,"version_minor":0},"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/4000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset.save_to_disk('../Files/train_questions_dataset_4000_questions_answer_label')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3676,"status":"ok","timestamp":1731017116523,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"Jj_LdZJUg0AE","outputId":"c21eb577-0c72-47a8-a3f9-2e18d7b48c6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["4000\n"]}],"source":["from datasets import load_from_disk\n","\n","dataset_path = '/content/drive/MyDrive/Mestrado/Aula_IA024/Projeto_final/Files/train_questions_dataset_4000_questions_answer_label'\n","\n","dataset = load_from_disk(dataset_path)\n","\n","print(len(dataset))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1731017116523,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"qCNe5_fcg0AE","outputId":"d62e2090-e93f-4351-a7af-1601dc967534"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'conversations': [{'from': 'human',\n","   'value': 'Question: What is the main characteristic of a Reconfigurable Intelligent Surface (RIS)?\\nOptions:\\noption 1: It is a fully passive device\\noption 2: It requires high power consumption\\noption 3: It amplifies and introduces noise when reflecting/transmitting/reshaping signals\\noption 4: All of the above\\noption 5: None of the above\\n'},\n","  {'from': 'gpt',\n","   'value': 'Answer: option 1: It is a fully passive device\\nExplanation: The main characteristic of a Reconfigurable Intelligent Surface (RIS) is that it is a fully passive device, meaning it does not require any amplification stage and does not amplify or introduce noise when reflecting, transmitting, or reshaping signals.'}]}"]},"metadata":{},"execution_count":8}],"source":["dataset[0]"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1731017116523,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"YFi1oCHlg0AE","outputId":"6a2fdd86-a185-4dcb-ea8c-14fa5f1f716f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'conversations': [{'from': 'human',\n","   'value': 'Question: What is the main objective of dictionary learning in graph signal processing?\\nOptions:\\noption 1: To design filters for graph representation\\noption 2: To design basis functions for graph signals\\noption 3: To develop techniques for graph learning\\noption 4: To reconstruct an estimated signal on the whole graph based on observed vertex measurements\\noption 5: To design dictionaries of atoms that sparsely represent graph signals\\n'},\n","  {'from': 'gpt',\n","   'value': 'Answer: option 5: To design dictionaries of atoms that sparsely represent graph signals\\nExplanation: The main objective of dictionary learning in graph signal processing is to design dictionaries of atoms that sparsely represent graph signals and incorporate the structure of the graph.'}]}"]},"metadata":{},"execution_count":9}],"source":["dataset[5]"]},{"cell_type":"markdown","metadata":{"id":"y1f9SM76g0AE"},"source":["# Build TeleQnA Dataset for training"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1731017116523,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"oroS6UEpg0AF"},"outputs":[],"source":["from unsloth.chat_templates import get_chat_template\n","\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template = \"llama-3.1\",\n",")\n","\n","def formatting_prompts_func(examples):\n","    convos = examples[\"conversations\"]\n","    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n","    return { \"text\" : texts, }\n","pass"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2249,"status":"ok","timestamp":1731017118768,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"SZn1oGR7g0AF"},"outputs":[],"source":["from unsloth.chat_templates import standardize_sharegpt\n","dataset = standardize_sharegpt(dataset)\n","dataset = dataset.map(formatting_prompts_func, batched = True,)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1731017118769,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"MRXD13qsg0AF","outputId":"68a62c5c-d9be-463a-b4e6-c55884ffd71c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['conversations', 'text'],\n","    num_rows: 4000\n","})"]},"metadata":{},"execution_count":12}],"source":["dataset"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1731017118769,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"eFV5iiCrg0AF","outputId":"4ad03b0b-8304-4c3b-c87d-2585cdef3f6e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQuestion: What is the main characteristic of a Reconfigurable Intelligent Surface (RIS)?\\nOptions:\\noption 1: It is a fully passive device\\noption 2: It requires high power consumption\\noption 3: It amplifies and introduces noise when reflecting/transmitting/reshaping signals\\noption 4: All of the above\\noption 5: None of the above\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAnswer: option 1: It is a fully passive device\\nExplanation: The main characteristic of a Reconfigurable Intelligent Surface (RIS) is that it is a fully passive device, meaning it does not require any amplification stage and does not amplify or introduce noise when reflecting, transmitting, or reshaping signals.<|eot_id|>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["dataset[0]['text']"]},{"cell_type":"markdown","metadata":{"id":"cINqstKzg0AG"},"source":["# Training"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7388,"status":"ok","timestamp":1731017126151,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"cvo69JQCg0AG","outputId":"8bab27c9-d2bc-472b-bc1f-f98b614244d3"},"outputs":[{"output_type":"stream","name":"stderr","text":["Unsloth 2024.11.5 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"]}],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUgKcVLeg0AH"},"outputs":[],"source":["# del model"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1595,"status":"ok","timestamp":1731017127743,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"lFb80Ns1g0AH","outputId":"a4a10fdf-a2b5-44b3-d747-7bc883c89e97"},"outputs":[{"output_type":"stream","name":"stderr","text":["max_steps is given, it will override any value given in num_train_epochs\n"]}],"source":["from trl import SFTTrainer\n","from transformers import TrainingArguments, DataCollatorForSeq2Seq\n","from unsloth import is_bfloat16_supported\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 16,\n","        warmup_steps = 5,\n","        # num_train_epochs = 1, # Set this for 1 full training run.\n","        max_steps = 400,\n","        learning_rate = 2e-4,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        report_to = \"none\", # Use this for WandB etc\n","    ),\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":1908,"status":"ok","timestamp":1731017129646,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"eDM5sIAzg0AH"},"outputs":[],"source":["from unsloth.chat_templates import train_on_responses_only\n","trainer = train_on_responses_only(\n","    trainer,\n","    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n","    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1731017129647,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"HiBTertbg0AH","outputId":"5480d150-b517-4e0c-b4b2-1f0124c52e02"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQuestion: What is the main characteristic of a Reconfigurable Intelligent Surface (RIS)?\\nOptions:\\noption 1: It is a fully passive device\\noption 2: It requires high power consumption\\noption 3: It amplifies and introduces noise when reflecting/transmitting/reshaping signals\\noption 4: All of the above\\noption 5: None of the above\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAnswer: option 1: It is a fully passive device\\nExplanation: The main characteristic of a Reconfigurable Intelligent Surface (RIS) is that it is a fully passive device, meaning it does not require any amplification stage and does not amplify or introduce noise when reflecting, transmitting, or reshaping signals.<|eot_id|>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1731017129647,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"lTwFrw-Ig0AI","outputId":"114961b0-97d5-4e63-bd83-2a9144407581"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'                                                                                                                 \\n\\nAnswer: option 1: It is a fully passive device\\nExplanation: The main characteristic of a Reconfigurable Intelligent Surface (RIS) is that it is a fully passive device, meaning it does not require any amplification stage and does not amplify or introduce noise when reflecting, transmitting, or reshaping signals.<|eot_id|>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}],"source":["space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n","tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]])"]},{"cell_type":"markdown","metadata":{"id":"icmrJOz5g0AI"},"source":["## Show current memory stats\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1731017129647,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"bxyuFibzg0AI","outputId":"eeefd9b7-3641-47f6-ed38-1b03a42d9b68"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU = Tesla T4. Max memory = 14.748 GB.\n","6.688 GB of memory reserved.\n"]}],"source":["#@title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")"]},{"cell_type":"markdown","metadata":{"id":"i2L3CVLgg0AI"},"source":["## Initial Loss"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1731017129647,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"sM0P3OkFg0AJ","outputId":"a31e73f1-9612-43a5-d551-12629bb5a5c8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_items([('input_ids', [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 5887, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 14924, 25, 3639, 374, 279, 1925, 29683, 315, 264, 1050, 1710, 18835, 78188, 28061, 320, 49, 1669, 87527, 3883, 512, 2091, 220, 16, 25, 1102, 374, 264, 7373, 28979, 3756, 198, 2091, 220, 17, 25, 1102, 7612, 1579, 2410, 15652, 198, 2091, 220, 18, 25, 1102, 23201, 9803, 323, 40019, 12248, 994, 42852, 81221, 1800, 1303, 14, 2548, 14550, 17738, 198, 2091, 220, 19, 25, 2052, 315, 279, 3485, 198, 2091, 220, 20, 25, 2290, 315, 279, 3485, 198, 128009, 128006, 78191, 128007, 271, 16533, 25, 3072, 220, 16, 25, 1102, 374, 264, 7373, 28979, 3756, 198, 70869, 25, 578, 1925, 29683, 315, 264, 1050, 1710, 18835, 78188, 28061, 320, 49, 1669, 8, 374, 430, 433, 374, 264, 7373, 28979, 3756, 11, 7438, 433, 1587, 539, 1397, 904, 23201, 2461, 6566, 323, 1587, 539, 97168, 477, 19678, 12248, 994, 42852, 11, 78768, 11, 477, 64793, 14550, 17738, 13, 128009]), ('attention_mask', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('labels', [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 271, 16533, 25, 3072, 220, 16, 25, 1102, 374, 264, 7373, 28979, 3756, 198, 70869, 25, 578, 1925, 29683, 315, 264, 1050, 1710, 18835, 78188, 28061, 320, 49, 1669, 8, 374, 430, 433, 374, 264, 7373, 28979, 3756, 11, 7438, 433, 1587, 539, 1397, 904, 23201, 2461, 6566, 323, 1587, 539, 97168, 477, 19678, 12248, 994, 42852, 11, 78768, 11, 477, 64793, 14550, 17738, 13, 128009])])"]},"metadata":{},"execution_count":20}],"source":["trainer.train_dataset[0].items()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":289104,"status":"ok","timestamp":1731016461619,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"vd5LQhYzg0AK","outputId":"d6b2517d-e358-4263-e290-04788d881cce"},"outputs":[{"output_type":"stream","name":"stderr","text":["Calculating initial loss: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [04:48<00:00,  6.92it/s]"]},{"output_type":"stream","name":"stdout","text":["Initial mean loss: 2.4320212636888026\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import torch\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","# Configura o collator e DataLoader\n","collator = DataCollatorForSeq2Seq(tokenizer)\n","loader = DataLoader(trainer.train_dataset,\n","                    batch_size=2,  # Tamanho do batch escolhido\n","                    collate_fn=collator,\n","                    num_workers=2)\n","\n","# Vari√°veis para armazenar a loss total e o n√∫mero de exemplos\n","total_loss = 0.0\n","num_batches = 0\n","\n","# Coloca o modelo em modo de avalia√ß√£o\n","model.eval()\n","\n","# Desativa o c√°lculo de gradiente para economizar mem√≥ria\n","with torch.no_grad():\n","    for batch in tqdm(loader, desc=\"Calculating initial loss\"):\n","        # Move o batch para a GPU (se dispon√≠vel)\n","        batch = {k: v.to(model.device) for k, v in batch.items()}\n","\n","        # Forward pass\n","        outputs = model(**batch)\n","\n","        # Acumula a loss\n","        total_loss += outputs.loss.item()\n","        num_batches += 1\n","\n","# Calcula a loss m√©dia\n","average_loss = total_loss / num_batches\n","print(f\"Initial mean loss: {average_loss}\")\n"]},{"cell_type":"markdown","metadata":{"id":"Wa0a6KCTg0AK"},"source":["## Training Session"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"qYWa1VUWg0AK","outputId":"f8d79748-9fd5-49dd-f97b-58a97b6fc8ff","executionInfo":{"status":"ok","timestamp":1731019801657,"user_tz":180,"elapsed":2672021,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 4,000 | Num Epochs = 4\n","O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 16\n","\\        /    Total batch size = 32 | Total steps = 400\n"," \"-____-\"     Number of trainable parameters = 24,313,856\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [400/400 44:08, Epoch 3/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.677400</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.482200</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.922100</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.545400</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.162500</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.979100</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.971600</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.087700</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.904200</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.117500</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.950200</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>1.068500</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.820000</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.816300</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.984600</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.903700</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.986600</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.878700</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>1.376600</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.717800</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.909200</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.906900</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.698800</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.811700</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.966900</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>1.063900</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.859900</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.749800</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.982200</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.942200</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.872700</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.848300</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>1.037000</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.830600</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.722500</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.761200</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.723700</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.745100</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.723300</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.837200</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.905400</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.816300</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.635400</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.754700</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.832700</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.934000</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.687100</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.999500</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.791000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.942200</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>1.017100</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.685300</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.596400</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>1.086500</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.655100</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.941400</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.714900</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.694300</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.688800</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.688100</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.634600</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.703700</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.439200</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.983100</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.519100</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.753200</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.803900</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.865700</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.830700</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.821200</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>1.010200</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.627500</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.692200</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>1.088800</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.716400</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.799600</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.850500</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>1.164600</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.997900</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.628400</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.574300</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.716000</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>1.108200</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.594100</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.708600</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.844600</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.581800</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.805900</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.692100</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.940900</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.776200</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>1.078300</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.714900</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.824300</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.682600</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.770200</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.636800</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.947900</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.695100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.675500</td>\n","    </tr>\n","    <tr>\n","      <td>101</td>\n","      <td>0.673700</td>\n","    </tr>\n","    <tr>\n","      <td>102</td>\n","      <td>0.772500</td>\n","    </tr>\n","    <tr>\n","      <td>103</td>\n","      <td>0.635300</td>\n","    </tr>\n","    <tr>\n","      <td>104</td>\n","      <td>0.682600</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>1.017600</td>\n","    </tr>\n","    <tr>\n","      <td>106</td>\n","      <td>0.981600</td>\n","    </tr>\n","    <tr>\n","      <td>107</td>\n","      <td>1.035400</td>\n","    </tr>\n","    <tr>\n","      <td>108</td>\n","      <td>1.052500</td>\n","    </tr>\n","    <tr>\n","      <td>109</td>\n","      <td>0.649600</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.854400</td>\n","    </tr>\n","    <tr>\n","      <td>111</td>\n","      <td>0.890400</td>\n","    </tr>\n","    <tr>\n","      <td>112</td>\n","      <td>0.751000</td>\n","    </tr>\n","    <tr>\n","      <td>113</td>\n","      <td>0.520400</td>\n","    </tr>\n","    <tr>\n","      <td>114</td>\n","      <td>0.531600</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>0.551400</td>\n","    </tr>\n","    <tr>\n","      <td>116</td>\n","      <td>0.669900</td>\n","    </tr>\n","    <tr>\n","      <td>117</td>\n","      <td>0.831800</td>\n","    </tr>\n","    <tr>\n","      <td>118</td>\n","      <td>0.697500</td>\n","    </tr>\n","    <tr>\n","      <td>119</td>\n","      <td>0.858800</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>1.067200</td>\n","    </tr>\n","    <tr>\n","      <td>121</td>\n","      <td>0.776200</td>\n","    </tr>\n","    <tr>\n","      <td>122</td>\n","      <td>0.684300</td>\n","    </tr>\n","    <tr>\n","      <td>123</td>\n","      <td>0.803300</td>\n","    </tr>\n","    <tr>\n","      <td>124</td>\n","      <td>0.677500</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.666400</td>\n","    </tr>\n","    <tr>\n","      <td>126</td>\n","      <td>0.850800</td>\n","    </tr>\n","    <tr>\n","      <td>127</td>\n","      <td>0.690500</td>\n","    </tr>\n","    <tr>\n","      <td>128</td>\n","      <td>0.545300</td>\n","    </tr>\n","    <tr>\n","      <td>129</td>\n","      <td>0.563700</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.618100</td>\n","    </tr>\n","    <tr>\n","      <td>131</td>\n","      <td>0.752900</td>\n","    </tr>\n","    <tr>\n","      <td>132</td>\n","      <td>0.712700</td>\n","    </tr>\n","    <tr>\n","      <td>133</td>\n","      <td>0.841700</td>\n","    </tr>\n","    <tr>\n","      <td>134</td>\n","      <td>0.628500</td>\n","    </tr>\n","    <tr>\n","      <td>135</td>\n","      <td>0.737000</td>\n","    </tr>\n","    <tr>\n","      <td>136</td>\n","      <td>0.840400</td>\n","    </tr>\n","    <tr>\n","      <td>137</td>\n","      <td>0.538000</td>\n","    </tr>\n","    <tr>\n","      <td>138</td>\n","      <td>0.807300</td>\n","    </tr>\n","    <tr>\n","      <td>139</td>\n","      <td>0.837000</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.975700</td>\n","    </tr>\n","    <tr>\n","      <td>141</td>\n","      <td>0.658900</td>\n","    </tr>\n","    <tr>\n","      <td>142</td>\n","      <td>1.061800</td>\n","    </tr>\n","    <tr>\n","      <td>143</td>\n","      <td>0.669400</td>\n","    </tr>\n","    <tr>\n","      <td>144</td>\n","      <td>0.528800</td>\n","    </tr>\n","    <tr>\n","      <td>145</td>\n","      <td>0.820200</td>\n","    </tr>\n","    <tr>\n","      <td>146</td>\n","      <td>0.731500</td>\n","    </tr>\n","    <tr>\n","      <td>147</td>\n","      <td>0.648600</td>\n","    </tr>\n","    <tr>\n","      <td>148</td>\n","      <td>0.409300</td>\n","    </tr>\n","    <tr>\n","      <td>149</td>\n","      <td>0.797000</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.812700</td>\n","    </tr>\n","    <tr>\n","      <td>151</td>\n","      <td>0.530600</td>\n","    </tr>\n","    <tr>\n","      <td>152</td>\n","      <td>0.605400</td>\n","    </tr>\n","    <tr>\n","      <td>153</td>\n","      <td>0.434500</td>\n","    </tr>\n","    <tr>\n","      <td>154</td>\n","      <td>0.782100</td>\n","    </tr>\n","    <tr>\n","      <td>155</td>\n","      <td>0.840300</td>\n","    </tr>\n","    <tr>\n","      <td>156</td>\n","      <td>0.740800</td>\n","    </tr>\n","    <tr>\n","      <td>157</td>\n","      <td>0.760900</td>\n","    </tr>\n","    <tr>\n","      <td>158</td>\n","      <td>0.525600</td>\n","    </tr>\n","    <tr>\n","      <td>159</td>\n","      <td>0.613900</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.562800</td>\n","    </tr>\n","    <tr>\n","      <td>161</td>\n","      <td>0.790500</td>\n","    </tr>\n","    <tr>\n","      <td>162</td>\n","      <td>0.494400</td>\n","    </tr>\n","    <tr>\n","      <td>163</td>\n","      <td>0.664900</td>\n","    </tr>\n","    <tr>\n","      <td>164</td>\n","      <td>0.676400</td>\n","    </tr>\n","    <tr>\n","      <td>165</td>\n","      <td>0.814400</td>\n","    </tr>\n","    <tr>\n","      <td>166</td>\n","      <td>0.654500</td>\n","    </tr>\n","    <tr>\n","      <td>167</td>\n","      <td>0.754800</td>\n","    </tr>\n","    <tr>\n","      <td>168</td>\n","      <td>0.561600</td>\n","    </tr>\n","    <tr>\n","      <td>169</td>\n","      <td>0.859000</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.734800</td>\n","    </tr>\n","    <tr>\n","      <td>171</td>\n","      <td>0.571300</td>\n","    </tr>\n","    <tr>\n","      <td>172</td>\n","      <td>0.979000</td>\n","    </tr>\n","    <tr>\n","      <td>173</td>\n","      <td>0.740300</td>\n","    </tr>\n","    <tr>\n","      <td>174</td>\n","      <td>0.809700</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.549600</td>\n","    </tr>\n","    <tr>\n","      <td>176</td>\n","      <td>0.769400</td>\n","    </tr>\n","    <tr>\n","      <td>177</td>\n","      <td>0.472100</td>\n","    </tr>\n","    <tr>\n","      <td>178</td>\n","      <td>0.707800</td>\n","    </tr>\n","    <tr>\n","      <td>179</td>\n","      <td>0.606300</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.887100</td>\n","    </tr>\n","    <tr>\n","      <td>181</td>\n","      <td>0.551800</td>\n","    </tr>\n","    <tr>\n","      <td>182</td>\n","      <td>0.752000</td>\n","    </tr>\n","    <tr>\n","      <td>183</td>\n","      <td>0.541500</td>\n","    </tr>\n","    <tr>\n","      <td>184</td>\n","      <td>0.586800</td>\n","    </tr>\n","    <tr>\n","      <td>185</td>\n","      <td>0.914200</td>\n","    </tr>\n","    <tr>\n","      <td>186</td>\n","      <td>0.509800</td>\n","    </tr>\n","    <tr>\n","      <td>187</td>\n","      <td>0.643600</td>\n","    </tr>\n","    <tr>\n","      <td>188</td>\n","      <td>0.530400</td>\n","    </tr>\n","    <tr>\n","      <td>189</td>\n","      <td>0.458200</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.685200</td>\n","    </tr>\n","    <tr>\n","      <td>191</td>\n","      <td>0.703300</td>\n","    </tr>\n","    <tr>\n","      <td>192</td>\n","      <td>1.111900</td>\n","    </tr>\n","    <tr>\n","      <td>193</td>\n","      <td>0.987900</td>\n","    </tr>\n","    <tr>\n","      <td>194</td>\n","      <td>0.627100</td>\n","    </tr>\n","    <tr>\n","      <td>195</td>\n","      <td>0.573100</td>\n","    </tr>\n","    <tr>\n","      <td>196</td>\n","      <td>0.690300</td>\n","    </tr>\n","    <tr>\n","      <td>197</td>\n","      <td>0.578600</td>\n","    </tr>\n","    <tr>\n","      <td>198</td>\n","      <td>0.366500</td>\n","    </tr>\n","    <tr>\n","      <td>199</td>\n","      <td>0.953600</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.607700</td>\n","    </tr>\n","    <tr>\n","      <td>201</td>\n","      <td>0.470600</td>\n","    </tr>\n","    <tr>\n","      <td>202</td>\n","      <td>0.690700</td>\n","    </tr>\n","    <tr>\n","      <td>203</td>\n","      <td>0.670800</td>\n","    </tr>\n","    <tr>\n","      <td>204</td>\n","      <td>0.641900</td>\n","    </tr>\n","    <tr>\n","      <td>205</td>\n","      <td>0.658000</td>\n","    </tr>\n","    <tr>\n","      <td>206</td>\n","      <td>0.676400</td>\n","    </tr>\n","    <tr>\n","      <td>207</td>\n","      <td>0.729700</td>\n","    </tr>\n","    <tr>\n","      <td>208</td>\n","      <td>0.599300</td>\n","    </tr>\n","    <tr>\n","      <td>209</td>\n","      <td>0.609400</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.890900</td>\n","    </tr>\n","    <tr>\n","      <td>211</td>\n","      <td>0.821800</td>\n","    </tr>\n","    <tr>\n","      <td>212</td>\n","      <td>0.671600</td>\n","    </tr>\n","    <tr>\n","      <td>213</td>\n","      <td>0.884800</td>\n","    </tr>\n","    <tr>\n","      <td>214</td>\n","      <td>0.753500</td>\n","    </tr>\n","    <tr>\n","      <td>215</td>\n","      <td>0.912500</td>\n","    </tr>\n","    <tr>\n","      <td>216</td>\n","      <td>0.699800</td>\n","    </tr>\n","    <tr>\n","      <td>217</td>\n","      <td>0.863000</td>\n","    </tr>\n","    <tr>\n","      <td>218</td>\n","      <td>0.636500</td>\n","    </tr>\n","    <tr>\n","      <td>219</td>\n","      <td>0.558700</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.830200</td>\n","    </tr>\n","    <tr>\n","      <td>221</td>\n","      <td>0.670900</td>\n","    </tr>\n","    <tr>\n","      <td>222</td>\n","      <td>0.496300</td>\n","    </tr>\n","    <tr>\n","      <td>223</td>\n","      <td>0.809100</td>\n","    </tr>\n","    <tr>\n","      <td>224</td>\n","      <td>0.629300</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>0.508300</td>\n","    </tr>\n","    <tr>\n","      <td>226</td>\n","      <td>0.704900</td>\n","    </tr>\n","    <tr>\n","      <td>227</td>\n","      <td>0.816000</td>\n","    </tr>\n","    <tr>\n","      <td>228</td>\n","      <td>0.723000</td>\n","    </tr>\n","    <tr>\n","      <td>229</td>\n","      <td>0.539100</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.590600</td>\n","    </tr>\n","    <tr>\n","      <td>231</td>\n","      <td>0.592600</td>\n","    </tr>\n","    <tr>\n","      <td>232</td>\n","      <td>0.723000</td>\n","    </tr>\n","    <tr>\n","      <td>233</td>\n","      <td>0.907100</td>\n","    </tr>\n","    <tr>\n","      <td>234</td>\n","      <td>0.589300</td>\n","    </tr>\n","    <tr>\n","      <td>235</td>\n","      <td>0.608400</td>\n","    </tr>\n","    <tr>\n","      <td>236</td>\n","      <td>0.674600</td>\n","    </tr>\n","    <tr>\n","      <td>237</td>\n","      <td>0.490300</td>\n","    </tr>\n","    <tr>\n","      <td>238</td>\n","      <td>0.551300</td>\n","    </tr>\n","    <tr>\n","      <td>239</td>\n","      <td>0.569600</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.829200</td>\n","    </tr>\n","    <tr>\n","      <td>241</td>\n","      <td>0.430100</td>\n","    </tr>\n","    <tr>\n","      <td>242</td>\n","      <td>0.691700</td>\n","    </tr>\n","    <tr>\n","      <td>243</td>\n","      <td>0.757100</td>\n","    </tr>\n","    <tr>\n","      <td>244</td>\n","      <td>0.544900</td>\n","    </tr>\n","    <tr>\n","      <td>245</td>\n","      <td>0.868100</td>\n","    </tr>\n","    <tr>\n","      <td>246</td>\n","      <td>0.611000</td>\n","    </tr>\n","    <tr>\n","      <td>247</td>\n","      <td>0.531600</td>\n","    </tr>\n","    <tr>\n","      <td>248</td>\n","      <td>0.719000</td>\n","    </tr>\n","    <tr>\n","      <td>249</td>\n","      <td>0.489200</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.756600</td>\n","    </tr>\n","    <tr>\n","      <td>251</td>\n","      <td>0.743200</td>\n","    </tr>\n","    <tr>\n","      <td>252</td>\n","      <td>0.583200</td>\n","    </tr>\n","    <tr>\n","      <td>253</td>\n","      <td>0.263400</td>\n","    </tr>\n","    <tr>\n","      <td>254</td>\n","      <td>0.540800</td>\n","    </tr>\n","    <tr>\n","      <td>255</td>\n","      <td>0.530100</td>\n","    </tr>\n","    <tr>\n","      <td>256</td>\n","      <td>0.452400</td>\n","    </tr>\n","    <tr>\n","      <td>257</td>\n","      <td>0.454300</td>\n","    </tr>\n","    <tr>\n","      <td>258</td>\n","      <td>0.467800</td>\n","    </tr>\n","    <tr>\n","      <td>259</td>\n","      <td>0.448200</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.469100</td>\n","    </tr>\n","    <tr>\n","      <td>261</td>\n","      <td>0.390700</td>\n","    </tr>\n","    <tr>\n","      <td>262</td>\n","      <td>0.510700</td>\n","    </tr>\n","    <tr>\n","      <td>263</td>\n","      <td>0.702800</td>\n","    </tr>\n","    <tr>\n","      <td>264</td>\n","      <td>0.652300</td>\n","    </tr>\n","    <tr>\n","      <td>265</td>\n","      <td>0.531300</td>\n","    </tr>\n","    <tr>\n","      <td>266</td>\n","      <td>0.591800</td>\n","    </tr>\n","    <tr>\n","      <td>267</td>\n","      <td>0.456800</td>\n","    </tr>\n","    <tr>\n","      <td>268</td>\n","      <td>0.557100</td>\n","    </tr>\n","    <tr>\n","      <td>269</td>\n","      <td>0.550700</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.514400</td>\n","    </tr>\n","    <tr>\n","      <td>271</td>\n","      <td>0.588500</td>\n","    </tr>\n","    <tr>\n","      <td>272</td>\n","      <td>0.599100</td>\n","    </tr>\n","    <tr>\n","      <td>273</td>\n","      <td>0.610900</td>\n","    </tr>\n","    <tr>\n","      <td>274</td>\n","      <td>0.396000</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>0.544400</td>\n","    </tr>\n","    <tr>\n","      <td>276</td>\n","      <td>0.686200</td>\n","    </tr>\n","    <tr>\n","      <td>277</td>\n","      <td>0.593900</td>\n","    </tr>\n","    <tr>\n","      <td>278</td>\n","      <td>0.368100</td>\n","    </tr>\n","    <tr>\n","      <td>279</td>\n","      <td>0.542500</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.620200</td>\n","    </tr>\n","    <tr>\n","      <td>281</td>\n","      <td>0.607900</td>\n","    </tr>\n","    <tr>\n","      <td>282</td>\n","      <td>0.461700</td>\n","    </tr>\n","    <tr>\n","      <td>283</td>\n","      <td>0.350300</td>\n","    </tr>\n","    <tr>\n","      <td>284</td>\n","      <td>0.315300</td>\n","    </tr>\n","    <tr>\n","      <td>285</td>\n","      <td>0.676400</td>\n","    </tr>\n","    <tr>\n","      <td>286</td>\n","      <td>0.494800</td>\n","    </tr>\n","    <tr>\n","      <td>287</td>\n","      <td>0.499400</td>\n","    </tr>\n","    <tr>\n","      <td>288</td>\n","      <td>0.531100</td>\n","    </tr>\n","    <tr>\n","      <td>289</td>\n","      <td>0.708800</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.626200</td>\n","    </tr>\n","    <tr>\n","      <td>291</td>\n","      <td>0.502600</td>\n","    </tr>\n","    <tr>\n","      <td>292</td>\n","      <td>0.552600</td>\n","    </tr>\n","    <tr>\n","      <td>293</td>\n","      <td>0.681200</td>\n","    </tr>\n","    <tr>\n","      <td>294</td>\n","      <td>0.560100</td>\n","    </tr>\n","    <tr>\n","      <td>295</td>\n","      <td>0.533900</td>\n","    </tr>\n","    <tr>\n","      <td>296</td>\n","      <td>0.569500</td>\n","    </tr>\n","    <tr>\n","      <td>297</td>\n","      <td>0.446600</td>\n","    </tr>\n","    <tr>\n","      <td>298</td>\n","      <td>0.465900</td>\n","    </tr>\n","    <tr>\n","      <td>299</td>\n","      <td>0.402200</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.848900</td>\n","    </tr>\n","    <tr>\n","      <td>301</td>\n","      <td>0.624900</td>\n","    </tr>\n","    <tr>\n","      <td>302</td>\n","      <td>0.417400</td>\n","    </tr>\n","    <tr>\n","      <td>303</td>\n","      <td>0.475300</td>\n","    </tr>\n","    <tr>\n","      <td>304</td>\n","      <td>0.473500</td>\n","    </tr>\n","    <tr>\n","      <td>305</td>\n","      <td>0.651600</td>\n","    </tr>\n","    <tr>\n","      <td>306</td>\n","      <td>0.624900</td>\n","    </tr>\n","    <tr>\n","      <td>307</td>\n","      <td>0.418300</td>\n","    </tr>\n","    <tr>\n","      <td>308</td>\n","      <td>0.383700</td>\n","    </tr>\n","    <tr>\n","      <td>309</td>\n","      <td>0.570800</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.639200</td>\n","    </tr>\n","    <tr>\n","      <td>311</td>\n","      <td>0.466200</td>\n","    </tr>\n","    <tr>\n","      <td>312</td>\n","      <td>0.421500</td>\n","    </tr>\n","    <tr>\n","      <td>313</td>\n","      <td>0.321200</td>\n","    </tr>\n","    <tr>\n","      <td>314</td>\n","      <td>0.593000</td>\n","    </tr>\n","    <tr>\n","      <td>315</td>\n","      <td>0.571600</td>\n","    </tr>\n","    <tr>\n","      <td>316</td>\n","      <td>0.419100</td>\n","    </tr>\n","    <tr>\n","      <td>317</td>\n","      <td>0.639700</td>\n","    </tr>\n","    <tr>\n","      <td>318</td>\n","      <td>0.465000</td>\n","    </tr>\n","    <tr>\n","      <td>319</td>\n","      <td>0.313400</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.465300</td>\n","    </tr>\n","    <tr>\n","      <td>321</td>\n","      <td>0.405100</td>\n","    </tr>\n","    <tr>\n","      <td>322</td>\n","      <td>0.509600</td>\n","    </tr>\n","    <tr>\n","      <td>323</td>\n","      <td>0.505700</td>\n","    </tr>\n","    <tr>\n","      <td>324</td>\n","      <td>0.458900</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>0.544700</td>\n","    </tr>\n","    <tr>\n","      <td>326</td>\n","      <td>0.543900</td>\n","    </tr>\n","    <tr>\n","      <td>327</td>\n","      <td>0.660800</td>\n","    </tr>\n","    <tr>\n","      <td>328</td>\n","      <td>0.463900</td>\n","    </tr>\n","    <tr>\n","      <td>329</td>\n","      <td>0.637500</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.488000</td>\n","    </tr>\n","    <tr>\n","      <td>331</td>\n","      <td>0.561600</td>\n","    </tr>\n","    <tr>\n","      <td>332</td>\n","      <td>0.564600</td>\n","    </tr>\n","    <tr>\n","      <td>333</td>\n","      <td>0.447600</td>\n","    </tr>\n","    <tr>\n","      <td>334</td>\n","      <td>0.473400</td>\n","    </tr>\n","    <tr>\n","      <td>335</td>\n","      <td>0.711500</td>\n","    </tr>\n","    <tr>\n","      <td>336</td>\n","      <td>0.406400</td>\n","    </tr>\n","    <tr>\n","      <td>337</td>\n","      <td>0.527300</td>\n","    </tr>\n","    <tr>\n","      <td>338</td>\n","      <td>0.652700</td>\n","    </tr>\n","    <tr>\n","      <td>339</td>\n","      <td>0.616100</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.632200</td>\n","    </tr>\n","    <tr>\n","      <td>341</td>\n","      <td>0.466000</td>\n","    </tr>\n","    <tr>\n","      <td>342</td>\n","      <td>0.432600</td>\n","    </tr>\n","    <tr>\n","      <td>343</td>\n","      <td>0.477000</td>\n","    </tr>\n","    <tr>\n","      <td>344</td>\n","      <td>0.518000</td>\n","    </tr>\n","    <tr>\n","      <td>345</td>\n","      <td>0.468600</td>\n","    </tr>\n","    <tr>\n","      <td>346</td>\n","      <td>0.626200</td>\n","    </tr>\n","    <tr>\n","      <td>347</td>\n","      <td>0.546500</td>\n","    </tr>\n","    <tr>\n","      <td>348</td>\n","      <td>0.489300</td>\n","    </tr>\n","    <tr>\n","      <td>349</td>\n","      <td>0.438100</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.374000</td>\n","    </tr>\n","    <tr>\n","      <td>351</td>\n","      <td>0.466100</td>\n","    </tr>\n","    <tr>\n","      <td>352</td>\n","      <td>0.481900</td>\n","    </tr>\n","    <tr>\n","      <td>353</td>\n","      <td>0.396700</td>\n","    </tr>\n","    <tr>\n","      <td>354</td>\n","      <td>0.413200</td>\n","    </tr>\n","    <tr>\n","      <td>355</td>\n","      <td>0.445800</td>\n","    </tr>\n","    <tr>\n","      <td>356</td>\n","      <td>0.423300</td>\n","    </tr>\n","    <tr>\n","      <td>357</td>\n","      <td>0.581700</td>\n","    </tr>\n","    <tr>\n","      <td>358</td>\n","      <td>0.423900</td>\n","    </tr>\n","    <tr>\n","      <td>359</td>\n","      <td>0.685100</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.434200</td>\n","    </tr>\n","    <tr>\n","      <td>361</td>\n","      <td>0.414800</td>\n","    </tr>\n","    <tr>\n","      <td>362</td>\n","      <td>0.321100</td>\n","    </tr>\n","    <tr>\n","      <td>363</td>\n","      <td>0.304200</td>\n","    </tr>\n","    <tr>\n","      <td>364</td>\n","      <td>0.443100</td>\n","    </tr>\n","    <tr>\n","      <td>365</td>\n","      <td>0.343100</td>\n","    </tr>\n","    <tr>\n","      <td>366</td>\n","      <td>0.477400</td>\n","    </tr>\n","    <tr>\n","      <td>367</td>\n","      <td>0.485400</td>\n","    </tr>\n","    <tr>\n","      <td>368</td>\n","      <td>0.473800</td>\n","    </tr>\n","    <tr>\n","      <td>369</td>\n","      <td>0.690700</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.419900</td>\n","    </tr>\n","    <tr>\n","      <td>371</td>\n","      <td>0.461000</td>\n","    </tr>\n","    <tr>\n","      <td>372</td>\n","      <td>0.429700</td>\n","    </tr>\n","    <tr>\n","      <td>373</td>\n","      <td>0.621300</td>\n","    </tr>\n","    <tr>\n","      <td>374</td>\n","      <td>0.600600</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>0.494100</td>\n","    </tr>\n","    <tr>\n","      <td>376</td>\n","      <td>0.438100</td>\n","    </tr>\n","    <tr>\n","      <td>377</td>\n","      <td>0.397100</td>\n","    </tr>\n","    <tr>\n","      <td>378</td>\n","      <td>0.514600</td>\n","    </tr>\n","    <tr>\n","      <td>379</td>\n","      <td>0.468800</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.323000</td>\n","    </tr>\n","    <tr>\n","      <td>381</td>\n","      <td>0.517200</td>\n","    </tr>\n","    <tr>\n","      <td>382</td>\n","      <td>0.466400</td>\n","    </tr>\n","    <tr>\n","      <td>383</td>\n","      <td>0.439700</td>\n","    </tr>\n","    <tr>\n","      <td>384</td>\n","      <td>0.373400</td>\n","    </tr>\n","    <tr>\n","      <td>385</td>\n","      <td>0.387500</td>\n","    </tr>\n","    <tr>\n","      <td>386</td>\n","      <td>0.496000</td>\n","    </tr>\n","    <tr>\n","      <td>387</td>\n","      <td>0.464700</td>\n","    </tr>\n","    <tr>\n","      <td>388</td>\n","      <td>0.373400</td>\n","    </tr>\n","    <tr>\n","      <td>389</td>\n","      <td>0.294200</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.223700</td>\n","    </tr>\n","    <tr>\n","      <td>391</td>\n","      <td>0.349500</td>\n","    </tr>\n","    <tr>\n","      <td>392</td>\n","      <td>0.459700</td>\n","    </tr>\n","    <tr>\n","      <td>393</td>\n","      <td>0.476000</td>\n","    </tr>\n","    <tr>\n","      <td>394</td>\n","      <td>0.402900</td>\n","    </tr>\n","    <tr>\n","      <td>395</td>\n","      <td>0.353100</td>\n","    </tr>\n","    <tr>\n","      <td>396</td>\n","      <td>0.299400</td>\n","    </tr>\n","    <tr>\n","      <td>397</td>\n","      <td>0.469100</td>\n","    </tr>\n","    <tr>\n","      <td>398</td>\n","      <td>0.282600</td>\n","    </tr>\n","    <tr>\n","      <td>399</td>\n","      <td>0.398000</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.565500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["trainer_stats = trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"6U2ealuWg0AK"},"source":["## Show final memory and time stats"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1731019801658,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"T5dWmBGhg0AK","outputId":"d8885332-66d9-456e-bc19-54bb71ac8ca3"},"outputs":[{"output_type":"stream","name":"stdout","text":["2665.9034 seconds used for training.\n","44.43 minutes used for training.\n","Peak reserved memory = 8.627 GB.\n","Peak reserved memory for training = 1.939 GB.\n","Peak reserved memory % of max memory = 58.496 %.\n","Peak reserved memory for training % of max memory = 13.148 %.\n"]}],"source":["#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1731019801944,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"G84F9jpUg0AK","outputId":"8162d1b0-ac58-4e57-9038-a0d55e2c9f0a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): LlamaForCausalLM(\n","      (model): LlamaModel(\n","        (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n","        (layers): ModuleList(\n","          (0-27): 28 x LlamaDecoderLayer(\n","            (self_attn): LlamaAttention(\n","              (q_proj): lora.Linear(\n","                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=3072, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (k_proj): lora.Linear(\n","                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=1024, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (v_proj): lora.Linear(\n","                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=1024, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (o_proj): lora.Linear(\n","                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=3072, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (rotary_emb): LlamaExtendedRotaryEmbedding()\n","            )\n","            (mlp): LlamaMLP(\n","              (gate_proj): lora.Linear(\n","                (base_layer): Linear(in_features=3072, out_features=8192, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=8192, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (up_proj): lora.Linear(\n","                (base_layer): Linear(in_features=3072, out_features=8192, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=8192, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (down_proj): lora.Linear(\n","                (base_layer): Linear(in_features=8192, out_features=3072, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=8192, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=3072, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (act_fn): SiLU()\n","            )\n","            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n","            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n","          )\n","        )\n","        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n","        (rotary_emb): LlamaRotaryEmbedding()\n","      )\n","      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":23}],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"nSz8P7t1g0AK"},"source":["## Loss Pos training"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252485,"status":"ok","timestamp":1731020054421,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"ej1rDzxEg0AL","outputId":"6cac6866-60a7-4561-c15f-30603fdc7bff"},"outputs":[{"output_type":"stream","name":"stderr","text":["Calculating loss post-training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [04:12<00:00,  7.92it/s]"]},{"output_type":"stream","name":"stdout","text":["Number of valid batches: 2000\n","Post-training mean loss: 0.491649413067149\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import torch\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","# Configure the collator and DataLoader\n","collator = DataCollatorForSeq2Seq(tokenizer)\n","loader = DataLoader(\n","    trainer.train_dataset,\n","    batch_size=2,  # Chosen batch size\n","    collate_fn=collator,\n","    num_workers=2\n",")\n","\n","# Variables to store the total loss and the number of valid batches\n","total_loss = 0.0\n","num_batches = 0\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Disable gradient calculation to save memory\n","with torch.no_grad():\n","    for batch in tqdm(loader, desc=\"Calculating loss post-training\"):\n","        # Move the batch to GPU (if available)\n","        batch = {k: v.to(model.device) for k, v in batch.items()}\n","\n","        # Forward pass\n","        outputs = model(**batch)\n","\n","        # Check if the loss is NaN\n","        if not torch.isnan(outputs.loss):\n","            total_loss += outputs.loss.item()\n","            num_batches += 1\n","            # print(outputs.loss.item())\n","\n","print(f\"Number of valid batches: {num_batches}\")\n","\n","# Calculate the average loss\n","if num_batches > 0:\n","    average_loss = total_loss / num_batches\n","    print(f\"Post-training mean loss: {average_loss}\")\n","else:\n","    print(\"No valid batches found.\")\n"]},{"cell_type":"markdown","metadata":{"id":"SkZwaFDBg0AL"},"source":["# Inference"]},{"cell_type":"markdown","metadata":{"id":"Yo9qGJMRg0AL"},"source":["## Question with option"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":510,"status":"ok","timestamp":1730566986423,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"sth9gjB2g0AL","outputId":"8322baeb-49a8-453e-b4b3-b1deb0c5b860"},"outputs":[{"data":{"text/plain":["{'from': 'human',\n"," 'value': 'Question: What is one disadvantage of performing a CMA (Cloud-based Mobile Augmentation) process on mobile computing?\\nOptions:\\noption 1: Dependency on high performance networking infrastructure\\noption 2: Excessive communication overhead and traffic\\noption 3: Unauthorized access to offloaded data\\noption 4: Application development complexity\\noption 5: All of the above\\n'}"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["dataset[5]['conversations'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vgSmxQJg0AL"},"outputs":[],"source":["question = dataset[5]['conversations'][0]['value']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3502,"status":"ok","timestamp":1730566994534,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"DSjrsAkTg0AM","outputId":"bfd360a6-0afc-486a-ecc7-57d1c3a709c4"},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"data":{"text/plain":["['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQuestion: What is one disadvantage of performing a CMA (Cloud-based Mobile Augmentation) process on mobile computing?\\nOptions:\\noption 1: Dependency on high performance networking infrastructure\\noption 2: Excessive communication overhead and traffic\\noption 3: Unauthorized access to offloaded data\\noption 4: Application development complexity\\noption 5: All of the above\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAnswer: option 5: All of the above\\nExplanation: CMA process has limitations like dependency on high performance networking infrastructure, excessive communication overhead, unauthorized access to offloaded data, and increased application development complexity.<|eot_id|>']"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["from unsloth.chat_templates import get_chat_template\n","\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template = \"llama-3.1\",\n",")\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","messages = [\n","    # {\"role\": \"user\", \"content\": \"How much is 1+1?\"},\n","    {\"role\": \"user\", \"content\": question},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","\n","outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n","                         temperature = 1.5, min_p = 0.1)\n","tokenizer.batch_decode(outputs)"]},{"cell_type":"markdown","metadata":{"id":"9N7LfjaNg0AM"},"source":["## Question with no option"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":1039,"status":"ok","timestamp":1730567024855,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"AiphXs0mg0AM","outputId":"10f6774b-d33d-4dc9-8fff-652a05805c80"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'What is the purpose of the rotation matrix R in the LCS to GCS transformation? [3GPP Release 17]'"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["dataset[0]['conversations'][0]['value']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIlHofKGg0AM"},"outputs":[],"source":["question = dataset[0]['conversations'][0]['value']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2694,"status":"ok","timestamp":1730567029848,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"WM_BzRaTg0AN","outputId":"42f13d67-95dc-450f-edeb-f898f5a69e4e"},"outputs":[{"data":{"text/plain":["['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the purpose of the rotation matrix R in the LCS to GCS transformation? [3GPP Release 17]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe purpose of the rotation matrix R in the LCS to GCS transformation is to transform the spatial coordinates into Earth-fixed coordinates.<|eot_id|>']"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["from unsloth.chat_templates import get_chat_template\n","\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template = \"llama-3.1\",\n",")\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","messages = [\n","    # {\"role\": \"user\", \"content\": \"How much is 1+1?\"},\n","    {\"role\": \"user\", \"content\": question},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","\n","outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n","                         temperature = 1.5, min_p = 0.1)\n","tokenizer.batch_decode(outputs)"]},{"cell_type":"markdown","metadata":{"id":"hlvDBrbzg0AN"},"source":["# Save model"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1703,"status":"ok","timestamp":1731020524737,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"4uQSkRwig0AN","outputId":"d1b6de87-8bc3-46ea-f033-6f5e82df518e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/Mestrado/Aula_IA024/Projeto_final/Models/llama_3.2_FT_lora_4000_questions_answer_label_2/tokenizer_config.json',\n"," '/content/drive/MyDrive/Mestrado/Aula_IA024/Projeto_final/Models/llama_3.2_FT_lora_4000_questions_answer_label_2/special_tokens_map.json',\n"," '/content/drive/MyDrive/Mestrado/Aula_IA024/Projeto_final/Models/llama_3.2_FT_lora_4000_questions_answer_label_2/tokenizer.json')"]},"metadata":{},"execution_count":27}],"source":["model.save_pretrained(\"/content/drive/MyDrive/Mestrado/Aula_IA024/Projeto_final/Models/llama_3.2_FT_lora_4000_questions_answer_label_2\", safe_serialization=False)\n","tokenizer.save_pretrained(\"/content/drive/MyDrive/Mestrado/Aula_IA024/Projeto_final/Models/llama_3.2_FT_lora_4000_questions_answer_label_2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PX0EgEI3g0AN"},"outputs":[],"source":["# model.save_pretrained_merged(\"model_3.2_lora_4bits\", tokenizer, save_method = \"merged_16bit\",)\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1132,"status":"ok","timestamp":1731020055545,"user":{"displayName":"Jos√© de Arimat√©a Passos Lopes J√∫nior","userId":"03876583231751055706"},"user_tz":180},"id":"xKcsJYi8xghz","outputId":"d0a10d82-8c50-4b71-dc1b-efd5451f2bc1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/Mestrado/Aula_IA024/Projeto_final/Models/llama_3.2_FT_lora_4000_questions_answer_label/tokenizer_config.json',\n"," '/content/drive/MyDrive/Mestrado/Aula_IA024/Projeto_final/Models/llama_3.2_FT_lora_4000_questions_answer_label/special_tokens_map.json',\n"," '/content/drive/MyDrive/Mestrado/Aula_IA024/Projeto_final/Models/llama_3.2_FT_lora_4000_questions_answer_label/tokenizer.json')"]},"metadata":{},"execution_count":25}],"source":["model.save_pretrained(\"/content/drive/MyDrive/Mestrado/Aula_IA024/Projeto_final/Models/llama_3.2_FT_lora_4000_questions_answer_label\")\n","tokenizer.save_pretrained(\"/content/drive/MyDrive/Mestrado/Aula_IA024/Projeto_final/Models/llama_3.2_FT_lora_4000_questions_answer_label\")"]},{"cell_type":"markdown","metadata":{"id":"QeVU_0qxg0AN"},"source":["# Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3ODVEJAg0AO"},"outputs":[],"source":["max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pP4ShG3kg0AO","outputId":"89d640d4-80e5-4b2b-b1db-8a58e6c09fe3"},"outputs":[{"name":"stdout","output_type":"stream","text":["==((====))==  Unsloth 2024.10.6: Fast Llama patching. Transformers = 4.46.0.\n","   \\\\   /|    GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU. Max memory: 3.712 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.6. CUDA Toolkit = 12.4.\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"name":"stderr","output_type":"stream","text":["Unsloth 2024.10.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"]}],"source":["model_path = \"../Models/llama_3.2_FT_lora_2000_questions\"\n","# model_path = \"model_3.2_lora_4bits\"\n","\n","# Carregar o modelo e o tokenizador separadamente\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=model_path,\n","    max_seq_length=max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit=load_in_4bit\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5v3oM8kcg0AO","outputId":"0c786381-bc10-48dd-d6a4-821088ae4f24"},"outputs":[{"data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): LlamaForCausalLM(\n","      (model): LlamaModel(\n","        (embed_tokens): Embedding(128256, 3072)\n","        (layers): ModuleList(\n","          (0-27): 28 x LlamaDecoderLayer(\n","            (self_attn): LlamaAttention(\n","              (q_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=3072, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (k_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=1024, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (v_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=1024, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (o_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=3072, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (rotary_emb): LlamaExtendedRotaryEmbedding()\n","            )\n","            (mlp): LlamaMLP(\n","              (gate_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=8192, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (up_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=3072, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=8192, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (down_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=8192, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=3072, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (act_fn): SiLU()\n","            )\n","            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n","            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n","          )\n","        )\n","        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n","        (rotary_emb): LlamaRotaryEmbedding()\n","      )\n","      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n","    )\n","  )\n",")"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"ENnhqn6og0AO"},"source":["# Test Loaded Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u09bzsI2g0AP","outputId":"a557d274-9908-4763-cd6f-92fb76e8671b"},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"data":{"text/plain":["['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQuestion: Which decoding algorithm for low-density parity-check codes involves a complex non-linear function in the check-node processing?\\nOptions:\\noption 1: Normalized min-sum\\noption 2: Off-set min-sum\\noption 3: Belief propagation\\noption 4: Successive cancellation\\noption 5: Sequential decoding\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAnswer: option 3: Belief propagation\\nExplanation: The belief propagation algorithm for low-density parity-check codes involves a complex non-linear function in the check-node processing, specifically the Q-function.<|eot_id|>']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from unsloth.chat_templates import get_chat_template\n","\n","question = dataset[5]['conversations'][0]['value']\n","\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template = \"llama-3.1\",\n",")\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","messages = [\n","    # {\"role\": \"user\", \"content\": \"How much is 1+1?\"},\n","    {\"role\": \"user\", \"content\": question},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","\n","outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n","                         temperature = 1.5, min_p = 0.1)\n","tokenizer.batch_decode(outputs)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"70c0af1d8d2b43048b98a7558a196327":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5693be8916cb46acb0ff8dcb02972f76","IPY_MODEL_6d50d49b67cb41ab9036fd5e6bbba79c","IPY_MODEL_fedef75fe8d644daa85359f6f547076b"],"layout":"IPY_MODEL_9fa2c10760f741ce9706f43920b46455"}},"5693be8916cb46acb0ff8dcb02972f76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80921a69a24d4820902bdc9d1174d30b","placeholder":"‚Äã","style":"IPY_MODEL_29122429dd534ceeb87bfc4f1d82c20c","value":"model.safetensors:‚Äá100%"}},"6d50d49b67cb41ab9036fd5e6bbba79c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_de0115e5b2f74b219dc50590d1ba2e58","max":6425529112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13c5613b5bf445d6b786f7387353c943","value":6425528500}},"fedef75fe8d644daa85359f6f547076b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_781f3988f9b044c4a46bccf715645df4","placeholder":"‚Äã","style":"IPY_MODEL_33baf9c73c05438ca162689161d0e984","value":"‚Äá6.43G/6.43G‚Äá[01:01&lt;00:00,‚Äá437MB/s]"}},"9fa2c10760f741ce9706f43920b46455":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80921a69a24d4820902bdc9d1174d30b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29122429dd534ceeb87bfc4f1d82c20c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de0115e5b2f74b219dc50590d1ba2e58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13c5613b5bf445d6b786f7387353c943":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"781f3988f9b044c4a46bccf715645df4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33baf9c73c05438ca162689161d0e984":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6735d3e0bdb4a8598dcc136afd91bf6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_157edc5a3b6145ac8a6a5150402a4fe4","IPY_MODEL_61c5801a7c094afca8cea0b53ce7ed7d","IPY_MODEL_3b56f1186c0142c58894fa2ff003935a"],"layout":"IPY_MODEL_e1a273de001a431ba2af7040d724427d"}},"157edc5a3b6145ac8a6a5150402a4fe4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77e22a9bdffa40bc8cab3b37178527b0","placeholder":"‚Äã","style":"IPY_MODEL_e13c8fc0699a45ee961875985ef951dc","value":"generation_config.json:‚Äá100%"}},"61c5801a7c094afca8cea0b53ce7ed7d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c65b12ea656f4feebf29350f525221c2","max":184,"min":0,"orientation":"horizontal","style":"IPY_MODEL_57e4cb7047f5401c8ca5b283afae4140","value":184}},"3b56f1186c0142c58894fa2ff003935a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_212522fb975143faa36fedbdb8507c37","placeholder":"‚Äã","style":"IPY_MODEL_b87314e38a1443d89925d3e2e3a2393b","value":"‚Äá184/184‚Äá[00:00&lt;00:00,‚Äá6.43kB/s]"}},"e1a273de001a431ba2af7040d724427d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77e22a9bdffa40bc8cab3b37178527b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e13c8fc0699a45ee961875985ef951dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c65b12ea656f4feebf29350f525221c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57e4cb7047f5401c8ca5b283afae4140":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"212522fb975143faa36fedbdb8507c37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b87314e38a1443d89925d3e2e3a2393b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"566ac0227a914449980c9ebc8dda5853":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_70b1d46452d94c88a2704cf066593cc0","IPY_MODEL_0a6edf146e6440559bcb7bb41408fa12","IPY_MODEL_51abf5b57ce04275ae12f938f68afbed"],"layout":"IPY_MODEL_2ba17ca986424d548abada440e800257"}},"70b1d46452d94c88a2704cf066593cc0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cde443acb343413faff3b6995c71ac41","placeholder":"‚Äã","style":"IPY_MODEL_6f582bc0a6384593aea63880bc72e487","value":"tokenizer_config.json:‚Äá100%"}},"0a6edf146e6440559bcb7bb41408fa12":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0fae63651c345d5876423a05f97f591","max":54598,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35075f81768645a4bce2bb5747daea70","value":54598}},"51abf5b57ce04275ae12f938f68afbed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed8f46d017184c7aaf6fad01291ecba9","placeholder":"‚Äã","style":"IPY_MODEL_6376357f99894f2086cf43984bcc5b66","value":"‚Äá54.6k/54.6k‚Äá[00:00&lt;00:00,‚Äá1.13MB/s]"}},"2ba17ca986424d548abada440e800257":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cde443acb343413faff3b6995c71ac41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f582bc0a6384593aea63880bc72e487":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0fae63651c345d5876423a05f97f591":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35075f81768645a4bce2bb5747daea70":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed8f46d017184c7aaf6fad01291ecba9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6376357f99894f2086cf43984bcc5b66":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68f924b54a964a3290d27a7b58e5a9ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_434f3a37ea3f46ab98bb3c9b5c20d099","IPY_MODEL_0425bacbba834fca831ffb0775a31518","IPY_MODEL_c2e8e75bc55c4a7597fda906ec564f89"],"layout":"IPY_MODEL_38003c547f60452e8da46f33a036d805"}},"434f3a37ea3f46ab98bb3c9b5c20d099":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f92821a826f42e0be1b2aeb18447253","placeholder":"‚Äã","style":"IPY_MODEL_9db75b28b6b64c3ca4acf3e1036ead05","value":"tokenizer.json:‚Äá100%"}},"0425bacbba834fca831ffb0775a31518":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d534bc062c2c4adeb01b7e4f55d3b83c","max":9085657,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1df91a71450542dba3807b74400f1c75","value":9085657}},"c2e8e75bc55c4a7597fda906ec564f89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_224e593273fd4de5901fd71f7288e189","placeholder":"‚Äã","style":"IPY_MODEL_ecf4cc7c30a3486e82a543df62c98e8e","value":"‚Äá9.09M/9.09M‚Äá[00:00&lt;00:00,‚Äá24.8MB/s]"}},"38003c547f60452e8da46f33a036d805":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f92821a826f42e0be1b2aeb18447253":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9db75b28b6b64c3ca4acf3e1036ead05":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d534bc062c2c4adeb01b7e4f55d3b83c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1df91a71450542dba3807b74400f1c75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"224e593273fd4de5901fd71f7288e189":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecf4cc7c30a3486e82a543df62c98e8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7bee969b3eaa4609b90a9f7289e4d053":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_476fda16ae734861ab6ca3a1d7a4e43a","IPY_MODEL_83d70ad13fc24bb1981e584ec637df4f","IPY_MODEL_320c3a68d6d34bd4b54705d07b88a05b"],"layout":"IPY_MODEL_31840c13880548e49796119958c0ddc6"}},"476fda16ae734861ab6ca3a1d7a4e43a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd3bdc26d2e5421db8f1112d873860e6","placeholder":"‚Äã","style":"IPY_MODEL_f25d2b0422b34e00a38fff8d5510b436","value":"special_tokens_map.json:‚Äá100%"}},"83d70ad13fc24bb1981e584ec637df4f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_418dececeea54f019e5d42707c65b59a","max":454,"min":0,"orientation":"horizontal","style":"IPY_MODEL_da473b6a9330463daebc6aaa43e47536","value":454}},"320c3a68d6d34bd4b54705d07b88a05b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_830020de55754b088db319d324cf1ee7","placeholder":"‚Äã","style":"IPY_MODEL_a200b3a0f6284e5cbe2bda2fbc5744f5","value":"‚Äá454/454‚Äá[00:00&lt;00:00,‚Äá11.7kB/s]"}},"31840c13880548e49796119958c0ddc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd3bdc26d2e5421db8f1112d873860e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f25d2b0422b34e00a38fff8d5510b436":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"418dececeea54f019e5d42707c65b59a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da473b6a9330463daebc6aaa43e47536":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"830020de55754b088db319d324cf1ee7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a200b3a0f6284e5cbe2bda2fbc5744f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}