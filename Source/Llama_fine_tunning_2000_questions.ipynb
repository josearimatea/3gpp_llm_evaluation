{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tunning using unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "CUDA Device Name:  NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "print(\"CUDA Device Name: \", torch.cuda.get_device_name(0))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verificar se CUDA est√° dispon√≠vel para acelerar o processamento\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 14:08:12.485380: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-29 14:08:12.497384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-29 14:08:12.513186: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-29 14:08:12.517931: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-29 14:08:12.529997: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-29 14:08:13.173145: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.10.6: Fast Llama patching. Transformers = 4.46.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU. Max memory: 3.712 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.6. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "initial_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name=\"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # device_map=\"auto\"\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = initial_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mover o modelo para CPU antes de deletar\n",
    "# model.to(\"cpu\")\n",
    "# # del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import gc\n",
    "\n",
    "# # Remover refer√™ncia ao modelo\n",
    "# del model\n",
    "\n",
    "# # Coleta de lixo para liberar objetos √≥rf√£os\n",
    "# gc.collect()\n",
    "\n",
    "# # Liberar mem√≥ria na GPU\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset TeleQnA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Release 17 Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the TeleQnA processed question in JSON file\n",
    "rel17_questions_path = r\"../Files/rel17_questions.json\"\n",
    "\n",
    "# Load the TeleQnA data just release 17\n",
    "with open(rel17_questions_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    rel17_questions = json.load(file)\n",
    "print(len(rel17_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Path to the TeleQnA processed question in JSON file\n",
    "rel17_100_questions_path = r\"../Files/rel17_100_questions.json\"\n",
    "\n",
    "# Load the TeleQnA data just release 17\n",
    "with open(rel17_100_questions_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    rel17_100_questions = json.load(file)\n",
    "print(len(rel17_100_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633\n"
     ]
    }
   ],
   "source": [
    "rel17_other_questions = [q for q in rel17_questions if q not in rel17_100_questions]\n",
    "print(len(rel17_other_questions))\n",
    "\n",
    "rel17_other_questions_length = 500\n",
    "\n",
    "rel17_other_questions = rel17_other_questions[:rel17_other_questions_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Which physical channel informs the UE and the RN about the number of OFDM symbols used for the PDCCHs? [3GPP Release 17]',\n",
       " 'option 1': 'PBCH',\n",
       " 'option 2': 'PCFICH',\n",
       " 'option 3': 'PDSCH',\n",
       " 'option 4': 'PHICH',\n",
       " 'answer': 'option 2: PCFICH',\n",
       " 'explanation': 'The physical control format indicator channel (PCFICH) informs the UE and the RN about the number of OFDM symbols used for the PDCCHs.',\n",
       " 'category': 'Standards specifications'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel17_other_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions without rel 17 and 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8487\n"
     ]
    }
   ],
   "source": [
    "# Path to the TeleQnA processed question in JSON file\n",
    "questions_no_rel_17_18_path = r\"../Files/questions_no_rel_17_18.json\"\n",
    "\n",
    "# Load the TeleQnA data just release 17\n",
    "with open(questions_no_rel_17_18_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    questions_no_rel_17_18 = json.load(file)\n",
    "print(len(questions_no_rel_17_18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "questions_no_rel_17_18_length = 1500\n",
    "questions_no_rel_17_18 = questions_no_rel_17_18[:questions_no_rel_17_18_length]\n",
    "print(len(questions_no_rel_17_18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Which non-orthogonal multiple access scheme utilizes the low-complexity message passing algorithm at the receiver for user data detection?',\n",
       " 'option 1': 'NOMA',\n",
       " 'option 2': 'PDMA',\n",
       " 'option 3': 'MUSA',\n",
       " 'option 4': 'MUST',\n",
       " 'option 5': 'SCMA',\n",
       " 'answer': 'option 5: SCMA',\n",
       " 'explanation': 'The SCMA scheme utilizes the low-complexity message passing algorithm at the receiver for user data detection.',\n",
       " 'category': 'Research publications'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_no_rel_17_18[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# train_questions = rel17_other_questions\n",
    "train_questions = rel17_other_questions + questions_no_rel_17_18\n",
    "print(len(train_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Which physical channel informs the UE and the RN about the number of OFDM symbols used for the PDCCHs? [3GPP Release 17]',\n",
       " 'option 1': 'PBCH',\n",
       " 'option 2': 'PCFICH',\n",
       " 'option 3': 'PDSCH',\n",
       " 'option 4': 'PHICH',\n",
       " 'answer': 'option 2: PCFICH',\n",
       " 'explanation': 'The physical control format indicator channel (PCFICH) informs the UE and the RN about the number of OFDM symbols used for the PDCCHs.',\n",
       " 'category': 'Standards specifications'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'from': 'human', 'value': 'Which physical channel informs the UE and the RN about the number of OFDM symbols used for the PDCCHs? [3GPP Release 17]'}, {'from': 'gpt', 'value': 'The physical control format indicator channel (PCFICH) informs the UE and the RN about the number of OFDM symbols used for the PDCCHs.'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Structure to store pairs of questions and explanations\n",
    "data = []\n",
    "\n",
    "half_questions = len(train_questions)//2\n",
    "\n",
    "# Fill the dataset with (question, explanation) pairs\n",
    "for item in train_questions[:half_questions]:\n",
    "    \n",
    "    human_value = (\n",
    "        f\"{item['question']}\"\n",
    "    )\n",
    "    \n",
    "    # Combine the answer and explanation\n",
    "    gpt_value = (\n",
    "        f\"{item['explanation']}\"\n",
    "    )\n",
    "    \n",
    "    # Create a dictionary for each input pair\n",
    "    pair = [\n",
    "        {'from': 'human', 'value': human_value},  # For the question\n",
    "        {'from': 'gpt', 'value': gpt_value}  # For the explanation\n",
    "    ]\n",
    "    \n",
    "    data.append(pair)  # Add the pair to the dataset\n",
    "\n",
    "data_no_options = data\n",
    "print(data_no_options[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'from': 'human', 'value': 'Question: What is the key challenge for SDON (Software Defined Optical Network) virtualization?\\nOptions:\\noption 1: High costs of optical infrastructures\\noption 2: Creating multiple VONs over the optical network infrastructure\\noption 3: Amortizing costs over relatively few users\\noption 4: Accounting for the specific optical transmission and signal propagation characteristics\\n'}, {'from': 'gpt', 'value': 'Answer: option 4: Accounting for the specific optical transmission and signal propagation characteristics\\nExplanation: The key challenge for SDON virtualization is accounting for the specific optical transmission and signal propagation characteristics.'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Structure to store pairs of questions, options, answers, and explanations\n",
    "data = []\n",
    "\n",
    "# Fill the dataset with (question + options, answer + explanation) pairs\n",
    "for item in train_questions[half_questions:]:\n",
    "    \n",
    "    # Extract options\n",
    "    options = [f\"{key}: {value}\" for key, value in item.items() if 'option' in key]\n",
    "    # Combine the question and options\n",
    "    human_value = (\n",
    "        f\"Question: {item['question']}\\n\"\n",
    "        f\"Options:\\n\" + \"\\n\".join(options) + \"\\n\"\n",
    "    )\n",
    "    \n",
    "    # Combine the answer and explanation\n",
    "    gpt_value = (\n",
    "        f\"Answer: {item['answer']}\\n\"\n",
    "        f\"Explanation: {item['explanation']}\"\n",
    "    )\n",
    "\n",
    "    # Create a dictionary for each input pair\n",
    "    pair = [\n",
    "        {'from': 'human', 'value': human_value},  # Question with options\n",
    "        {'from': 'gpt', 'value': gpt_value}       # Answer with explanation\n",
    "    ]\n",
    "    \n",
    "    data.append(pair)  # Add the pair to the dataset\n",
    "\n",
    "# Create the dataset using Hugging Face\n",
    "data_options = data\n",
    "print(data_options[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "data_total = data_no_options + data_options\n",
    "# Shuffle the combined data\n",
    "random.shuffle(data_total)\n",
    "print(len(data_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "{'conversations': [{'from': 'human', 'value': 'What do the TYPE(TAG) and VLAN TPID fields in an IEEE802.1Q tagged frame specify? [IEEE 802.3]'}, {'from': 'gpt', 'value': 'The TYPE(TAG) and VLAN TPID fields in an IEEE802.1Q tagged frame specify the VLAN identification.'}]}\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset using Hugging Face\n",
    "# Convert the list of pairs into the appropriate format\n",
    "formatted_data = {'conversations': data_total}\n",
    "\n",
    "# Transform the data into a Dataset\n",
    "dataset = Dataset.from_dict(formatted_data)\n",
    "\n",
    "# Display information about the created dataset\n",
    "print(dataset)\n",
    "\n",
    "# Display the first entry of the dataset\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.save_to_disk('../Files/train_questions_dataset_2000_questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_path = '../Files/train_questions_dataset_2000_questions'\n",
    "\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': 'What is the purpose of sequence and cyclic shift hopping in PUCCH formats 0, 1, 3, and 4? [3GPP Release 17]'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Sequence and cyclic shift hopping are used in PUCCH formats 0, 1, 3, and 4 to reduce the interference between PUCCH transmissions. These hopping mechanisms ensure that different UEs use different sequences and cyclic shifts, increasing the orthogonality of the transmitted signals and reducing inter-UE interference.'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': 'Question: Which decoding algorithm for low-density parity-check codes involves a complex non-linear function in the check-node processing?\\nOptions:\\noption 1: Normalized min-sum\\noption 2: Off-set min-sum\\noption 3: Belief propagation\\noption 4: Successive cancellation\\noption 5: Sequential decoding\\n'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Answer: option 3: Belief propagation\\nExplanation: The belief propagation decoding algorithm of low-density parity-check codes involves a complex non-linear function in the check-node processing.'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TeleQnA Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8252b44fd1346269ba3585f97e15092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'text'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQuestion: Which decoding algorithm for low-density parity-check codes involves a complex non-linear function in the check-node processing?\\nOptions:\\noption 1: Normalized min-sum\\noption 2: Off-set min-sum\\noption 3: Belief propagation\\noption 4: Successive cancellation\\noption 5: Sequential decoding\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAnswer: option 3: Belief propagation\\nExplanation: The belief propagation decoding algorithm of low-density parity-check codes involves a complex non-linear function in the check-node processing.<|eot_id|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.10.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c18ea01ade74ecf8fe1c82788b6e2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimatea/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 16,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 150,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cf8e92f7c14482be28661066e94bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the purpose of sequence and cyclic shift hopping in PUCCH formats 0, 1, 3, and 4? [3GPP Release 17]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSequence and cyclic shift hopping are used in PUCCH formats 0, 1, 3, and 4 to reduce the interference between PUCCH transmissions. These hopping mechanisms ensure that different UEs use different sequences and cyclic shifts, increasing the orthogonality of the transmitted signals and reducing inter-UE interference.<|eot_id|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                      \\n\\nSequence and cyclic shift hopping are used in PUCCH formats 0, 1, 3, and 4 to reduce the interference between PUCCH transmissions. These hopping mechanisms ensure that different UEs use different sequences and cyclic shifts, increasing the orthogonality of the transmitted signals and reducing inter-UE interference.<|eot_id|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show current memory stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3050 Ti Laptop GPU. Max memory = 3.712 GB.\n",
      "2.768 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('input_ids', [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 5887, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 3923, 374, 279, 7580, 315, 8668, 323, 77102, 6541, 93338, 304, 393, 5576, 2198, 20447, 220, 15, 11, 220, 16, 11, 220, 18, 11, 323, 220, 19, 30, 510, 18, 38, 4505, 17836, 220, 1114, 60, 128009, 128006, 78191, 128007, 271, 14405, 323, 77102, 6541, 93338, 527, 1511, 304, 393, 5576, 2198, 20447, 220, 15, 11, 220, 16, 11, 220, 18, 11, 323, 220, 19, 311, 8108, 279, 32317, 1990, 393, 5576, 2198, 92096, 13, 4314, 93338, 24717, 6106, 430, 2204, 549, 17812, 1005, 2204, 24630, 323, 77102, 29735, 11, 7859, 279, 30299, 540, 263, 2786, 315, 279, 34699, 17738, 323, 18189, 958, 12, 2279, 32317, 13, 128009]), ('attention_mask', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('labels', [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 271, 14405, 323, 77102, 6541, 93338, 527, 1511, 304, 393, 5576, 2198, 20447, 220, 15, 11, 220, 16, 11, 220, 18, 11, 323, 220, 19, 311, 8108, 279, 32317, 1990, 393, 5576, 2198, 92096, 13, 4314, 93338, 24717, 6106, 430, 2204, 549, 17812, 1005, 2204, 24630, 323, 77102, 29735, 11, 7859, 279, 30299, 540, 263, 2786, 315, 279, 34699, 17738, 323, 18189, 958, 12, 2279, 32317, 13, 128009])])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset[0].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating initial loss: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [10:09<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial mean loss: 1.7596246767640114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configura o collator e DataLoader\n",
    "collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "loader = DataLoader(trainer.train_dataset, \n",
    "                    batch_size=2,  # Tamanho do batch escolhido\n",
    "                    collate_fn=collator, \n",
    "                    num_workers=2)\n",
    "\n",
    "# Vari√°veis para armazenar a loss total e o n√∫mero de exemplos\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "# Coloca o modelo em modo de avalia√ß√£o\n",
    "model.eval()\n",
    "\n",
    "# Desativa o c√°lculo de gradiente para economizar mem√≥ria\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Calculating initial loss\"):\n",
    "        # Move o batch para a GPU (se dispon√≠vel)\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        # Acumula a loss\n",
    "        total_loss += outputs.loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Calcula a loss m√©dia\n",
    "average_loss = total_loss / num_batches\n",
    "print(f\"Initial mean loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,000 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 16\n",
      "\\        /    Total batch size = 32 | Total steps = 150\n",
      " \"-____-\"     Number of trainable parameters = 24,313,856\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a6e55c3c7e44dd986b868a7c494d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7554, 'grad_norm': 1.1779676675796509, 'learning_rate': 4e-05, 'epoch': 0.02}\n",
      "{'loss': 1.795, 'grad_norm': 1.1981481313705444, 'learning_rate': 8e-05, 'epoch': 0.03}\n",
      "{'loss': 1.6159, 'grad_norm': 1.178970456123352, 'learning_rate': 0.00012, 'epoch': 0.05}\n",
      "{'loss': 1.5085, 'grad_norm': 1.2041479349136353, 'learning_rate': 0.00016, 'epoch': 0.06}\n",
      "{'loss': 1.4737, 'grad_norm': 1.4692350625991821, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.0093, 'grad_norm': 0.9795082211494446, 'learning_rate': 0.00019862068965517243, 'epoch': 0.1}\n",
      "{'loss': 1.23, 'grad_norm': 0.4913758933544159, 'learning_rate': 0.00019724137931034484, 'epoch': 0.11}\n",
      "{'loss': 1.1138, 'grad_norm': 0.37414249777793884, 'learning_rate': 0.00019586206896551723, 'epoch': 0.13}\n",
      "{'loss': 1.0801, 'grad_norm': 0.32851642370224, 'learning_rate': 0.00019448275862068965, 'epoch': 0.14}\n",
      "{'loss': 1.1364, 'grad_norm': 0.5934054851531982, 'learning_rate': 0.0001931034482758621, 'epoch': 0.16}\n",
      "{'loss': 0.7544, 'grad_norm': 0.34499478340148926, 'learning_rate': 0.0001917241379310345, 'epoch': 0.18}\n",
      "{'loss': 0.7932, 'grad_norm': 0.32605043053627014, 'learning_rate': 0.0001903448275862069, 'epoch': 0.19}\n",
      "{'loss': 1.1399, 'grad_norm': 0.3528296649456024, 'learning_rate': 0.00018896551724137932, 'epoch': 0.21}\n",
      "{'loss': 0.9928, 'grad_norm': 0.3313586115837097, 'learning_rate': 0.00018758620689655173, 'epoch': 0.22}\n",
      "{'loss': 0.9857, 'grad_norm': 0.3274710476398468, 'learning_rate': 0.00018620689655172415, 'epoch': 0.24}\n",
      "{'loss': 0.9504, 'grad_norm': 0.3382256031036377, 'learning_rate': 0.00018482758620689654, 'epoch': 0.26}\n",
      "{'loss': 0.855, 'grad_norm': 0.29286906123161316, 'learning_rate': 0.00018344827586206896, 'epoch': 0.27}\n",
      "{'loss': 0.7386, 'grad_norm': 0.26041853427886963, 'learning_rate': 0.0001820689655172414, 'epoch': 0.29}\n",
      "{'loss': 1.1183, 'grad_norm': 0.3692443072795868, 'learning_rate': 0.00018068965517241382, 'epoch': 0.3}\n",
      "{'loss': 0.9723, 'grad_norm': 0.37787047028541565, 'learning_rate': 0.0001793103448275862, 'epoch': 0.32}\n",
      "{'loss': 0.9736, 'grad_norm': 0.3014410138130188, 'learning_rate': 0.00017793103448275862, 'epoch': 0.34}\n",
      "{'loss': 0.9615, 'grad_norm': 0.41476479172706604, 'learning_rate': 0.00017655172413793104, 'epoch': 0.35}\n",
      "{'loss': 0.8115, 'grad_norm': 0.3183571398258209, 'learning_rate': 0.00017517241379310346, 'epoch': 0.37}\n",
      "{'loss': 1.0328, 'grad_norm': 0.34240689873695374, 'learning_rate': 0.00017379310344827587, 'epoch': 0.38}\n",
      "{'loss': 0.9066, 'grad_norm': 0.3039093017578125, 'learning_rate': 0.00017241379310344826, 'epoch': 0.4}\n",
      "{'loss': 1.0844, 'grad_norm': 0.3191166818141937, 'learning_rate': 0.0001710344827586207, 'epoch': 0.42}\n",
      "{'loss': 1.0357, 'grad_norm': 0.3095075786113739, 'learning_rate': 0.00016965517241379312, 'epoch': 0.43}\n",
      "{'loss': 0.6701, 'grad_norm': 0.28943249583244324, 'learning_rate': 0.00016827586206896554, 'epoch': 0.45}\n",
      "{'loss': 1.1016, 'grad_norm': 0.3450203239917755, 'learning_rate': 0.00016689655172413793, 'epoch': 0.46}\n",
      "{'loss': 0.8666, 'grad_norm': 0.27881231904029846, 'learning_rate': 0.00016551724137931035, 'epoch': 0.48}\n",
      "{'loss': 0.8516, 'grad_norm': 0.36159172654151917, 'learning_rate': 0.00016413793103448276, 'epoch': 0.5}\n",
      "{'loss': 1.0186, 'grad_norm': 0.32479628920555115, 'learning_rate': 0.00016275862068965518, 'epoch': 0.51}\n",
      "{'loss': 1.2612, 'grad_norm': 0.43454626202583313, 'learning_rate': 0.0001613793103448276, 'epoch': 0.53}\n",
      "{'loss': 0.8249, 'grad_norm': 0.2913656234741211, 'learning_rate': 0.00016, 'epoch': 0.54}\n",
      "{'loss': 0.7973, 'grad_norm': 0.30668628215789795, 'learning_rate': 0.00015862068965517243, 'epoch': 0.56}\n",
      "{'loss': 0.9877, 'grad_norm': 0.2862425744533539, 'learning_rate': 0.00015724137931034485, 'epoch': 0.58}\n",
      "{'loss': 0.7981, 'grad_norm': 0.263672798871994, 'learning_rate': 0.00015586206896551724, 'epoch': 0.59}\n",
      "{'loss': 0.8251, 'grad_norm': 0.2737390398979187, 'learning_rate': 0.00015448275862068965, 'epoch': 0.61}\n",
      "{'loss': 1.0424, 'grad_norm': 0.393291711807251, 'learning_rate': 0.00015310344827586207, 'epoch': 0.62}\n",
      "{'loss': 0.7633, 'grad_norm': 0.2923906147480011, 'learning_rate': 0.00015172413793103449, 'epoch': 0.64}\n",
      "{'loss': 1.0509, 'grad_norm': 0.30008330941200256, 'learning_rate': 0.0001503448275862069, 'epoch': 0.66}\n",
      "{'loss': 0.9494, 'grad_norm': 0.33438077569007874, 'learning_rate': 0.00014896551724137932, 'epoch': 0.67}\n",
      "{'loss': 1.1097, 'grad_norm': 0.34148675203323364, 'learning_rate': 0.00014758620689655174, 'epoch': 0.69}\n",
      "{'loss': 0.8819, 'grad_norm': 0.3010514974594116, 'learning_rate': 0.00014620689655172415, 'epoch': 0.7}\n",
      "{'loss': 1.048, 'grad_norm': 0.3552398681640625, 'learning_rate': 0.00014482758620689657, 'epoch': 0.72}\n",
      "{'loss': 1.1741, 'grad_norm': 0.3935079872608185, 'learning_rate': 0.00014344827586206896, 'epoch': 0.74}\n",
      "{'loss': 0.818, 'grad_norm': 0.4532277584075928, 'learning_rate': 0.00014206896551724138, 'epoch': 0.75}\n",
      "{'loss': 1.0268, 'grad_norm': 0.3042525351047516, 'learning_rate': 0.0001406896551724138, 'epoch': 0.77}\n",
      "{'loss': 0.9341, 'grad_norm': 0.3042876422405243, 'learning_rate': 0.0001393103448275862, 'epoch': 0.78}\n",
      "{'loss': 1.0253, 'grad_norm': 0.3101802468299866, 'learning_rate': 0.00013793103448275863, 'epoch': 0.8}\n",
      "{'loss': 1.0906, 'grad_norm': 0.3322308361530304, 'learning_rate': 0.00013655172413793104, 'epoch': 0.82}\n",
      "{'loss': 0.9119, 'grad_norm': 0.35019901394844055, 'learning_rate': 0.00013517241379310346, 'epoch': 0.83}\n",
      "{'loss': 0.7564, 'grad_norm': 0.27668535709381104, 'learning_rate': 0.00013379310344827588, 'epoch': 0.85}\n",
      "{'loss': 0.9372, 'grad_norm': 0.34000542759895325, 'learning_rate': 0.0001324137931034483, 'epoch': 0.86}\n",
      "{'loss': 0.966, 'grad_norm': 0.3114943206310272, 'learning_rate': 0.00013103448275862068, 'epoch': 0.88}\n",
      "{'loss': 0.9492, 'grad_norm': 0.31141892075538635, 'learning_rate': 0.0001296551724137931, 'epoch': 0.9}\n",
      "{'loss': 0.8245, 'grad_norm': 0.2651841342449188, 'learning_rate': 0.00012827586206896552, 'epoch': 0.91}\n",
      "{'loss': 0.842, 'grad_norm': 0.31623297929763794, 'learning_rate': 0.00012689655172413793, 'epoch': 0.93}\n",
      "{'loss': 0.9397, 'grad_norm': 0.30961737036705017, 'learning_rate': 0.00012551724137931035, 'epoch': 0.94}\n",
      "{'loss': 1.25, 'grad_norm': 0.33731383085250854, 'learning_rate': 0.00012413793103448277, 'epoch': 0.96}\n",
      "{'loss': 0.991, 'grad_norm': 0.2881174683570862, 'learning_rate': 0.00012275862068965518, 'epoch': 0.98}\n",
      "{'loss': 0.6976, 'grad_norm': 0.2912195324897766, 'learning_rate': 0.00012137931034482759, 'epoch': 0.99}\n",
      "{'loss': 1.1602, 'grad_norm': 0.4890190064907074, 'learning_rate': 0.00012, 'epoch': 1.01}\n",
      "{'loss': 0.7576, 'grad_norm': 0.28467264771461487, 'learning_rate': 0.0001186206896551724, 'epoch': 1.02}\n",
      "{'loss': 0.9808, 'grad_norm': 0.35689160227775574, 'learning_rate': 0.00011724137931034482, 'epoch': 1.04}\n",
      "{'loss': 0.7641, 'grad_norm': 0.2608800232410431, 'learning_rate': 0.00011586206896551725, 'epoch': 1.06}\n",
      "{'loss': 0.89, 'grad_norm': 0.28996801376342773, 'learning_rate': 0.00011448275862068967, 'epoch': 1.07}\n",
      "{'loss': 0.7662, 'grad_norm': 0.3088463544845581, 'learning_rate': 0.00011310344827586207, 'epoch': 1.09}\n",
      "{'loss': 0.7774, 'grad_norm': 0.29502564668655396, 'learning_rate': 0.00011172413793103449, 'epoch': 1.1}\n",
      "{'loss': 0.8706, 'grad_norm': 0.3129150867462158, 'learning_rate': 0.0001103448275862069, 'epoch': 1.12}\n",
      "{'loss': 0.8296, 'grad_norm': 0.2948050796985626, 'learning_rate': 0.00010896551724137931, 'epoch': 1.14}\n",
      "{'loss': 0.9244, 'grad_norm': 0.31470370292663574, 'learning_rate': 0.00010758620689655173, 'epoch': 1.15}\n",
      "{'loss': 0.8289, 'grad_norm': 0.32066601514816284, 'learning_rate': 0.00010620689655172413, 'epoch': 1.17}\n",
      "{'loss': 0.8159, 'grad_norm': 0.3197520077228546, 'learning_rate': 0.00010482758620689656, 'epoch': 1.18}\n",
      "{'loss': 0.7891, 'grad_norm': 0.34311437606811523, 'learning_rate': 0.00010344827586206898, 'epoch': 1.2}\n",
      "{'loss': 1.0157, 'grad_norm': 0.3632943630218506, 'learning_rate': 0.0001020689655172414, 'epoch': 1.22}\n",
      "{'loss': 1.1234, 'grad_norm': 0.37968435883522034, 'learning_rate': 0.0001006896551724138, 'epoch': 1.23}\n",
      "{'loss': 0.7416, 'grad_norm': 0.3286125063896179, 'learning_rate': 9.931034482758621e-05, 'epoch': 1.25}\n",
      "{'loss': 0.8448, 'grad_norm': 0.3295709192752838, 'learning_rate': 9.793103448275862e-05, 'epoch': 1.26}\n",
      "{'loss': 0.8361, 'grad_norm': 0.331557959318161, 'learning_rate': 9.655172413793105e-05, 'epoch': 1.28}\n",
      "{'loss': 0.7797, 'grad_norm': 0.43757304549217224, 'learning_rate': 9.517241379310345e-05, 'epoch': 1.3}\n",
      "{'loss': 1.1995, 'grad_norm': 0.48660919070243835, 'learning_rate': 9.379310344827587e-05, 'epoch': 1.31}\n",
      "{'loss': 0.743, 'grad_norm': 0.3262024521827698, 'learning_rate': 9.241379310344827e-05, 'epoch': 1.33}\n",
      "{'loss': 0.7272, 'grad_norm': 0.3429403305053711, 'learning_rate': 9.10344827586207e-05, 'epoch': 1.34}\n",
      "{'loss': 0.7992, 'grad_norm': 0.3692304193973541, 'learning_rate': 8.96551724137931e-05, 'epoch': 1.36}\n",
      "{'loss': 0.8683, 'grad_norm': 0.428960919380188, 'learning_rate': 8.827586206896552e-05, 'epoch': 1.38}\n",
      "{'loss': 0.9628, 'grad_norm': 0.3710855543613434, 'learning_rate': 8.689655172413794e-05, 'epoch': 1.39}\n",
      "{'loss': 0.705, 'grad_norm': 0.3372655510902405, 'learning_rate': 8.551724137931035e-05, 'epoch': 1.41}\n",
      "{'loss': 0.6895, 'grad_norm': 0.3470989763736725, 'learning_rate': 8.413793103448277e-05, 'epoch': 1.42}\n",
      "{'loss': 0.7421, 'grad_norm': 0.3625238239765167, 'learning_rate': 8.275862068965517e-05, 'epoch': 1.44}\n",
      "{'loss': 0.9103, 'grad_norm': 0.42948779463768005, 'learning_rate': 8.137931034482759e-05, 'epoch': 1.46}\n",
      "{'loss': 0.7076, 'grad_norm': 0.3625190258026123, 'learning_rate': 8e-05, 'epoch': 1.47}\n",
      "{'loss': 0.8833, 'grad_norm': 0.39743730425834656, 'learning_rate': 7.862068965517242e-05, 'epoch': 1.49}\n",
      "{'loss': 0.7198, 'grad_norm': 0.3700655996799469, 'learning_rate': 7.724137931034483e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6603, 'grad_norm': 0.3826592266559601, 'learning_rate': 7.586206896551724e-05, 'epoch': 1.52}\n",
      "{'loss': 0.872, 'grad_norm': 0.41519513726234436, 'learning_rate': 7.448275862068966e-05, 'epoch': 1.54}\n",
      "{'loss': 0.7515, 'grad_norm': 0.3825671374797821, 'learning_rate': 7.310344827586208e-05, 'epoch': 1.55}\n",
      "{'loss': 0.8456, 'grad_norm': 0.41856345534324646, 'learning_rate': 7.172413793103448e-05, 'epoch': 1.57}\n",
      "{'loss': 0.7571, 'grad_norm': 0.44004565477371216, 'learning_rate': 7.03448275862069e-05, 'epoch': 1.58}\n",
      "{'loss': 0.8077, 'grad_norm': 0.4437609016895294, 'learning_rate': 6.896551724137931e-05, 'epoch': 1.6}\n",
      "{'loss': 0.7571, 'grad_norm': 0.42849430441856384, 'learning_rate': 6.758620689655173e-05, 'epoch': 1.62}\n",
      "{'loss': 0.8797, 'grad_norm': 0.4685274660587311, 'learning_rate': 6.620689655172415e-05, 'epoch': 1.63}\n",
      "{'loss': 0.8385, 'grad_norm': 0.4373624920845032, 'learning_rate': 6.482758620689655e-05, 'epoch': 1.65}\n",
      "{'loss': 0.702, 'grad_norm': 0.42957255244255066, 'learning_rate': 6.344827586206897e-05, 'epoch': 1.66}\n",
      "{'loss': 0.8713, 'grad_norm': 0.47857359051704407, 'learning_rate': 6.206896551724138e-05, 'epoch': 1.68}\n",
      "{'loss': 1.0162, 'grad_norm': 0.5498894453048706, 'learning_rate': 6.068965517241379e-05, 'epoch': 1.7}\n",
      "{'loss': 0.8523, 'grad_norm': 0.44802266359329224, 'learning_rate': 5.93103448275862e-05, 'epoch': 1.71}\n",
      "{'loss': 0.8606, 'grad_norm': 0.41869059205055237, 'learning_rate': 5.7931034482758627e-05, 'epoch': 1.73}\n",
      "{'loss': 0.9199, 'grad_norm': 0.47212231159210205, 'learning_rate': 5.6551724137931037e-05, 'epoch': 1.74}\n",
      "{'loss': 0.8087, 'grad_norm': 0.43266093730926514, 'learning_rate': 5.517241379310345e-05, 'epoch': 1.76}\n",
      "{'loss': 0.7972, 'grad_norm': 0.4299498200416565, 'learning_rate': 5.379310344827586e-05, 'epoch': 1.78}\n",
      "{'loss': 0.5387, 'grad_norm': 0.34071844816207886, 'learning_rate': 5.241379310344828e-05, 'epoch': 1.79}\n",
      "{'loss': 1.0582, 'grad_norm': 0.4968048632144928, 'learning_rate': 5.10344827586207e-05, 'epoch': 1.81}\n",
      "{'loss': 0.8303, 'grad_norm': 0.46751782298088074, 'learning_rate': 4.9655172413793107e-05, 'epoch': 1.82}\n",
      "{'loss': 0.9427, 'grad_norm': 0.4937218129634857, 'learning_rate': 4.827586206896552e-05, 'epoch': 1.84}\n",
      "{'loss': 1.0166, 'grad_norm': 0.4953802227973938, 'learning_rate': 4.689655172413793e-05, 'epoch': 1.86}\n",
      "{'loss': 0.9346, 'grad_norm': 0.4782645106315613, 'learning_rate': 4.551724137931035e-05, 'epoch': 1.87}\n",
      "{'loss': 0.6902, 'grad_norm': 0.45187708735466003, 'learning_rate': 4.413793103448276e-05, 'epoch': 1.89}\n",
      "{'loss': 0.799, 'grad_norm': 0.4356216490268707, 'learning_rate': 4.275862068965518e-05, 'epoch': 1.9}\n",
      "{'loss': 0.8905, 'grad_norm': 0.4908713400363922, 'learning_rate': 4.1379310344827587e-05, 'epoch': 1.92}\n",
      "{'loss': 0.9512, 'grad_norm': 0.4955331087112427, 'learning_rate': 4e-05, 'epoch': 1.94}\n",
      "{'loss': 0.8222, 'grad_norm': 0.40872102975845337, 'learning_rate': 3.862068965517241e-05, 'epoch': 1.95}\n",
      "{'loss': 0.895, 'grad_norm': 0.466973215341568, 'learning_rate': 3.724137931034483e-05, 'epoch': 1.97}\n",
      "{'loss': 0.8448, 'grad_norm': 0.4502619802951813, 'learning_rate': 3.586206896551724e-05, 'epoch': 1.98}\n",
      "{'loss': 1.2584, 'grad_norm': 0.6628237962722778, 'learning_rate': 3.4482758620689657e-05, 'epoch': 2.0}\n",
      "{'loss': 0.7397, 'grad_norm': 0.46802204847335815, 'learning_rate': 3.310344827586207e-05, 'epoch': 2.02}\n",
      "{'loss': 0.7856, 'grad_norm': 0.4239163398742676, 'learning_rate': 3.172413793103448e-05, 'epoch': 2.03}\n",
      "{'loss': 0.7642, 'grad_norm': 0.40442290902137756, 'learning_rate': 3.0344827586206897e-05, 'epoch': 2.05}\n",
      "{'loss': 0.8826, 'grad_norm': 0.4213408827781677, 'learning_rate': 2.8965517241379313e-05, 'epoch': 2.06}\n",
      "{'loss': 0.764, 'grad_norm': 0.38510775566101074, 'learning_rate': 2.7586206896551727e-05, 'epoch': 2.08}\n",
      "{'loss': 0.7505, 'grad_norm': 0.4317578077316284, 'learning_rate': 2.620689655172414e-05, 'epoch': 2.1}\n",
      "{'loss': 0.6354, 'grad_norm': 0.42476537823677063, 'learning_rate': 2.4827586206896553e-05, 'epoch': 2.11}\n",
      "{'loss': 0.5849, 'grad_norm': 0.40117043256759644, 'learning_rate': 2.3448275862068967e-05, 'epoch': 2.13}\n",
      "{'loss': 0.6659, 'grad_norm': 0.4119439125061035, 'learning_rate': 2.206896551724138e-05, 'epoch': 2.14}\n",
      "{'loss': 0.9097, 'grad_norm': 0.4394450783729553, 'learning_rate': 2.0689655172413793e-05, 'epoch': 2.16}\n",
      "{'loss': 0.7492, 'grad_norm': 0.47096681594848633, 'learning_rate': 1.9310344827586207e-05, 'epoch': 2.18}\n",
      "{'loss': 0.7945, 'grad_norm': 0.49793344736099243, 'learning_rate': 1.793103448275862e-05, 'epoch': 2.19}\n",
      "{'loss': 0.7223, 'grad_norm': 0.45681706070899963, 'learning_rate': 1.6551724137931037e-05, 'epoch': 2.21}\n",
      "{'loss': 0.5111, 'grad_norm': 0.36589905619621277, 'learning_rate': 1.5172413793103448e-05, 'epoch': 2.22}\n",
      "{'loss': 0.9152, 'grad_norm': 0.5148359537124634, 'learning_rate': 1.3793103448275863e-05, 'epoch': 2.24}\n",
      "{'loss': 1.0164, 'grad_norm': 0.6123867630958557, 'learning_rate': 1.2413793103448277e-05, 'epoch': 2.26}\n",
      "{'loss': 0.8032, 'grad_norm': 0.4966006577014923, 'learning_rate': 1.103448275862069e-05, 'epoch': 2.27}\n",
      "{'loss': 0.8304, 'grad_norm': 0.494733601808548, 'learning_rate': 9.655172413793103e-06, 'epoch': 2.29}\n",
      "{'loss': 0.8462, 'grad_norm': 0.4915632903575897, 'learning_rate': 8.275862068965518e-06, 'epoch': 2.3}\n",
      "{'loss': 0.8934, 'grad_norm': 0.5541467070579529, 'learning_rate': 6.896551724137932e-06, 'epoch': 2.32}\n",
      "{'loss': 0.7313, 'grad_norm': 0.43902111053466797, 'learning_rate': 5.517241379310345e-06, 'epoch': 2.34}\n",
      "{'loss': 0.7109, 'grad_norm': 0.4492824375629425, 'learning_rate': 4.137931034482759e-06, 'epoch': 2.35}\n",
      "{'loss': 0.6339, 'grad_norm': 0.41646960377693176, 'learning_rate': 2.7586206896551725e-06, 'epoch': 2.37}\n",
      "{'loss': 0.6806, 'grad_norm': 0.4369356632232666, 'learning_rate': 1.3793103448275862e-06, 'epoch': 2.38}\n",
      "{'loss': 0.641, 'grad_norm': 0.4601697623729706, 'learning_rate': 0.0, 'epoch': 2.4}\n",
      "{'train_runtime': 2639.3835, 'train_samples_per_second': 1.819, 'train_steps_per_second': 0.057, 'train_loss': 0.9026268283526103, 'epoch': 2.4}\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show final memory and time stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Pos training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss post-training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [06:08<00:00,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid batches: 1000\n",
      "Post-training mean loss: 0.7625631860084832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure the collator and DataLoader\n",
    "collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "loader = DataLoader(\n",
    "    trainer.train_dataset, \n",
    "    batch_size=2,  # Chosen batch size\n",
    "    collate_fn=collator, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Variables to store the total loss and the number of valid batches\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculation to save memory\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Calculating loss post-training\"):\n",
    "        # Move the batch to GPU (if available)\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # Check if the loss is NaN\n",
    "        if not torch.isnan(outputs.loss):\n",
    "            total_loss += outputs.loss.item()\n",
    "            num_batches += 1\n",
    "            # print(outputs.loss.item())\n",
    "\n",
    "print(f\"Number of valid batches: {num_batches}\")\n",
    "\n",
    "# Calculate the average loss\n",
    "if num_batches > 0:\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f\"Post-training mean loss: {average_loss}\")\n",
    "else:\n",
    "    print(\"No valid batches found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question with option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Which decoding algorithm for low-density parity-check codes involves a complex non-linear function in the check-node processing?\\nOptions:\\noption 1: Normalized min-sum\\noption 2: Off-set min-sum\\noption 3: Belief propagation\\noption 4: Successive cancellation\\noption 5: Sequential decoding\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]['conversations'][0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dataset[5]['conversations'][0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQuestion: Which decoding algorithm for low-density parity-check codes involves a complex non-linear function in the check-node processing?\\nOptions:\\noption 1: Normalized min-sum\\noption 2: Off-set min-sum\\noption 3: Belief propagation\\noption 4: Successive cancellation\\noption 5: Sequential decoding\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAnswer: option 3: Belief propagation\\nExplanation: Belief propagation decoding involves complex non-linear functions in the check-node processing.<|eot_id|>']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"user\", \"content\": \"How much is 1+1?\"},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question with no option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the purpose of sequence and cyclic shift hopping in PUCCH formats 0, 1, 3, and 4? [3GPP Release 17]'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['conversations'][0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dataset[0]['conversations'][0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the purpose of sequence and cyclic shift hopping in PUCCH formats 0, 1, 3, and 4? [3GPP Release 17]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSequence and cyclic shift hopping is used in PUCCH formats 0, 1, 3, and 4 to transmit data sequences over frequency domain.<|eot_id|>']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"user\", \"content\": \"How much is 1+1?\"},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Models/llama_3.2_FT_lora_2000_questions/tokenizer_config.json',\n",
       " '../Models/llama_3.2_FT_lora_2000_questions/special_tokens_map.json',\n",
       " '../Models/llama_3.2_FT_lora_2000_questions/tokenizer.json')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"../Models/llama_3.2_FT_lora_2000_questions\", safe_serialization=False)\n",
    "tokenizer.save_pretrained(\"../Models/llama_3.2_FT_lora_2000_questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_merged(\"model_3.2_lora_4bits\", tokenizer, save_method = \"merged_16bit\",)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.10.6: Fast Llama patching. Transformers = 4.46.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU. Max memory: 3.712 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.6. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.10.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../Models/llama_3.2_FT_lora_2000_questions\"\n",
    "# model_path = \"model_3.2_lora_4bits\"\n",
    "\n",
    "# Carregar o modelo e o tokenizador separadamente\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQuestion: Which decoding algorithm for low-density parity-check codes involves a complex non-linear function in the check-node processing?\\nOptions:\\noption 1: Normalized min-sum\\noption 2: Off-set min-sum\\noption 3: Belief propagation\\noption 4: Successive cancellation\\noption 5: Sequential decoding\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAnswer: option 3: Belief propagation\\nExplanation: The belief propagation algorithm for low-density parity-check codes involves a complex non-linear function in the check-node processing, specifically the Q-function.<|eot_id|>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "question = dataset[5]['conversations'][0]['value']\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"user\", \"content\": \"How much is 1+1?\"},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
