{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.4,
  "eval_steps": 500,
  "global_step": 300,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008,
      "grad_norm": 1.6106375455856323,
      "learning_rate": 4e-05,
      "loss": 2.2714,
      "step": 1
    },
    {
      "epoch": 0.016,
      "grad_norm": 1.9808603525161743,
      "learning_rate": 8e-05,
      "loss": 1.7244,
      "step": 2
    },
    {
      "epoch": 0.024,
      "grad_norm": 1.5503244400024414,
      "learning_rate": 0.00012,
      "loss": 2.7361,
      "step": 3
    },
    {
      "epoch": 0.032,
      "grad_norm": 1.911511778831482,
      "learning_rate": 0.00016,
      "loss": 2.2551,
      "step": 4
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.9331849813461304,
      "learning_rate": 0.0002,
      "loss": 2.4328,
      "step": 5
    },
    {
      "epoch": 0.048,
      "grad_norm": 1.5289150476455688,
      "learning_rate": 0.0001993220338983051,
      "loss": 1.8712,
      "step": 6
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.9005128741264343,
      "learning_rate": 0.00019864406779661017,
      "loss": 2.2262,
      "step": 7
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.9030927419662476,
      "learning_rate": 0.00019796610169491526,
      "loss": 1.7621,
      "step": 8
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.8726301193237305,
      "learning_rate": 0.00019728813559322035,
      "loss": 1.7585,
      "step": 9
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.9981874227523804,
      "learning_rate": 0.00019661016949152545,
      "loss": 2.0563,
      "step": 10
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.9257978200912476,
      "learning_rate": 0.0001959322033898305,
      "loss": 1.3275,
      "step": 11
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.7058268189430237,
      "learning_rate": 0.0001952542372881356,
      "loss": 0.9804,
      "step": 12
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.8421456217765808,
      "learning_rate": 0.0001945762711864407,
      "loss": 1.3489,
      "step": 13
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.8374772071838379,
      "learning_rate": 0.0001938983050847458,
      "loss": 1.932,
      "step": 14
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.9490576982498169,
      "learning_rate": 0.00019322033898305085,
      "loss": 1.6739,
      "step": 15
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.9021246433258057,
      "learning_rate": 0.00019254237288135595,
      "loss": 1.4674,
      "step": 16
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.9152685403823853,
      "learning_rate": 0.000191864406779661,
      "loss": 1.4888,
      "step": 17
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.9558353424072266,
      "learning_rate": 0.0001911864406779661,
      "loss": 1.2274,
      "step": 18
    },
    {
      "epoch": 0.152,
      "grad_norm": 1.0959396362304688,
      "learning_rate": 0.0001905084745762712,
      "loss": 1.5371,
      "step": 19
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.8095158338546753,
      "learning_rate": 0.0001898305084745763,
      "loss": 1.5466,
      "step": 20
    },
    {
      "epoch": 0.168,
      "grad_norm": 1.0013095140457153,
      "learning_rate": 0.00018915254237288136,
      "loss": 1.0089,
      "step": 21
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.8801665306091309,
      "learning_rate": 0.00018847457627118645,
      "loss": 1.3495,
      "step": 22
    },
    {
      "epoch": 0.184,
      "grad_norm": 1.028415322303772,
      "learning_rate": 0.00018779661016949151,
      "loss": 1.4418,
      "step": 23
    },
    {
      "epoch": 0.192,
      "grad_norm": 1.121482253074646,
      "learning_rate": 0.00018711864406779663,
      "loss": 1.4526,
      "step": 24
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.9805183410644531,
      "learning_rate": 0.0001864406779661017,
      "loss": 1.5776,
      "step": 25
    },
    {
      "epoch": 0.208,
      "grad_norm": 1.1853283643722534,
      "learning_rate": 0.0001857627118644068,
      "loss": 1.8553,
      "step": 26
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.8680059313774109,
      "learning_rate": 0.00018508474576271186,
      "loss": 1.7438,
      "step": 27
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.8434310555458069,
      "learning_rate": 0.00018440677966101695,
      "loss": 1.241,
      "step": 28
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.9591973423957825,
      "learning_rate": 0.00018372881355932204,
      "loss": 1.8508,
      "step": 29
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.046636939048767,
      "learning_rate": 0.00018305084745762714,
      "loss": 1.7883,
      "step": 30
    },
    {
      "epoch": 0.248,
      "grad_norm": 1.225361704826355,
      "learning_rate": 0.0001823728813559322,
      "loss": 1.6957,
      "step": 31
    },
    {
      "epoch": 0.256,
      "grad_norm": 1.3202846050262451,
      "learning_rate": 0.0001816949152542373,
      "loss": 1.9041,
      "step": 32
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.7697598934173584,
      "learning_rate": 0.00018101694915254239,
      "loss": 1.5471,
      "step": 33
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.933994472026825,
      "learning_rate": 0.00018033898305084748,
      "loss": 1.5953,
      "step": 34
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.9306190609931946,
      "learning_rate": 0.00017966101694915257,
      "loss": 1.6441,
      "step": 35
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.8173523545265198,
      "learning_rate": 0.00017898305084745764,
      "loss": 1.3782,
      "step": 36
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.9672834873199463,
      "learning_rate": 0.00017830508474576273,
      "loss": 1.4486,
      "step": 37
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.7846447229385376,
      "learning_rate": 0.0001776271186440678,
      "loss": 1.531,
      "step": 38
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.7969328761100769,
      "learning_rate": 0.0001769491525423729,
      "loss": 1.6737,
      "step": 39
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.0558996200561523,
      "learning_rate": 0.00017627118644067798,
      "loss": 1.5925,
      "step": 40
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.8502400517463684,
      "learning_rate": 0.00017559322033898307,
      "loss": 1.1357,
      "step": 41
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.9371669888496399,
      "learning_rate": 0.00017491525423728814,
      "loss": 2.1521,
      "step": 42
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.922394871711731,
      "learning_rate": 0.00017423728813559323,
      "loss": 1.235,
      "step": 43
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.7402117848396301,
      "learning_rate": 0.0001735593220338983,
      "loss": 1.2949,
      "step": 44
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.858314573764801,
      "learning_rate": 0.00017288135593220342,
      "loss": 1.1458,
      "step": 45
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.7905563116073608,
      "learning_rate": 0.00017220338983050848,
      "loss": 1.4998,
      "step": 46
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.880731999874115,
      "learning_rate": 0.00017152542372881357,
      "loss": 1.5999,
      "step": 47
    },
    {
      "epoch": 0.384,
      "grad_norm": 1.0839818716049194,
      "learning_rate": 0.00017084745762711864,
      "loss": 1.7376,
      "step": 48
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.8112084865570068,
      "learning_rate": 0.00017016949152542373,
      "loss": 1.4504,
      "step": 49
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.8896393179893494,
      "learning_rate": 0.00016949152542372882,
      "loss": 1.5303,
      "step": 50
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.9502297639846802,
      "learning_rate": 0.00016881355932203392,
      "loss": 1.7827,
      "step": 51
    },
    {
      "epoch": 0.416,
      "grad_norm": 1.140242576599121,
      "learning_rate": 0.00016813559322033898,
      "loss": 2.0265,
      "step": 52
    },
    {
      "epoch": 0.424,
      "grad_norm": 1.0263473987579346,
      "learning_rate": 0.00016745762711864408,
      "loss": 1.5923,
      "step": 53
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.8802300691604614,
      "learning_rate": 0.00016677966101694914,
      "loss": 1.5797,
      "step": 54
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.9786914587020874,
      "learning_rate": 0.00016610169491525423,
      "loss": 1.8881,
      "step": 55
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.9943042397499084,
      "learning_rate": 0.00016542372881355933,
      "loss": 1.7013,
      "step": 56
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.7802377343177795,
      "learning_rate": 0.00016474576271186442,
      "loss": 1.4216,
      "step": 57
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.9149603843688965,
      "learning_rate": 0.00016406779661016948,
      "loss": 1.7269,
      "step": 58
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.7391743063926697,
      "learning_rate": 0.00016338983050847458,
      "loss": 2.0013,
      "step": 59
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.7497445940971375,
      "learning_rate": 0.00016271186440677967,
      "loss": 1.4691,
      "step": 60
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.9572010636329651,
      "learning_rate": 0.00016203389830508476,
      "loss": 1.2397,
      "step": 61
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.8301321864128113,
      "learning_rate": 0.00016135593220338985,
      "loss": 1.5105,
      "step": 62
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.8758363723754883,
      "learning_rate": 0.00016067796610169492,
      "loss": 1.7342,
      "step": 63
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.946755588054657,
      "learning_rate": 0.00016,
      "loss": 1.3389,
      "step": 64
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.9913042783737183,
      "learning_rate": 0.00015932203389830508,
      "loss": 1.8166,
      "step": 65
    },
    {
      "epoch": 0.528,
      "grad_norm": 1.0553890466690063,
      "learning_rate": 0.0001586440677966102,
      "loss": 1.4744,
      "step": 66
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.918673038482666,
      "learning_rate": 0.00015796610169491526,
      "loss": 1.1514,
      "step": 67
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.776348888874054,
      "learning_rate": 0.00015728813559322036,
      "loss": 1.1658,
      "step": 68
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.8255602717399597,
      "learning_rate": 0.00015661016949152542,
      "loss": 1.5177,
      "step": 69
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.2050896883010864,
      "learning_rate": 0.00015593220338983051,
      "loss": 1.3972,
      "step": 70
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.8003722429275513,
      "learning_rate": 0.0001552542372881356,
      "loss": 1.4798,
      "step": 71
    },
    {
      "epoch": 0.576,
      "grad_norm": 1.1021289825439453,
      "learning_rate": 0.0001545762711864407,
      "loss": 1.4737,
      "step": 72
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.9348180890083313,
      "learning_rate": 0.00015389830508474577,
      "loss": 1.5236,
      "step": 73
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.9684078693389893,
      "learning_rate": 0.00015322033898305086,
      "loss": 2.0122,
      "step": 74
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.9739277362823486,
      "learning_rate": 0.00015254237288135592,
      "loss": 1.7801,
      "step": 75
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.8061067461967468,
      "learning_rate": 0.00015186440677966102,
      "loss": 1.7703,
      "step": 76
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.8148946762084961,
      "learning_rate": 0.0001511864406779661,
      "loss": 1.4306,
      "step": 77
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.8548436164855957,
      "learning_rate": 0.0001505084745762712,
      "loss": 1.5852,
      "step": 78
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.7965044975280762,
      "learning_rate": 0.00014983050847457627,
      "loss": 1.1754,
      "step": 79
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.0639348030090332,
      "learning_rate": 0.00014915254237288136,
      "loss": 1.9868,
      "step": 80
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.9654049873352051,
      "learning_rate": 0.00014847457627118645,
      "loss": 1.307,
      "step": 81
    },
    {
      "epoch": 0.656,
      "grad_norm": 1.055154800415039,
      "learning_rate": 0.00014779661016949154,
      "loss": 1.2391,
      "step": 82
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.737607479095459,
      "learning_rate": 0.0001471186440677966,
      "loss": 1.5035,
      "step": 83
    },
    {
      "epoch": 0.672,
      "grad_norm": 1.081926941871643,
      "learning_rate": 0.0001464406779661017,
      "loss": 1.8437,
      "step": 84
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.7827016711235046,
      "learning_rate": 0.00014576271186440677,
      "loss": 1.5491,
      "step": 85
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.866194486618042,
      "learning_rate": 0.00014508474576271186,
      "loss": 1.18,
      "step": 86
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.7850391268730164,
      "learning_rate": 0.00014440677966101695,
      "loss": 1.5501,
      "step": 87
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.6321282386779785,
      "learning_rate": 0.00014372881355932205,
      "loss": 1.1585,
      "step": 88
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.9258256554603577,
      "learning_rate": 0.00014305084745762714,
      "loss": 1.4765,
      "step": 89
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.037558913230896,
      "learning_rate": 0.0001423728813559322,
      "loss": 1.8039,
      "step": 90
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.9300054907798767,
      "learning_rate": 0.0001416949152542373,
      "loss": 1.5329,
      "step": 91
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.7929487824440002,
      "learning_rate": 0.0001410169491525424,
      "loss": 1.4819,
      "step": 92
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.8717362284660339,
      "learning_rate": 0.00014033898305084748,
      "loss": 1.2492,
      "step": 93
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.8389996290206909,
      "learning_rate": 0.00013966101694915255,
      "loss": 1.4126,
      "step": 94
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.0533819198608398,
      "learning_rate": 0.00013898305084745764,
      "loss": 1.8214,
      "step": 95
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.9426549673080444,
      "learning_rate": 0.0001383050847457627,
      "loss": 2.0461,
      "step": 96
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.769136369228363,
      "learning_rate": 0.0001376271186440678,
      "loss": 1.1104,
      "step": 97
    },
    {
      "epoch": 0.784,
      "grad_norm": 1.05534029006958,
      "learning_rate": 0.0001369491525423729,
      "loss": 1.7071,
      "step": 98
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.8809515237808228,
      "learning_rate": 0.00013627118644067798,
      "loss": 1.7688,
      "step": 99
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.8896448612213135,
      "learning_rate": 0.00013559322033898305,
      "loss": 1.5267,
      "step": 100
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.8760001063346863,
      "learning_rate": 0.00013491525423728814,
      "loss": 1.4693,
      "step": 101
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.9386512041091919,
      "learning_rate": 0.0001342372881355932,
      "loss": 1.255,
      "step": 102
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.6737567782402039,
      "learning_rate": 0.00013355932203389833,
      "loss": 1.1716,
      "step": 103
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.8574091792106628,
      "learning_rate": 0.0001328813559322034,
      "loss": 1.7503,
      "step": 104
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.9673986434936523,
      "learning_rate": 0.00013220338983050849,
      "loss": 1.6144,
      "step": 105
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.7799674272537231,
      "learning_rate": 0.00013152542372881355,
      "loss": 1.2303,
      "step": 106
    },
    {
      "epoch": 0.856,
      "grad_norm": 1.482923150062561,
      "learning_rate": 0.00013084745762711864,
      "loss": 1.9795,
      "step": 107
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.9852720499038696,
      "learning_rate": 0.00013016949152542374,
      "loss": 1.434,
      "step": 108
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.9407005310058594,
      "learning_rate": 0.00012949152542372883,
      "loss": 1.4477,
      "step": 109
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.8551958799362183,
      "learning_rate": 0.0001288135593220339,
      "loss": 1.3108,
      "step": 110
    },
    {
      "epoch": 0.888,
      "grad_norm": 1.0088306665420532,
      "learning_rate": 0.000128135593220339,
      "loss": 1.5392,
      "step": 111
    },
    {
      "epoch": 0.896,
      "grad_norm": 1.1464316844940186,
      "learning_rate": 0.00012745762711864405,
      "loss": 1.4459,
      "step": 112
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.8406959772109985,
      "learning_rate": 0.00012677966101694917,
      "loss": 1.3542,
      "step": 113
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.8320776224136353,
      "learning_rate": 0.00012610169491525426,
      "loss": 1.5628,
      "step": 114
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.0167078971862793,
      "learning_rate": 0.00012542372881355933,
      "loss": 2.1952,
      "step": 115
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.7328945994377136,
      "learning_rate": 0.00012474576271186442,
      "loss": 1.3545,
      "step": 116
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.8335226774215698,
      "learning_rate": 0.0001240677966101695,
      "loss": 1.3452,
      "step": 117
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.9876905679702759,
      "learning_rate": 0.00012338983050847458,
      "loss": 1.9274,
      "step": 118
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.9999080300331116,
      "learning_rate": 0.00012271186440677967,
      "loss": 1.9823,
      "step": 119
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.9803486466407776,
      "learning_rate": 0.00012203389830508477,
      "loss": 1.1052,
      "step": 120
    },
    {
      "epoch": 0.968,
      "grad_norm": 1.037550926208496,
      "learning_rate": 0.00012135593220338983,
      "loss": 1.4613,
      "step": 121
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.8493233919143677,
      "learning_rate": 0.00012067796610169492,
      "loss": 1.7153,
      "step": 122
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.8539213538169861,
      "learning_rate": 0.00012,
      "loss": 1.9196,
      "step": 123
    },
    {
      "epoch": 0.992,
      "grad_norm": 1.0253887176513672,
      "learning_rate": 0.0001193220338983051,
      "loss": 1.455,
      "step": 124
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7905411124229431,
      "learning_rate": 0.00011864406779661017,
      "loss": 0.9126,
      "step": 125
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.9749678373336792,
      "learning_rate": 0.00011796610169491527,
      "loss": 1.367,
      "step": 126
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.6988453269004822,
      "learning_rate": 0.00011728813559322033,
      "loss": 1.3999,
      "step": 127
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.9182418584823608,
      "learning_rate": 0.00011661016949152544,
      "loss": 1.4427,
      "step": 128
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.8632371425628662,
      "learning_rate": 0.0001159322033898305,
      "loss": 0.8508,
      "step": 129
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.8229799270629883,
      "learning_rate": 0.0001152542372881356,
      "loss": 1.468,
      "step": 130
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.9632782340049744,
      "learning_rate": 0.00011457627118644068,
      "loss": 1.5204,
      "step": 131
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.7916066646575928,
      "learning_rate": 0.00011389830508474577,
      "loss": 1.328,
      "step": 132
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.8636873364448547,
      "learning_rate": 0.00011322033898305085,
      "loss": 1.1021,
      "step": 133
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.8152046799659729,
      "learning_rate": 0.00011254237288135594,
      "loss": 1.3209,
      "step": 134
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.8921478986740112,
      "learning_rate": 0.00011186440677966102,
      "loss": 1.2596,
      "step": 135
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.8939640522003174,
      "learning_rate": 0.00011118644067796611,
      "loss": 1.5699,
      "step": 136
    },
    {
      "epoch": 1.096,
      "grad_norm": 1.0619823932647705,
      "learning_rate": 0.00011050847457627118,
      "loss": 1.3594,
      "step": 137
    },
    {
      "epoch": 1.104,
      "grad_norm": 1.039967656135559,
      "learning_rate": 0.00010983050847457627,
      "loss": 1.2468,
      "step": 138
    },
    {
      "epoch": 1.112,
      "grad_norm": 1.097264051437378,
      "learning_rate": 0.00010915254237288135,
      "loss": 1.4662,
      "step": 139
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.9763365983963013,
      "learning_rate": 0.00010847457627118644,
      "loss": 1.3336,
      "step": 140
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.9436061382293701,
      "learning_rate": 0.00010779661016949153,
      "loss": 1.0954,
      "step": 141
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 1.0987873077392578,
      "learning_rate": 0.00010711864406779661,
      "loss": 1.6767,
      "step": 142
    },
    {
      "epoch": 1.144,
      "grad_norm": 1.1282151937484741,
      "learning_rate": 0.0001064406779661017,
      "loss": 1.2511,
      "step": 143
    },
    {
      "epoch": 1.152,
      "grad_norm": 1.1000876426696777,
      "learning_rate": 0.00010576271186440679,
      "loss": 1.1135,
      "step": 144
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.0238397121429443,
      "learning_rate": 0.00010508474576271188,
      "loss": 1.0104,
      "step": 145
    },
    {
      "epoch": 1.168,
      "grad_norm": 1.1322201490402222,
      "learning_rate": 0.00010440677966101696,
      "loss": 1.5412,
      "step": 146
    },
    {
      "epoch": 1.176,
      "grad_norm": 1.198483943939209,
      "learning_rate": 0.00010372881355932205,
      "loss": 1.2444,
      "step": 147
    },
    {
      "epoch": 1.184,
      "grad_norm": 1.4379099607467651,
      "learning_rate": 0.00010305084745762712,
      "loss": 1.4624,
      "step": 148
    },
    {
      "epoch": 1.192,
      "grad_norm": 1.090644121170044,
      "learning_rate": 0.00010237288135593222,
      "loss": 1.2478,
      "step": 149
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.0392913818359375,
      "learning_rate": 0.00010169491525423729,
      "loss": 0.9265,
      "step": 150
    },
    {
      "epoch": 1.208,
      "grad_norm": 1.1010851860046387,
      "learning_rate": 0.00010101694915254238,
      "loss": 1.1383,
      "step": 151
    },
    {
      "epoch": 1.216,
      "grad_norm": 1.040676474571228,
      "learning_rate": 0.00010033898305084746,
      "loss": 1.3621,
      "step": 152
    },
    {
      "epoch": 1.224,
      "grad_norm": 1.3732011318206787,
      "learning_rate": 9.966101694915255e-05,
      "loss": 1.4862,
      "step": 153
    },
    {
      "epoch": 1.232,
      "grad_norm": 1.1461024284362793,
      "learning_rate": 9.898305084745763e-05,
      "loss": 1.7042,
      "step": 154
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.3142528533935547,
      "learning_rate": 9.830508474576272e-05,
      "loss": 1.2014,
      "step": 155
    },
    {
      "epoch": 1.248,
      "grad_norm": 1.209840178489685,
      "learning_rate": 9.76271186440678e-05,
      "loss": 0.7174,
      "step": 156
    },
    {
      "epoch": 1.256,
      "grad_norm": 1.1083582639694214,
      "learning_rate": 9.69491525423729e-05,
      "loss": 1.4362,
      "step": 157
    },
    {
      "epoch": 1.264,
      "grad_norm": 1.0933632850646973,
      "learning_rate": 9.627118644067797e-05,
      "loss": 1.1649,
      "step": 158
    },
    {
      "epoch": 1.272,
      "grad_norm": 1.2215474843978882,
      "learning_rate": 9.559322033898305e-05,
      "loss": 1.0342,
      "step": 159
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.2638568878173828,
      "learning_rate": 9.491525423728815e-05,
      "loss": 1.1975,
      "step": 160
    },
    {
      "epoch": 1.288,
      "grad_norm": 1.5259548425674438,
      "learning_rate": 9.423728813559322e-05,
      "loss": 1.3132,
      "step": 161
    },
    {
      "epoch": 1.296,
      "grad_norm": 1.2769147157669067,
      "learning_rate": 9.355932203389832e-05,
      "loss": 1.5862,
      "step": 162
    },
    {
      "epoch": 1.304,
      "grad_norm": 1.3069474697113037,
      "learning_rate": 9.28813559322034e-05,
      "loss": 1.4284,
      "step": 163
    },
    {
      "epoch": 1.312,
      "grad_norm": 1.2991653680801392,
      "learning_rate": 9.220338983050847e-05,
      "loss": 1.6259,
      "step": 164
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.2668266296386719,
      "learning_rate": 9.152542372881357e-05,
      "loss": 1.2513,
      "step": 165
    },
    {
      "epoch": 1.328,
      "grad_norm": 1.4460984468460083,
      "learning_rate": 9.084745762711865e-05,
      "loss": 1.5889,
      "step": 166
    },
    {
      "epoch": 1.336,
      "grad_norm": 1.5618098974227905,
      "learning_rate": 9.016949152542374e-05,
      "loss": 1.4797,
      "step": 167
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 1.814680576324463,
      "learning_rate": 8.949152542372882e-05,
      "loss": 1.5348,
      "step": 168
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 1.1733258962631226,
      "learning_rate": 8.88135593220339e-05,
      "loss": 0.8169,
      "step": 169
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.823862910270691,
      "learning_rate": 8.813559322033899e-05,
      "loss": 1.6396,
      "step": 170
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 1.9289110898971558,
      "learning_rate": 8.745762711864407e-05,
      "loss": 1.3837,
      "step": 171
    },
    {
      "epoch": 1.376,
      "grad_norm": 1.272768497467041,
      "learning_rate": 8.677966101694915e-05,
      "loss": 1.1086,
      "step": 172
    },
    {
      "epoch": 1.384,
      "grad_norm": 1.1311992406845093,
      "learning_rate": 8.610169491525424e-05,
      "loss": 1.0683,
      "step": 173
    },
    {
      "epoch": 1.392,
      "grad_norm": 1.3855431079864502,
      "learning_rate": 8.542372881355932e-05,
      "loss": 1.0617,
      "step": 174
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.112266182899475,
      "learning_rate": 8.474576271186441e-05,
      "loss": 1.0009,
      "step": 175
    },
    {
      "epoch": 1.408,
      "grad_norm": 1.3659093379974365,
      "learning_rate": 8.406779661016949e-05,
      "loss": 1.4282,
      "step": 176
    },
    {
      "epoch": 1.416,
      "grad_norm": 1.3097859621047974,
      "learning_rate": 8.338983050847457e-05,
      "loss": 1.2043,
      "step": 177
    },
    {
      "epoch": 1.424,
      "grad_norm": 1.449746012687683,
      "learning_rate": 8.271186440677966e-05,
      "loss": 1.5414,
      "step": 178
    },
    {
      "epoch": 1.432,
      "grad_norm": 1.4596872329711914,
      "learning_rate": 8.203389830508474e-05,
      "loss": 1.2093,
      "step": 179
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.5172220468521118,
      "learning_rate": 8.135593220338983e-05,
      "loss": 1.4399,
      "step": 180
    },
    {
      "epoch": 1.448,
      "grad_norm": 1.3011935949325562,
      "learning_rate": 8.067796610169493e-05,
      "loss": 1.4322,
      "step": 181
    },
    {
      "epoch": 1.456,
      "grad_norm": 1.4637683629989624,
      "learning_rate": 8e-05,
      "loss": 1.6313,
      "step": 182
    },
    {
      "epoch": 1.464,
      "grad_norm": 1.6741461753845215,
      "learning_rate": 7.93220338983051e-05,
      "loss": 1.9118,
      "step": 183
    },
    {
      "epoch": 1.472,
      "grad_norm": 1.5757449865341187,
      "learning_rate": 7.864406779661018e-05,
      "loss": 1.4451,
      "step": 184
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.0834635496139526,
      "learning_rate": 7.796610169491526e-05,
      "loss": 0.8783,
      "step": 185
    },
    {
      "epoch": 1.488,
      "grad_norm": 1.2021197080612183,
      "learning_rate": 7.728813559322035e-05,
      "loss": 1.4236,
      "step": 186
    },
    {
      "epoch": 1.496,
      "grad_norm": 1.3722227811813354,
      "learning_rate": 7.661016949152543e-05,
      "loss": 1.4699,
      "step": 187
    },
    {
      "epoch": 1.504,
      "grad_norm": 1.348251223564148,
      "learning_rate": 7.593220338983051e-05,
      "loss": 1.3709,
      "step": 188
    },
    {
      "epoch": 1.512,
      "grad_norm": 1.2403806447982788,
      "learning_rate": 7.52542372881356e-05,
      "loss": 0.9617,
      "step": 189
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.5304477214813232,
      "learning_rate": 7.457627118644068e-05,
      "loss": 1.3058,
      "step": 190
    },
    {
      "epoch": 1.528,
      "grad_norm": 1.4213664531707764,
      "learning_rate": 7.389830508474577e-05,
      "loss": 1.1576,
      "step": 191
    },
    {
      "epoch": 1.536,
      "grad_norm": 1.5391027927398682,
      "learning_rate": 7.322033898305085e-05,
      "loss": 1.3184,
      "step": 192
    },
    {
      "epoch": 1.544,
      "grad_norm": 1.3493571281433105,
      "learning_rate": 7.254237288135593e-05,
      "loss": 0.9381,
      "step": 193
    },
    {
      "epoch": 1.552,
      "grad_norm": 1.1653602123260498,
      "learning_rate": 7.186440677966102e-05,
      "loss": 1.1126,
      "step": 194
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.3546686172485352,
      "learning_rate": 7.11864406779661e-05,
      "loss": 1.0661,
      "step": 195
    },
    {
      "epoch": 1.568,
      "grad_norm": 1.497628927230835,
      "learning_rate": 7.05084745762712e-05,
      "loss": 1.2729,
      "step": 196
    },
    {
      "epoch": 1.576,
      "grad_norm": 1.6061534881591797,
      "learning_rate": 6.983050847457627e-05,
      "loss": 1.3166,
      "step": 197
    },
    {
      "epoch": 1.584,
      "grad_norm": 1.6730440855026245,
      "learning_rate": 6.915254237288135e-05,
      "loss": 1.079,
      "step": 198
    },
    {
      "epoch": 1.592,
      "grad_norm": 1.579337239265442,
      "learning_rate": 6.847457627118645e-05,
      "loss": 0.8202,
      "step": 199
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.4662604331970215,
      "learning_rate": 6.779661016949152e-05,
      "loss": 1.0984,
      "step": 200
    },
    {
      "epoch": 1.608,
      "grad_norm": 1.2078368663787842,
      "learning_rate": 6.71186440677966e-05,
      "loss": 1.2341,
      "step": 201
    },
    {
      "epoch": 1.616,
      "grad_norm": 1.3794220685958862,
      "learning_rate": 6.64406779661017e-05,
      "loss": 0.8729,
      "step": 202
    },
    {
      "epoch": 1.624,
      "grad_norm": 1.2723037004470825,
      "learning_rate": 6.576271186440678e-05,
      "loss": 0.9005,
      "step": 203
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 1.4340308904647827,
      "learning_rate": 6.508474576271187e-05,
      "loss": 0.9654,
      "step": 204
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.3395947217941284,
      "learning_rate": 6.440677966101695e-05,
      "loss": 1.0357,
      "step": 205
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 1.4104552268981934,
      "learning_rate": 6.372881355932203e-05,
      "loss": 1.0522,
      "step": 206
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 1.3921483755111694,
      "learning_rate": 6.305084745762713e-05,
      "loss": 1.241,
      "step": 207
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 1.485031008720398,
      "learning_rate": 6.237288135593221e-05,
      "loss": 0.8863,
      "step": 208
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 1.3653011322021484,
      "learning_rate": 6.169491525423729e-05,
      "loss": 0.9431,
      "step": 209
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.3176833391189575,
      "learning_rate": 6.101694915254238e-05,
      "loss": 1.3358,
      "step": 210
    },
    {
      "epoch": 1.688,
      "grad_norm": 1.120706558227539,
      "learning_rate": 6.033898305084746e-05,
      "loss": 0.8255,
      "step": 211
    },
    {
      "epoch": 1.696,
      "grad_norm": 1.4554377794265747,
      "learning_rate": 5.966101694915255e-05,
      "loss": 1.0571,
      "step": 212
    },
    {
      "epoch": 1.704,
      "grad_norm": 1.5911868810653687,
      "learning_rate": 5.8983050847457634e-05,
      "loss": 1.5727,
      "step": 213
    },
    {
      "epoch": 1.712,
      "grad_norm": 1.4930588006973267,
      "learning_rate": 5.830508474576272e-05,
      "loss": 1.414,
      "step": 214
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.440727949142456,
      "learning_rate": 5.76271186440678e-05,
      "loss": 1.1016,
      "step": 215
    },
    {
      "epoch": 1.728,
      "grad_norm": 1.6269129514694214,
      "learning_rate": 5.6949152542372884e-05,
      "loss": 1.228,
      "step": 216
    },
    {
      "epoch": 1.736,
      "grad_norm": 1.3461189270019531,
      "learning_rate": 5.627118644067797e-05,
      "loss": 0.6708,
      "step": 217
    },
    {
      "epoch": 1.744,
      "grad_norm": 1.3805351257324219,
      "learning_rate": 5.5593220338983056e-05,
      "loss": 1.1049,
      "step": 218
    },
    {
      "epoch": 1.752,
      "grad_norm": 1.7782272100448608,
      "learning_rate": 5.4915254237288135e-05,
      "loss": 1.2766,
      "step": 219
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.7207728624343872,
      "learning_rate": 5.423728813559322e-05,
      "loss": 1.0814,
      "step": 220
    },
    {
      "epoch": 1.768,
      "grad_norm": 1.522964358329773,
      "learning_rate": 5.355932203389831e-05,
      "loss": 1.4117,
      "step": 221
    },
    {
      "epoch": 1.776,
      "grad_norm": 1.3202263116836548,
      "learning_rate": 5.288135593220339e-05,
      "loss": 1.2186,
      "step": 222
    },
    {
      "epoch": 1.784,
      "grad_norm": 1.3563847541809082,
      "learning_rate": 5.220338983050848e-05,
      "loss": 1.1565,
      "step": 223
    },
    {
      "epoch": 1.792,
      "grad_norm": 1.3545503616333008,
      "learning_rate": 5.152542372881356e-05,
      "loss": 0.9566,
      "step": 224
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.512083888053894,
      "learning_rate": 5.0847457627118643e-05,
      "loss": 1.2956,
      "step": 225
    },
    {
      "epoch": 1.808,
      "grad_norm": 1.5350672006607056,
      "learning_rate": 5.016949152542373e-05,
      "loss": 1.2174,
      "step": 226
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 1.6135767698287964,
      "learning_rate": 4.9491525423728815e-05,
      "loss": 1.5423,
      "step": 227
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 1.2207740545272827,
      "learning_rate": 4.88135593220339e-05,
      "loss": 0.9142,
      "step": 228
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 1.460485577583313,
      "learning_rate": 4.813559322033899e-05,
      "loss": 1.3507,
      "step": 229
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.7923144102096558,
      "learning_rate": 4.745762711864407e-05,
      "loss": 1.5029,
      "step": 230
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 1.3379323482513428,
      "learning_rate": 4.677966101694916e-05,
      "loss": 1.2573,
      "step": 231
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 1.469694972038269,
      "learning_rate": 4.610169491525424e-05,
      "loss": 1.2368,
      "step": 232
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 1.5394262075424194,
      "learning_rate": 4.542372881355932e-05,
      "loss": 1.3684,
      "step": 233
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 1.7943748235702515,
      "learning_rate": 4.474576271186441e-05,
      "loss": 1.1494,
      "step": 234
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.260443925857544,
      "learning_rate": 4.4067796610169495e-05,
      "loss": 1.3281,
      "step": 235
    },
    {
      "epoch": 1.888,
      "grad_norm": 1.169546365737915,
      "learning_rate": 4.3389830508474574e-05,
      "loss": 0.5091,
      "step": 236
    },
    {
      "epoch": 1.896,
      "grad_norm": 1.337491750717163,
      "learning_rate": 4.271186440677966e-05,
      "loss": 1.5051,
      "step": 237
    },
    {
      "epoch": 1.904,
      "grad_norm": 1.661220669746399,
      "learning_rate": 4.2033898305084746e-05,
      "loss": 1.0304,
      "step": 238
    },
    {
      "epoch": 1.912,
      "grad_norm": 2.034637689590454,
      "learning_rate": 4.135593220338983e-05,
      "loss": 1.3837,
      "step": 239
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.4352614879608154,
      "learning_rate": 4.067796610169492e-05,
      "loss": 0.9911,
      "step": 240
    },
    {
      "epoch": 1.928,
      "grad_norm": 1.4573091268539429,
      "learning_rate": 4e-05,
      "loss": 1.3726,
      "step": 241
    },
    {
      "epoch": 1.936,
      "grad_norm": 1.3900632858276367,
      "learning_rate": 3.932203389830509e-05,
      "loss": 1.4303,
      "step": 242
    },
    {
      "epoch": 1.944,
      "grad_norm": 1.3344241380691528,
      "learning_rate": 3.8644067796610175e-05,
      "loss": 1.0537,
      "step": 243
    },
    {
      "epoch": 1.952,
      "grad_norm": 1.619892954826355,
      "learning_rate": 3.7966101694915254e-05,
      "loss": 1.0525,
      "step": 244
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.4634268283843994,
      "learning_rate": 3.728813559322034e-05,
      "loss": 1.047,
      "step": 245
    },
    {
      "epoch": 1.968,
      "grad_norm": 1.2603552341461182,
      "learning_rate": 3.6610169491525426e-05,
      "loss": 1.1131,
      "step": 246
    },
    {
      "epoch": 1.976,
      "grad_norm": 1.2943940162658691,
      "learning_rate": 3.593220338983051e-05,
      "loss": 1.0578,
      "step": 247
    },
    {
      "epoch": 1.984,
      "grad_norm": 1.5254689455032349,
      "learning_rate": 3.52542372881356e-05,
      "loss": 1.0878,
      "step": 248
    },
    {
      "epoch": 1.992,
      "grad_norm": 1.5455644130706787,
      "learning_rate": 3.4576271186440676e-05,
      "loss": 1.6046,
      "step": 249
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.4871834516525269,
      "learning_rate": 3.389830508474576e-05,
      "loss": 1.3983,
      "step": 250
    },
    {
      "epoch": 2.008,
      "grad_norm": 1.5038529634475708,
      "learning_rate": 3.322033898305085e-05,
      "loss": 0.9782,
      "step": 251
    },
    {
      "epoch": 2.016,
      "grad_norm": 1.312536358833313,
      "learning_rate": 3.2542372881355934e-05,
      "loss": 1.0096,
      "step": 252
    },
    {
      "epoch": 2.024,
      "grad_norm": 1.3531494140625,
      "learning_rate": 3.186440677966101e-05,
      "loss": 1.2062,
      "step": 253
    },
    {
      "epoch": 2.032,
      "grad_norm": 1.2077211141586304,
      "learning_rate": 3.1186440677966106e-05,
      "loss": 1.0295,
      "step": 254
    },
    {
      "epoch": 2.04,
      "grad_norm": 1.5790205001831055,
      "learning_rate": 3.050847457627119e-05,
      "loss": 1.0439,
      "step": 255
    },
    {
      "epoch": 2.048,
      "grad_norm": 1.0127264261245728,
      "learning_rate": 2.9830508474576274e-05,
      "loss": 0.7012,
      "step": 256
    },
    {
      "epoch": 2.056,
      "grad_norm": 1.148388147354126,
      "learning_rate": 2.915254237288136e-05,
      "loss": 0.5312,
      "step": 257
    },
    {
      "epoch": 2.064,
      "grad_norm": 1.4107211828231812,
      "learning_rate": 2.8474576271186442e-05,
      "loss": 1.0841,
      "step": 258
    },
    {
      "epoch": 2.072,
      "grad_norm": 1.1646870374679565,
      "learning_rate": 2.7796610169491528e-05,
      "loss": 0.9337,
      "step": 259
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.1770199537277222,
      "learning_rate": 2.711864406779661e-05,
      "loss": 0.844,
      "step": 260
    },
    {
      "epoch": 2.088,
      "grad_norm": 1.1770840883255005,
      "learning_rate": 2.6440677966101696e-05,
      "loss": 0.5273,
      "step": 261
    },
    {
      "epoch": 2.096,
      "grad_norm": 1.611702799797058,
      "learning_rate": 2.576271186440678e-05,
      "loss": 1.029,
      "step": 262
    },
    {
      "epoch": 2.104,
      "grad_norm": 1.6027580499649048,
      "learning_rate": 2.5084745762711865e-05,
      "loss": 1.1248,
      "step": 263
    },
    {
      "epoch": 2.112,
      "grad_norm": 1.6365426778793335,
      "learning_rate": 2.440677966101695e-05,
      "loss": 0.9858,
      "step": 264
    },
    {
      "epoch": 2.12,
      "grad_norm": 1.1940827369689941,
      "learning_rate": 2.3728813559322036e-05,
      "loss": 0.7251,
      "step": 265
    },
    {
      "epoch": 2.128,
      "grad_norm": 1.4978107213974,
      "learning_rate": 2.305084745762712e-05,
      "loss": 1.2281,
      "step": 266
    },
    {
      "epoch": 2.136,
      "grad_norm": 1.3789520263671875,
      "learning_rate": 2.2372881355932205e-05,
      "loss": 0.8448,
      "step": 267
    },
    {
      "epoch": 2.144,
      "grad_norm": 1.2937158346176147,
      "learning_rate": 2.1694915254237287e-05,
      "loss": 0.7036,
      "step": 268
    },
    {
      "epoch": 2.152,
      "grad_norm": 1.4137576818466187,
      "learning_rate": 2.1016949152542373e-05,
      "loss": 0.6909,
      "step": 269
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.687817096710205,
      "learning_rate": 2.033898305084746e-05,
      "loss": 1.4121,
      "step": 270
    },
    {
      "epoch": 2.168,
      "grad_norm": 1.6237659454345703,
      "learning_rate": 1.9661016949152545e-05,
      "loss": 0.8375,
      "step": 271
    },
    {
      "epoch": 2.176,
      "grad_norm": 1.5536137819290161,
      "learning_rate": 1.8983050847457627e-05,
      "loss": 1.1751,
      "step": 272
    },
    {
      "epoch": 2.184,
      "grad_norm": 1.6521198749542236,
      "learning_rate": 1.8305084745762713e-05,
      "loss": 0.9787,
      "step": 273
    },
    {
      "epoch": 2.192,
      "grad_norm": 1.4723339080810547,
      "learning_rate": 1.76271186440678e-05,
      "loss": 0.8918,
      "step": 274
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.6614420413970947,
      "learning_rate": 1.694915254237288e-05,
      "loss": 0.9165,
      "step": 275
    },
    {
      "epoch": 2.208,
      "grad_norm": 1.6922342777252197,
      "learning_rate": 1.6271186440677967e-05,
      "loss": 0.9092,
      "step": 276
    },
    {
      "epoch": 2.216,
      "grad_norm": 1.5726494789123535,
      "learning_rate": 1.5593220338983053e-05,
      "loss": 0.6947,
      "step": 277
    },
    {
      "epoch": 2.224,
      "grad_norm": 2.0240652561187744,
      "learning_rate": 1.4915254237288137e-05,
      "loss": 0.8528,
      "step": 278
    },
    {
      "epoch": 2.232,
      "grad_norm": 1.7034975290298462,
      "learning_rate": 1.4237288135593221e-05,
      "loss": 0.8251,
      "step": 279
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.5527937412261963,
      "learning_rate": 1.3559322033898305e-05,
      "loss": 0.7287,
      "step": 280
    },
    {
      "epoch": 2.248,
      "grad_norm": 1.4272499084472656,
      "learning_rate": 1.288135593220339e-05,
      "loss": 0.6858,
      "step": 281
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 2.3250112533569336,
      "learning_rate": 1.2203389830508475e-05,
      "loss": 1.4549,
      "step": 282
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 1.7096129655838013,
      "learning_rate": 1.152542372881356e-05,
      "loss": 0.9297,
      "step": 283
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 1.570917010307312,
      "learning_rate": 1.0847457627118644e-05,
      "loss": 1.0014,
      "step": 284
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 2.1060755252838135,
      "learning_rate": 1.016949152542373e-05,
      "loss": 0.8778,
      "step": 285
    },
    {
      "epoch": 2.288,
      "grad_norm": 1.4649488925933838,
      "learning_rate": 9.491525423728814e-06,
      "loss": 0.9111,
      "step": 286
    },
    {
      "epoch": 2.296,
      "grad_norm": 1.5887492895126343,
      "learning_rate": 8.8135593220339e-06,
      "loss": 0.7445,
      "step": 287
    },
    {
      "epoch": 2.304,
      "grad_norm": 2.1804747581481934,
      "learning_rate": 8.135593220338983e-06,
      "loss": 1.1057,
      "step": 288
    },
    {
      "epoch": 2.312,
      "grad_norm": 1.9596027135849,
      "learning_rate": 7.4576271186440685e-06,
      "loss": 1.4964,
      "step": 289
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.5228140354156494,
      "learning_rate": 6.779661016949153e-06,
      "loss": 0.9491,
      "step": 290
    },
    {
      "epoch": 2.328,
      "grad_norm": 1.4822771549224854,
      "learning_rate": 6.101694915254238e-06,
      "loss": 0.7548,
      "step": 291
    },
    {
      "epoch": 2.336,
      "grad_norm": 2.046403169631958,
      "learning_rate": 5.423728813559322e-06,
      "loss": 1.1776,
      "step": 292
    },
    {
      "epoch": 2.344,
      "grad_norm": 2.086979627609253,
      "learning_rate": 4.745762711864407e-06,
      "loss": 1.0578,
      "step": 293
    },
    {
      "epoch": 2.352,
      "grad_norm": 1.4615048170089722,
      "learning_rate": 4.067796610169492e-06,
      "loss": 1.1549,
      "step": 294
    },
    {
      "epoch": 2.36,
      "grad_norm": 1.688761830329895,
      "learning_rate": 3.3898305084745763e-06,
      "loss": 0.9857,
      "step": 295
    },
    {
      "epoch": 2.368,
      "grad_norm": 1.891719102859497,
      "learning_rate": 2.711864406779661e-06,
      "loss": 0.9146,
      "step": 296
    },
    {
      "epoch": 2.376,
      "grad_norm": 1.4787254333496094,
      "learning_rate": 2.033898305084746e-06,
      "loss": 0.7649,
      "step": 297
    },
    {
      "epoch": 2.384,
      "grad_norm": 1.7899492979049683,
      "learning_rate": 1.3559322033898304e-06,
      "loss": 0.7881,
      "step": 298
    },
    {
      "epoch": 2.392,
      "grad_norm": 2.170947551727295,
      "learning_rate": 6.779661016949152e-07,
      "loss": 0.994,
      "step": 299
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.6584793329238892,
      "learning_rate": 0.0,
      "loss": 0.9879,
      "step": 300
    }
  ],
  "logging_steps": 1,
  "max_steps": 300,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4227087235891200.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
