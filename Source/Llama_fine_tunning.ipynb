{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tunning using unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "CUDA Device Name:  NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "print(\"CUDA Device Name: \", torch.cuda.get_device_name(0))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verificar se CUDA est√° dispon√≠vel para acelerar o processamento\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 13:37:43.193732: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-26 13:37:43.204970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-26 13:37:43.220555: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-26 13:37:43.225181: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-26 13:37:43.237003: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-26 13:37:43.957772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.10.6: Fast Llama patching. Transformers = 4.46.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU. Max memory: 3.712 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.6. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "initial_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name=\"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # device_map=\"auto\"\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = initial_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mover o modelo para CPU antes de deletar\n",
    "# model.to(\"cpu\")\n",
    "# # del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import gc\n",
    "\n",
    "# # Remover refer√™ncia ao modelo\n",
    "# del model\n",
    "\n",
    "# # Coleta de lixo para liberar objetos √≥rf√£os\n",
    "# gc.collect()\n",
    "\n",
    "# # Liberar mem√≥ria na GPU\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset TeleQnA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Release 17 Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the TeleQnA processed question in JSON file\n",
    "rel17_questions_path = r\"../Files/rel17_questions.json\"\n",
    "\n",
    "# Load the TeleQnA data just release 17\n",
    "with open(rel17_questions_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    rel17_questions = json.load(file)\n",
    "print(len(rel17_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Path to the TeleQnA processed question in JSON file\n",
    "rel17_100_questions_path = r\"../Files/rel17_100_questions.json\"\n",
    "\n",
    "# Load the TeleQnA data just release 17\n",
    "with open(rel17_100_questions_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    rel17_100_questions = json.load(file)\n",
    "print(len(rel17_100_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633\n"
     ]
    }
   ],
   "source": [
    "rel17_other_questions = [q for q in rel17_questions if q not in rel17_100_questions]\n",
    "print(len(rel17_other_questions))\n",
    "\n",
    "rel17_other_questions_length = 500\n",
    "\n",
    "rel17_other_questions = rel17_other_questions[:rel17_other_questions_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Which physical channel informs the UE and the RN about the number of OFDM symbols used for the PDCCHs? [3GPP Release 17]',\n",
       " 'option 1': 'PBCH',\n",
       " 'option 2': 'PCFICH',\n",
       " 'option 3': 'PDSCH',\n",
       " 'option 4': 'PHICH',\n",
       " 'answer': 'option 2: PCFICH',\n",
       " 'explanation': 'The physical control format indicator channel (PCFICH) informs the UE and the RN about the number of OFDM symbols used for the PDCCHs.',\n",
       " 'category': 'Standards specifications'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel17_other_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions without rel 17 and 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8487\n"
     ]
    }
   ],
   "source": [
    "# Path to the TeleQnA processed question in JSON file\n",
    "questions_no_rel_17_18_path = r\"../Files/questions_no_rel_17_18.json\"\n",
    "\n",
    "# Load the TeleQnA data just release 17\n",
    "with open(questions_no_rel_17_18_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    questions_no_rel_17_18 = json.load(file)\n",
    "print(len(questions_no_rel_17_18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "questions_no_rel_17_18_length = 500\n",
    "questions_no_rel_17_18 = questions_no_rel_17_18[:questions_no_rel_17_18_length]\n",
    "print(len(questions_no_rel_17_18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Which non-orthogonal multiple access scheme utilizes the low-complexity message passing algorithm at the receiver for user data detection?',\n",
       " 'option 1': 'NOMA',\n",
       " 'option 2': 'PDMA',\n",
       " 'option 3': 'MUSA',\n",
       " 'option 4': 'MUST',\n",
       " 'option 5': 'SCMA',\n",
       " 'answer': 'option 5: SCMA',\n",
       " 'explanation': 'The SCMA scheme utilizes the low-complexity message passing algorithm at the receiver for user data detection.',\n",
       " 'category': 'Research publications'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_no_rel_17_18[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# train_questions = rel17_other_questions\n",
    "train_questions = rel17_other_questions + questions_no_rel_17_18\n",
    "print(len(train_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Which physical channel informs the UE and the RN about the number of OFDM symbols used for the PDCCHs? [3GPP Release 17]',\n",
       " 'option 1': 'PBCH',\n",
       " 'option 2': 'PCFICH',\n",
       " 'option 3': 'PDSCH',\n",
       " 'option 4': 'PHICH',\n",
       " 'answer': 'option 2: PCFICH',\n",
       " 'explanation': 'The physical control format indicator channel (PCFICH) informs the UE and the RN about the number of OFDM symbols used for the PDCCHs.',\n",
       " 'category': 'Standards specifications'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "{'conversations': [{'from': 'human', 'value': 'Which physical channel informs the UE and the RN about the number of OFDM symbols used for the PDCCHs? [3GPP Release 17]'}, {'from': 'gpt', 'value': 'The physical control format indicator channel (PCFICH) informs the UE and the RN about the number of OFDM symbols used for the PDCCHs.'}]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d2c0e984d64f46bd66d64b39e5c4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Estrutura dos dados para armazenar pares de perguntas e explica√ß√µes\n",
    "data = []\n",
    "\n",
    "# Preencher o dataset com pares (pergunta, explica√ß√£o)\n",
    "for item in train_questions:\n",
    "    # Criar um dicion√°rio para cada par de entrada\n",
    "    pair = [\n",
    "        {'from': 'human', 'value': item['question']},  # Para a pergunta\n",
    "        {'from': 'gpt', 'value': item['explanation']}  # Para a explica√ß√£o\n",
    "    ]\n",
    "    \n",
    "    data.append(pair)  # Adiciona o par ao dataset\n",
    "\n",
    "# Criar o dataset usando Hugging Face\n",
    "# Convertendo a lista de pares em um formato apropriado\n",
    "formatted_data = {'conversations': data}\n",
    "\n",
    "# Transformar os dados em um Dataset\n",
    "dataset = Dataset.from_dict(formatted_data)\n",
    "\n",
    "# Exibir informa√ß√µes sobre o dataset criado\n",
    "print(dataset)\n",
    "\n",
    "# Exibir a primeira entrada do dataset\n",
    "print(dataset[0])\n",
    "\n",
    "# Opcional: Salvar o dataset para reutiliza√ß√£o futura\n",
    "dataset.save_to_disk('../Files/train_questions_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': 'Which physical channel informs the UE and the RN about the number of OFDM symbols used for the PDCCHs? [3GPP Release 17]'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The physical control format indicator channel (PCFICH) informs the UE and the RN about the number of OFDM symbols used for the PDCCHs.'}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TeleQnA Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59feb45967954d778b04d7b5d5e6e41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Standardizing format:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a859885996f44eca81e092727c78021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhich physical channel informs the UE and the RN about the number of OFDM symbols used for the PDCCHs? [3GPP Release 17]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe physical control format indicator channel (PCFICH) informs the UE and the RN about the number of OFDM symbols used for the PDCCHs.<|eot_id|>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.10.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5042fd96aafd42d788f653214dbf9b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimatea/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 300,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827cd3c9219b43d0bd33f6928c567bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhich physical channel informs the UE and the RN about the number of OFDM symbols used for the PDCCHs? [3GPP Release 17]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe physical control format indicator channel (PCFICH) informs the UE and the RN about the number of OFDM symbols used for the PDCCHs.<|eot_id|>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                  \\n\\nThe physical control format indicator channel (PCFICH) informs the UE and the RN about the number of OFDM symbols used for the PDCCHs.<|eot_id|>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show current memory stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3050 Ti Laptop GPU. Max memory = 3.712 GB.\n",
      "2.768 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('input_ids', [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 5887, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 23956, 7106, 5613, 64252, 279, 31088, 323, 279, 46916, 922, 279, 1396, 315, 3083, 8561, 18210, 1511, 369, 279, 393, 5744, 2198, 82, 30, 510, 18, 38, 4505, 17836, 220, 1114, 60, 128009, 128006, 78191, 128007, 271, 791, 7106, 2585, 3645, 21070, 5613, 320, 4977, 37, 33929, 8, 64252, 279, 31088, 323, 279, 46916, 922, 279, 1396, 315, 3083, 8561, 18210, 1511, 369, 279, 393, 5744, 2198, 82, 13, 128009]), ('attention_mask', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('labels', [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 271, 791, 7106, 2585, 3645, 21070, 5613, 320, 4977, 37, 33929, 8, 64252, 279, 31088, 323, 279, 46916, 922, 279, 1396, 315, 3083, 8561, 18210, 1511, 369, 279, 393, 5744, 2198, 82, 13, 128009])])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset[0].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating initial loss: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [02:17<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial mean loss: 2.2772102978229523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configura o collator e DataLoader\n",
    "collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "loader = DataLoader(trainer.train_dataset, \n",
    "                    batch_size=2,  # Tamanho do batch escolhido\n",
    "                    collate_fn=collator, \n",
    "                    num_workers=2)\n",
    "\n",
    "# Vari√°veis para armazenar a loss total e o n√∫mero de exemplos\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "# Coloca o modelo em modo de avalia√ß√£o\n",
    "model.eval()\n",
    "\n",
    "# Desativa o c√°lculo de gradiente para economizar mem√≥ria\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Calculating initial loss\"):\n",
    "        # Move o batch para a GPU (se dispon√≠vel)\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        # Acumula a loss\n",
    "        total_loss += outputs.loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Calcula a loss m√©dia\n",
    "average_loss = total_loss / num_batches\n",
    "print(f\"Initial mean loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 300\n",
      " \"-____-\"     Number of trainable parameters = 24,313,856\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e4df88441a48f2b68c11620066f554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2714, 'grad_norm': 1.6106375455856323, 'learning_rate': 4e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7244, 'grad_norm': 1.9808603525161743, 'learning_rate': 8e-05, 'epoch': 0.02}\n",
      "{'loss': 2.7361, 'grad_norm': 1.5503244400024414, 'learning_rate': 0.00012, 'epoch': 0.02}\n",
      "{'loss': 2.2551, 'grad_norm': 1.911511778831482, 'learning_rate': 0.00016, 'epoch': 0.03}\n",
      "{'loss': 2.4328, 'grad_norm': 1.9331849813461304, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8712, 'grad_norm': 1.5289150476455688, 'learning_rate': 0.0001993220338983051, 'epoch': 0.05}\n",
      "{'loss': 2.2262, 'grad_norm': 0.9005128741264343, 'learning_rate': 0.00019864406779661017, 'epoch': 0.06}\n",
      "{'loss': 1.7621, 'grad_norm': 0.9030927419662476, 'learning_rate': 0.00019796610169491526, 'epoch': 0.06}\n",
      "{'loss': 1.7585, 'grad_norm': 0.8726301193237305, 'learning_rate': 0.00019728813559322035, 'epoch': 0.07}\n",
      "{'loss': 2.0563, 'grad_norm': 0.9981874227523804, 'learning_rate': 0.00019661016949152545, 'epoch': 0.08}\n",
      "{'loss': 1.3275, 'grad_norm': 0.9257978200912476, 'learning_rate': 0.0001959322033898305, 'epoch': 0.09}\n",
      "{'loss': 0.9804, 'grad_norm': 0.7058268189430237, 'learning_rate': 0.0001952542372881356, 'epoch': 0.1}\n",
      "{'loss': 1.3489, 'grad_norm': 0.8421456217765808, 'learning_rate': 0.0001945762711864407, 'epoch': 0.1}\n",
      "{'loss': 1.932, 'grad_norm': 0.8374772071838379, 'learning_rate': 0.0001938983050847458, 'epoch': 0.11}\n",
      "{'loss': 1.6739, 'grad_norm': 0.9490576982498169, 'learning_rate': 0.00019322033898305085, 'epoch': 0.12}\n",
      "{'loss': 1.4674, 'grad_norm': 0.9021246433258057, 'learning_rate': 0.00019254237288135595, 'epoch': 0.13}\n",
      "{'loss': 1.4888, 'grad_norm': 0.9152685403823853, 'learning_rate': 0.000191864406779661, 'epoch': 0.14}\n",
      "{'loss': 1.2274, 'grad_norm': 0.9558353424072266, 'learning_rate': 0.0001911864406779661, 'epoch': 0.14}\n",
      "{'loss': 1.5371, 'grad_norm': 1.0959396362304688, 'learning_rate': 0.0001905084745762712, 'epoch': 0.15}\n",
      "{'loss': 1.5466, 'grad_norm': 0.8095158338546753, 'learning_rate': 0.0001898305084745763, 'epoch': 0.16}\n",
      "{'loss': 1.0089, 'grad_norm': 1.0013095140457153, 'learning_rate': 0.00018915254237288136, 'epoch': 0.17}\n",
      "{'loss': 1.3495, 'grad_norm': 0.8801665306091309, 'learning_rate': 0.00018847457627118645, 'epoch': 0.18}\n",
      "{'loss': 1.4418, 'grad_norm': 1.028415322303772, 'learning_rate': 0.00018779661016949151, 'epoch': 0.18}\n",
      "{'loss': 1.4526, 'grad_norm': 1.121482253074646, 'learning_rate': 0.00018711864406779663, 'epoch': 0.19}\n",
      "{'loss': 1.5776, 'grad_norm': 0.9805183410644531, 'learning_rate': 0.0001864406779661017, 'epoch': 0.2}\n",
      "{'loss': 1.8553, 'grad_norm': 1.1853283643722534, 'learning_rate': 0.0001857627118644068, 'epoch': 0.21}\n",
      "{'loss': 1.7438, 'grad_norm': 0.8680059313774109, 'learning_rate': 0.00018508474576271186, 'epoch': 0.22}\n",
      "{'loss': 1.241, 'grad_norm': 0.8434310555458069, 'learning_rate': 0.00018440677966101695, 'epoch': 0.22}\n",
      "{'loss': 1.8508, 'grad_norm': 0.9591973423957825, 'learning_rate': 0.00018372881355932204, 'epoch': 0.23}\n",
      "{'loss': 1.7883, 'grad_norm': 1.046636939048767, 'learning_rate': 0.00018305084745762714, 'epoch': 0.24}\n",
      "{'loss': 1.6957, 'grad_norm': 1.225361704826355, 'learning_rate': 0.0001823728813559322, 'epoch': 0.25}\n",
      "{'loss': 1.9041, 'grad_norm': 1.3202846050262451, 'learning_rate': 0.0001816949152542373, 'epoch': 0.26}\n",
      "{'loss': 1.5471, 'grad_norm': 0.7697598934173584, 'learning_rate': 0.00018101694915254239, 'epoch': 0.26}\n",
      "{'loss': 1.5953, 'grad_norm': 0.933994472026825, 'learning_rate': 0.00018033898305084748, 'epoch': 0.27}\n",
      "{'loss': 1.6441, 'grad_norm': 0.9306190609931946, 'learning_rate': 0.00017966101694915257, 'epoch': 0.28}\n",
      "{'loss': 1.3782, 'grad_norm': 0.8173523545265198, 'learning_rate': 0.00017898305084745764, 'epoch': 0.29}\n",
      "{'loss': 1.4486, 'grad_norm': 0.9672834873199463, 'learning_rate': 0.00017830508474576273, 'epoch': 0.3}\n",
      "{'loss': 1.531, 'grad_norm': 0.7846447229385376, 'learning_rate': 0.0001776271186440678, 'epoch': 0.3}\n",
      "{'loss': 1.6737, 'grad_norm': 0.7969328761100769, 'learning_rate': 0.0001769491525423729, 'epoch': 0.31}\n",
      "{'loss': 1.5925, 'grad_norm': 1.0558996200561523, 'learning_rate': 0.00017627118644067798, 'epoch': 0.32}\n",
      "{'loss': 1.1357, 'grad_norm': 0.8502400517463684, 'learning_rate': 0.00017559322033898307, 'epoch': 0.33}\n",
      "{'loss': 2.1521, 'grad_norm': 0.9371669888496399, 'learning_rate': 0.00017491525423728814, 'epoch': 0.34}\n",
      "{'loss': 1.235, 'grad_norm': 0.922394871711731, 'learning_rate': 0.00017423728813559323, 'epoch': 0.34}\n",
      "{'loss': 1.2949, 'grad_norm': 0.7402117848396301, 'learning_rate': 0.0001735593220338983, 'epoch': 0.35}\n",
      "{'loss': 1.1458, 'grad_norm': 0.858314573764801, 'learning_rate': 0.00017288135593220342, 'epoch': 0.36}\n",
      "{'loss': 1.4998, 'grad_norm': 0.7905563116073608, 'learning_rate': 0.00017220338983050848, 'epoch': 0.37}\n",
      "{'loss': 1.5999, 'grad_norm': 0.880731999874115, 'learning_rate': 0.00017152542372881357, 'epoch': 0.38}\n",
      "{'loss': 1.7376, 'grad_norm': 1.0839818716049194, 'learning_rate': 0.00017084745762711864, 'epoch': 0.38}\n",
      "{'loss': 1.4504, 'grad_norm': 0.8112084865570068, 'learning_rate': 0.00017016949152542373, 'epoch': 0.39}\n",
      "{'loss': 1.5303, 'grad_norm': 0.8896393179893494, 'learning_rate': 0.00016949152542372882, 'epoch': 0.4}\n",
      "{'loss': 1.7827, 'grad_norm': 0.9502297639846802, 'learning_rate': 0.00016881355932203392, 'epoch': 0.41}\n",
      "{'loss': 2.0265, 'grad_norm': 1.140242576599121, 'learning_rate': 0.00016813559322033898, 'epoch': 0.42}\n",
      "{'loss': 1.5923, 'grad_norm': 1.0263473987579346, 'learning_rate': 0.00016745762711864408, 'epoch': 0.42}\n",
      "{'loss': 1.5797, 'grad_norm': 0.8802300691604614, 'learning_rate': 0.00016677966101694914, 'epoch': 0.43}\n",
      "{'loss': 1.8881, 'grad_norm': 0.9786914587020874, 'learning_rate': 0.00016610169491525423, 'epoch': 0.44}\n",
      "{'loss': 1.7013, 'grad_norm': 0.9943042397499084, 'learning_rate': 0.00016542372881355933, 'epoch': 0.45}\n",
      "{'loss': 1.4216, 'grad_norm': 0.7802377343177795, 'learning_rate': 0.00016474576271186442, 'epoch': 0.46}\n",
      "{'loss': 1.7269, 'grad_norm': 0.9149603843688965, 'learning_rate': 0.00016406779661016948, 'epoch': 0.46}\n",
      "{'loss': 2.0013, 'grad_norm': 0.7391743063926697, 'learning_rate': 0.00016338983050847458, 'epoch': 0.47}\n",
      "{'loss': 1.4691, 'grad_norm': 0.7497445940971375, 'learning_rate': 0.00016271186440677967, 'epoch': 0.48}\n",
      "{'loss': 1.2397, 'grad_norm': 0.9572010636329651, 'learning_rate': 0.00016203389830508476, 'epoch': 0.49}\n",
      "{'loss': 1.5105, 'grad_norm': 0.8301321864128113, 'learning_rate': 0.00016135593220338985, 'epoch': 0.5}\n",
      "{'loss': 1.7342, 'grad_norm': 0.8758363723754883, 'learning_rate': 0.00016067796610169492, 'epoch': 0.5}\n",
      "{'loss': 1.3389, 'grad_norm': 0.946755588054657, 'learning_rate': 0.00016, 'epoch': 0.51}\n",
      "{'loss': 1.8166, 'grad_norm': 0.9913042783737183, 'learning_rate': 0.00015932203389830508, 'epoch': 0.52}\n",
      "{'loss': 1.4744, 'grad_norm': 1.0553890466690063, 'learning_rate': 0.0001586440677966102, 'epoch': 0.53}\n",
      "{'loss': 1.1514, 'grad_norm': 0.918673038482666, 'learning_rate': 0.00015796610169491526, 'epoch': 0.54}\n",
      "{'loss': 1.1658, 'grad_norm': 0.776348888874054, 'learning_rate': 0.00015728813559322036, 'epoch': 0.54}\n",
      "{'loss': 1.5177, 'grad_norm': 0.8255602717399597, 'learning_rate': 0.00015661016949152542, 'epoch': 0.55}\n",
      "{'loss': 1.3972, 'grad_norm': 1.2050896883010864, 'learning_rate': 0.00015593220338983051, 'epoch': 0.56}\n",
      "{'loss': 1.4798, 'grad_norm': 0.8003722429275513, 'learning_rate': 0.0001552542372881356, 'epoch': 0.57}\n",
      "{'loss': 1.4737, 'grad_norm': 1.1021289825439453, 'learning_rate': 0.0001545762711864407, 'epoch': 0.58}\n",
      "{'loss': 1.5236, 'grad_norm': 0.9348180890083313, 'learning_rate': 0.00015389830508474577, 'epoch': 0.58}\n",
      "{'loss': 2.0122, 'grad_norm': 0.9684078693389893, 'learning_rate': 0.00015322033898305086, 'epoch': 0.59}\n",
      "{'loss': 1.7801, 'grad_norm': 0.9739277362823486, 'learning_rate': 0.00015254237288135592, 'epoch': 0.6}\n",
      "{'loss': 1.7703, 'grad_norm': 0.8061067461967468, 'learning_rate': 0.00015186440677966102, 'epoch': 0.61}\n",
      "{'loss': 1.4306, 'grad_norm': 0.8148946762084961, 'learning_rate': 0.0001511864406779661, 'epoch': 0.62}\n",
      "{'loss': 1.5852, 'grad_norm': 0.8548436164855957, 'learning_rate': 0.0001505084745762712, 'epoch': 0.62}\n",
      "{'loss': 1.1754, 'grad_norm': 0.7965044975280762, 'learning_rate': 0.00014983050847457627, 'epoch': 0.63}\n",
      "{'loss': 1.9868, 'grad_norm': 1.0639348030090332, 'learning_rate': 0.00014915254237288136, 'epoch': 0.64}\n",
      "{'loss': 1.307, 'grad_norm': 0.9654049873352051, 'learning_rate': 0.00014847457627118645, 'epoch': 0.65}\n",
      "{'loss': 1.2391, 'grad_norm': 1.055154800415039, 'learning_rate': 0.00014779661016949154, 'epoch': 0.66}\n",
      "{'loss': 1.5035, 'grad_norm': 0.737607479095459, 'learning_rate': 0.0001471186440677966, 'epoch': 0.66}\n",
      "{'loss': 1.8437, 'grad_norm': 1.081926941871643, 'learning_rate': 0.0001464406779661017, 'epoch': 0.67}\n",
      "{'loss': 1.5491, 'grad_norm': 0.7827016711235046, 'learning_rate': 0.00014576271186440677, 'epoch': 0.68}\n",
      "{'loss': 1.18, 'grad_norm': 0.866194486618042, 'learning_rate': 0.00014508474576271186, 'epoch': 0.69}\n",
      "{'loss': 1.5501, 'grad_norm': 0.7850391268730164, 'learning_rate': 0.00014440677966101695, 'epoch': 0.7}\n",
      "{'loss': 1.1585, 'grad_norm': 0.6321282386779785, 'learning_rate': 0.00014372881355932205, 'epoch': 0.7}\n",
      "{'loss': 1.4765, 'grad_norm': 0.9258256554603577, 'learning_rate': 0.00014305084745762714, 'epoch': 0.71}\n",
      "{'loss': 1.8039, 'grad_norm': 1.037558913230896, 'learning_rate': 0.0001423728813559322, 'epoch': 0.72}\n",
      "{'loss': 1.5329, 'grad_norm': 0.9300054907798767, 'learning_rate': 0.0001416949152542373, 'epoch': 0.73}\n",
      "{'loss': 1.4819, 'grad_norm': 0.7929487824440002, 'learning_rate': 0.0001410169491525424, 'epoch': 0.74}\n",
      "{'loss': 1.2492, 'grad_norm': 0.8717362284660339, 'learning_rate': 0.00014033898305084748, 'epoch': 0.74}\n",
      "{'loss': 1.4126, 'grad_norm': 0.8389996290206909, 'learning_rate': 0.00013966101694915255, 'epoch': 0.75}\n",
      "{'loss': 1.8214, 'grad_norm': 1.0533819198608398, 'learning_rate': 0.00013898305084745764, 'epoch': 0.76}\n",
      "{'loss': 2.0461, 'grad_norm': 0.9426549673080444, 'learning_rate': 0.0001383050847457627, 'epoch': 0.77}\n",
      "{'loss': 1.1104, 'grad_norm': 0.769136369228363, 'learning_rate': 0.0001376271186440678, 'epoch': 0.78}\n",
      "{'loss': 1.7071, 'grad_norm': 1.05534029006958, 'learning_rate': 0.0001369491525423729, 'epoch': 0.78}\n",
      "{'loss': 1.7688, 'grad_norm': 0.8809515237808228, 'learning_rate': 0.00013627118644067798, 'epoch': 0.79}\n",
      "{'loss': 1.5267, 'grad_norm': 0.8896448612213135, 'learning_rate': 0.00013559322033898305, 'epoch': 0.8}\n",
      "{'loss': 1.4693, 'grad_norm': 0.8760001063346863, 'learning_rate': 0.00013491525423728814, 'epoch': 0.81}\n",
      "{'loss': 1.255, 'grad_norm': 0.9386512041091919, 'learning_rate': 0.0001342372881355932, 'epoch': 0.82}\n",
      "{'loss': 1.1716, 'grad_norm': 0.6737567782402039, 'learning_rate': 0.00013355932203389833, 'epoch': 0.82}\n",
      "{'loss': 1.7503, 'grad_norm': 0.8574091792106628, 'learning_rate': 0.0001328813559322034, 'epoch': 0.83}\n",
      "{'loss': 1.6144, 'grad_norm': 0.9673986434936523, 'learning_rate': 0.00013220338983050849, 'epoch': 0.84}\n",
      "{'loss': 1.2303, 'grad_norm': 0.7799674272537231, 'learning_rate': 0.00013152542372881355, 'epoch': 0.85}\n",
      "{'loss': 1.9795, 'grad_norm': 1.482923150062561, 'learning_rate': 0.00013084745762711864, 'epoch': 0.86}\n",
      "{'loss': 1.434, 'grad_norm': 0.9852720499038696, 'learning_rate': 0.00013016949152542374, 'epoch': 0.86}\n",
      "{'loss': 1.4477, 'grad_norm': 0.9407005310058594, 'learning_rate': 0.00012949152542372883, 'epoch': 0.87}\n",
      "{'loss': 1.3108, 'grad_norm': 0.8551958799362183, 'learning_rate': 0.0001288135593220339, 'epoch': 0.88}\n",
      "{'loss': 1.5392, 'grad_norm': 1.0088306665420532, 'learning_rate': 0.000128135593220339, 'epoch': 0.89}\n",
      "{'loss': 1.4459, 'grad_norm': 1.1464316844940186, 'learning_rate': 0.00012745762711864405, 'epoch': 0.9}\n",
      "{'loss': 1.3542, 'grad_norm': 0.8406959772109985, 'learning_rate': 0.00012677966101694917, 'epoch': 0.9}\n",
      "{'loss': 1.5628, 'grad_norm': 0.8320776224136353, 'learning_rate': 0.00012610169491525426, 'epoch': 0.91}\n",
      "{'loss': 2.1952, 'grad_norm': 1.0167078971862793, 'learning_rate': 0.00012542372881355933, 'epoch': 0.92}\n",
      "{'loss': 1.3545, 'grad_norm': 0.7328945994377136, 'learning_rate': 0.00012474576271186442, 'epoch': 0.93}\n",
      "{'loss': 1.3452, 'grad_norm': 0.8335226774215698, 'learning_rate': 0.0001240677966101695, 'epoch': 0.94}\n",
      "{'loss': 1.9274, 'grad_norm': 0.9876905679702759, 'learning_rate': 0.00012338983050847458, 'epoch': 0.94}\n",
      "{'loss': 1.9823, 'grad_norm': 0.9999080300331116, 'learning_rate': 0.00012271186440677967, 'epoch': 0.95}\n",
      "{'loss': 1.1052, 'grad_norm': 0.9803486466407776, 'learning_rate': 0.00012203389830508477, 'epoch': 0.96}\n",
      "{'loss': 1.4613, 'grad_norm': 1.037550926208496, 'learning_rate': 0.00012135593220338983, 'epoch': 0.97}\n",
      "{'loss': 1.7153, 'grad_norm': 0.8493233919143677, 'learning_rate': 0.00012067796610169492, 'epoch': 0.98}\n",
      "{'loss': 1.9196, 'grad_norm': 0.8539213538169861, 'learning_rate': 0.00012, 'epoch': 0.98}\n",
      "{'loss': 1.455, 'grad_norm': 1.0253887176513672, 'learning_rate': 0.0001193220338983051, 'epoch': 0.99}\n",
      "{'loss': 0.9126, 'grad_norm': 0.7905411124229431, 'learning_rate': 0.00011864406779661017, 'epoch': 1.0}\n",
      "{'loss': 1.367, 'grad_norm': 0.9749678373336792, 'learning_rate': 0.00011796610169491527, 'epoch': 1.01}\n",
      "{'loss': 1.3999, 'grad_norm': 0.6988453269004822, 'learning_rate': 0.00011728813559322033, 'epoch': 1.02}\n",
      "{'loss': 1.4427, 'grad_norm': 0.9182418584823608, 'learning_rate': 0.00011661016949152544, 'epoch': 1.02}\n",
      "{'loss': 0.8508, 'grad_norm': 0.8632371425628662, 'learning_rate': 0.0001159322033898305, 'epoch': 1.03}\n",
      "{'loss': 1.468, 'grad_norm': 0.8229799270629883, 'learning_rate': 0.0001152542372881356, 'epoch': 1.04}\n",
      "{'loss': 1.5204, 'grad_norm': 0.9632782340049744, 'learning_rate': 0.00011457627118644068, 'epoch': 1.05}\n",
      "{'loss': 1.328, 'grad_norm': 0.7916066646575928, 'learning_rate': 0.00011389830508474577, 'epoch': 1.06}\n",
      "{'loss': 1.1021, 'grad_norm': 0.8636873364448547, 'learning_rate': 0.00011322033898305085, 'epoch': 1.06}\n",
      "{'loss': 1.3209, 'grad_norm': 0.8152046799659729, 'learning_rate': 0.00011254237288135594, 'epoch': 1.07}\n",
      "{'loss': 1.2596, 'grad_norm': 0.8921478986740112, 'learning_rate': 0.00011186440677966102, 'epoch': 1.08}\n",
      "{'loss': 1.5699, 'grad_norm': 0.8939640522003174, 'learning_rate': 0.00011118644067796611, 'epoch': 1.09}\n",
      "{'loss': 1.3594, 'grad_norm': 1.0619823932647705, 'learning_rate': 0.00011050847457627118, 'epoch': 1.1}\n",
      "{'loss': 1.2468, 'grad_norm': 1.039967656135559, 'learning_rate': 0.00010983050847457627, 'epoch': 1.1}\n",
      "{'loss': 1.4662, 'grad_norm': 1.097264051437378, 'learning_rate': 0.00010915254237288135, 'epoch': 1.11}\n",
      "{'loss': 1.3336, 'grad_norm': 0.9763365983963013, 'learning_rate': 0.00010847457627118644, 'epoch': 1.12}\n",
      "{'loss': 1.0954, 'grad_norm': 0.9436061382293701, 'learning_rate': 0.00010779661016949153, 'epoch': 1.13}\n",
      "{'loss': 1.6767, 'grad_norm': 1.0987873077392578, 'learning_rate': 0.00010711864406779661, 'epoch': 1.14}\n",
      "{'loss': 1.2511, 'grad_norm': 1.1282151937484741, 'learning_rate': 0.0001064406779661017, 'epoch': 1.14}\n",
      "{'loss': 1.1135, 'grad_norm': 1.1000876426696777, 'learning_rate': 0.00010576271186440679, 'epoch': 1.15}\n",
      "{'loss': 1.0104, 'grad_norm': 1.0238397121429443, 'learning_rate': 0.00010508474576271188, 'epoch': 1.16}\n",
      "{'loss': 1.5412, 'grad_norm': 1.1322201490402222, 'learning_rate': 0.00010440677966101696, 'epoch': 1.17}\n",
      "{'loss': 1.2444, 'grad_norm': 1.198483943939209, 'learning_rate': 0.00010372881355932205, 'epoch': 1.18}\n",
      "{'loss': 1.4624, 'grad_norm': 1.4379099607467651, 'learning_rate': 0.00010305084745762712, 'epoch': 1.18}\n",
      "{'loss': 1.2478, 'grad_norm': 1.090644121170044, 'learning_rate': 0.00010237288135593222, 'epoch': 1.19}\n",
      "{'loss': 0.9265, 'grad_norm': 1.0392913818359375, 'learning_rate': 0.00010169491525423729, 'epoch': 1.2}\n",
      "{'loss': 1.1383, 'grad_norm': 1.1010851860046387, 'learning_rate': 0.00010101694915254238, 'epoch': 1.21}\n",
      "{'loss': 1.3621, 'grad_norm': 1.040676474571228, 'learning_rate': 0.00010033898305084746, 'epoch': 1.22}\n",
      "{'loss': 1.4862, 'grad_norm': 1.3732011318206787, 'learning_rate': 9.966101694915255e-05, 'epoch': 1.22}\n",
      "{'loss': 1.7042, 'grad_norm': 1.1461024284362793, 'learning_rate': 9.898305084745763e-05, 'epoch': 1.23}\n",
      "{'loss': 1.2014, 'grad_norm': 1.3142528533935547, 'learning_rate': 9.830508474576272e-05, 'epoch': 1.24}\n",
      "{'loss': 0.7174, 'grad_norm': 1.209840178489685, 'learning_rate': 9.76271186440678e-05, 'epoch': 1.25}\n",
      "{'loss': 1.4362, 'grad_norm': 1.1083582639694214, 'learning_rate': 9.69491525423729e-05, 'epoch': 1.26}\n",
      "{'loss': 1.1649, 'grad_norm': 1.0933632850646973, 'learning_rate': 9.627118644067797e-05, 'epoch': 1.26}\n",
      "{'loss': 1.0342, 'grad_norm': 1.2215474843978882, 'learning_rate': 9.559322033898305e-05, 'epoch': 1.27}\n",
      "{'loss': 1.1975, 'grad_norm': 1.2638568878173828, 'learning_rate': 9.491525423728815e-05, 'epoch': 1.28}\n",
      "{'loss': 1.3132, 'grad_norm': 1.5259548425674438, 'learning_rate': 9.423728813559322e-05, 'epoch': 1.29}\n",
      "{'loss': 1.5862, 'grad_norm': 1.2769147157669067, 'learning_rate': 9.355932203389832e-05, 'epoch': 1.3}\n",
      "{'loss': 1.4284, 'grad_norm': 1.3069474697113037, 'learning_rate': 9.28813559322034e-05, 'epoch': 1.3}\n",
      "{'loss': 1.6259, 'grad_norm': 1.2991653680801392, 'learning_rate': 9.220338983050847e-05, 'epoch': 1.31}\n",
      "{'loss': 1.2513, 'grad_norm': 1.2668266296386719, 'learning_rate': 9.152542372881357e-05, 'epoch': 1.32}\n",
      "{'loss': 1.5889, 'grad_norm': 1.4460984468460083, 'learning_rate': 9.084745762711865e-05, 'epoch': 1.33}\n",
      "{'loss': 1.4797, 'grad_norm': 1.5618098974227905, 'learning_rate': 9.016949152542374e-05, 'epoch': 1.34}\n",
      "{'loss': 1.5348, 'grad_norm': 1.814680576324463, 'learning_rate': 8.949152542372882e-05, 'epoch': 1.34}\n",
      "{'loss': 0.8169, 'grad_norm': 1.1733258962631226, 'learning_rate': 8.88135593220339e-05, 'epoch': 1.35}\n",
      "{'loss': 1.6396, 'grad_norm': 1.823862910270691, 'learning_rate': 8.813559322033899e-05, 'epoch': 1.36}\n",
      "{'loss': 1.3837, 'grad_norm': 1.9289110898971558, 'learning_rate': 8.745762711864407e-05, 'epoch': 1.37}\n",
      "{'loss': 1.1086, 'grad_norm': 1.272768497467041, 'learning_rate': 8.677966101694915e-05, 'epoch': 1.38}\n",
      "{'loss': 1.0683, 'grad_norm': 1.1311992406845093, 'learning_rate': 8.610169491525424e-05, 'epoch': 1.38}\n",
      "{'loss': 1.0617, 'grad_norm': 1.3855431079864502, 'learning_rate': 8.542372881355932e-05, 'epoch': 1.39}\n",
      "{'loss': 1.0009, 'grad_norm': 1.112266182899475, 'learning_rate': 8.474576271186441e-05, 'epoch': 1.4}\n",
      "{'loss': 1.4282, 'grad_norm': 1.3659093379974365, 'learning_rate': 8.406779661016949e-05, 'epoch': 1.41}\n",
      "{'loss': 1.2043, 'grad_norm': 1.3097859621047974, 'learning_rate': 8.338983050847457e-05, 'epoch': 1.42}\n",
      "{'loss': 1.5414, 'grad_norm': 1.449746012687683, 'learning_rate': 8.271186440677966e-05, 'epoch': 1.42}\n",
      "{'loss': 1.2093, 'grad_norm': 1.4596872329711914, 'learning_rate': 8.203389830508474e-05, 'epoch': 1.43}\n",
      "{'loss': 1.4399, 'grad_norm': 1.5172220468521118, 'learning_rate': 8.135593220338983e-05, 'epoch': 1.44}\n",
      "{'loss': 1.4322, 'grad_norm': 1.3011935949325562, 'learning_rate': 8.067796610169493e-05, 'epoch': 1.45}\n",
      "{'loss': 1.6313, 'grad_norm': 1.4637683629989624, 'learning_rate': 8e-05, 'epoch': 1.46}\n",
      "{'loss': 1.9118, 'grad_norm': 1.6741461753845215, 'learning_rate': 7.93220338983051e-05, 'epoch': 1.46}\n",
      "{'loss': 1.4451, 'grad_norm': 1.5757449865341187, 'learning_rate': 7.864406779661018e-05, 'epoch': 1.47}\n",
      "{'loss': 0.8783, 'grad_norm': 1.0834635496139526, 'learning_rate': 7.796610169491526e-05, 'epoch': 1.48}\n",
      "{'loss': 1.4236, 'grad_norm': 1.2021197080612183, 'learning_rate': 7.728813559322035e-05, 'epoch': 1.49}\n",
      "{'loss': 1.4699, 'grad_norm': 1.3722227811813354, 'learning_rate': 7.661016949152543e-05, 'epoch': 1.5}\n",
      "{'loss': 1.3709, 'grad_norm': 1.348251223564148, 'learning_rate': 7.593220338983051e-05, 'epoch': 1.5}\n",
      "{'loss': 0.9617, 'grad_norm': 1.2403806447982788, 'learning_rate': 7.52542372881356e-05, 'epoch': 1.51}\n",
      "{'loss': 1.3058, 'grad_norm': 1.5304477214813232, 'learning_rate': 7.457627118644068e-05, 'epoch': 1.52}\n",
      "{'loss': 1.1576, 'grad_norm': 1.4213664531707764, 'learning_rate': 7.389830508474577e-05, 'epoch': 1.53}\n",
      "{'loss': 1.3184, 'grad_norm': 1.5391027927398682, 'learning_rate': 7.322033898305085e-05, 'epoch': 1.54}\n",
      "{'loss': 0.9381, 'grad_norm': 1.3493571281433105, 'learning_rate': 7.254237288135593e-05, 'epoch': 1.54}\n",
      "{'loss': 1.1126, 'grad_norm': 1.1653602123260498, 'learning_rate': 7.186440677966102e-05, 'epoch': 1.55}\n",
      "{'loss': 1.0661, 'grad_norm': 1.3546686172485352, 'learning_rate': 7.11864406779661e-05, 'epoch': 1.56}\n",
      "{'loss': 1.2729, 'grad_norm': 1.497628927230835, 'learning_rate': 7.05084745762712e-05, 'epoch': 1.57}\n",
      "{'loss': 1.3166, 'grad_norm': 1.6061534881591797, 'learning_rate': 6.983050847457627e-05, 'epoch': 1.58}\n",
      "{'loss': 1.079, 'grad_norm': 1.6730440855026245, 'learning_rate': 6.915254237288135e-05, 'epoch': 1.58}\n",
      "{'loss': 0.8202, 'grad_norm': 1.579337239265442, 'learning_rate': 6.847457627118645e-05, 'epoch': 1.59}\n",
      "{'loss': 1.0984, 'grad_norm': 1.4662604331970215, 'learning_rate': 6.779661016949152e-05, 'epoch': 1.6}\n",
      "{'loss': 1.2341, 'grad_norm': 1.2078368663787842, 'learning_rate': 6.71186440677966e-05, 'epoch': 1.61}\n",
      "{'loss': 0.8729, 'grad_norm': 1.3794220685958862, 'learning_rate': 6.64406779661017e-05, 'epoch': 1.62}\n",
      "{'loss': 0.9005, 'grad_norm': 1.2723037004470825, 'learning_rate': 6.576271186440678e-05, 'epoch': 1.62}\n",
      "{'loss': 0.9654, 'grad_norm': 1.4340308904647827, 'learning_rate': 6.508474576271187e-05, 'epoch': 1.63}\n",
      "{'loss': 1.0357, 'grad_norm': 1.3395947217941284, 'learning_rate': 6.440677966101695e-05, 'epoch': 1.64}\n",
      "{'loss': 1.0522, 'grad_norm': 1.4104552268981934, 'learning_rate': 6.372881355932203e-05, 'epoch': 1.65}\n",
      "{'loss': 1.241, 'grad_norm': 1.3921483755111694, 'learning_rate': 6.305084745762713e-05, 'epoch': 1.66}\n",
      "{'loss': 0.8863, 'grad_norm': 1.485031008720398, 'learning_rate': 6.237288135593221e-05, 'epoch': 1.66}\n",
      "{'loss': 0.9431, 'grad_norm': 1.3653011322021484, 'learning_rate': 6.169491525423729e-05, 'epoch': 1.67}\n",
      "{'loss': 1.3358, 'grad_norm': 1.3176833391189575, 'learning_rate': 6.101694915254238e-05, 'epoch': 1.68}\n",
      "{'loss': 0.8255, 'grad_norm': 1.120706558227539, 'learning_rate': 6.033898305084746e-05, 'epoch': 1.69}\n",
      "{'loss': 1.0571, 'grad_norm': 1.4554377794265747, 'learning_rate': 5.966101694915255e-05, 'epoch': 1.7}\n",
      "{'loss': 1.5727, 'grad_norm': 1.5911868810653687, 'learning_rate': 5.8983050847457634e-05, 'epoch': 1.7}\n",
      "{'loss': 1.414, 'grad_norm': 1.4930588006973267, 'learning_rate': 5.830508474576272e-05, 'epoch': 1.71}\n",
      "{'loss': 1.1016, 'grad_norm': 1.440727949142456, 'learning_rate': 5.76271186440678e-05, 'epoch': 1.72}\n",
      "{'loss': 1.228, 'grad_norm': 1.6269129514694214, 'learning_rate': 5.6949152542372884e-05, 'epoch': 1.73}\n",
      "{'loss': 0.6708, 'grad_norm': 1.3461189270019531, 'learning_rate': 5.627118644067797e-05, 'epoch': 1.74}\n",
      "{'loss': 1.1049, 'grad_norm': 1.3805351257324219, 'learning_rate': 5.5593220338983056e-05, 'epoch': 1.74}\n",
      "{'loss': 1.2766, 'grad_norm': 1.7782272100448608, 'learning_rate': 5.4915254237288135e-05, 'epoch': 1.75}\n",
      "{'loss': 1.0814, 'grad_norm': 1.7207728624343872, 'learning_rate': 5.423728813559322e-05, 'epoch': 1.76}\n",
      "{'loss': 1.4117, 'grad_norm': 1.522964358329773, 'learning_rate': 5.355932203389831e-05, 'epoch': 1.77}\n",
      "{'loss': 1.2186, 'grad_norm': 1.3202263116836548, 'learning_rate': 5.288135593220339e-05, 'epoch': 1.78}\n",
      "{'loss': 1.1565, 'grad_norm': 1.3563847541809082, 'learning_rate': 5.220338983050848e-05, 'epoch': 1.78}\n",
      "{'loss': 0.9566, 'grad_norm': 1.3545503616333008, 'learning_rate': 5.152542372881356e-05, 'epoch': 1.79}\n",
      "{'loss': 1.2956, 'grad_norm': 1.512083888053894, 'learning_rate': 5.0847457627118643e-05, 'epoch': 1.8}\n",
      "{'loss': 1.2174, 'grad_norm': 1.5350672006607056, 'learning_rate': 5.016949152542373e-05, 'epoch': 1.81}\n",
      "{'loss': 1.5423, 'grad_norm': 1.6135767698287964, 'learning_rate': 4.9491525423728815e-05, 'epoch': 1.82}\n",
      "{'loss': 0.9142, 'grad_norm': 1.2207740545272827, 'learning_rate': 4.88135593220339e-05, 'epoch': 1.82}\n",
      "{'loss': 1.3507, 'grad_norm': 1.460485577583313, 'learning_rate': 4.813559322033899e-05, 'epoch': 1.83}\n",
      "{'loss': 1.5029, 'grad_norm': 1.7923144102096558, 'learning_rate': 4.745762711864407e-05, 'epoch': 1.84}\n",
      "{'loss': 1.2573, 'grad_norm': 1.3379323482513428, 'learning_rate': 4.677966101694916e-05, 'epoch': 1.85}\n",
      "{'loss': 1.2368, 'grad_norm': 1.469694972038269, 'learning_rate': 4.610169491525424e-05, 'epoch': 1.86}\n",
      "{'loss': 1.3684, 'grad_norm': 1.5394262075424194, 'learning_rate': 4.542372881355932e-05, 'epoch': 1.86}\n",
      "{'loss': 1.1494, 'grad_norm': 1.7943748235702515, 'learning_rate': 4.474576271186441e-05, 'epoch': 1.87}\n",
      "{'loss': 1.3281, 'grad_norm': 1.260443925857544, 'learning_rate': 4.4067796610169495e-05, 'epoch': 1.88}\n",
      "{'loss': 0.5091, 'grad_norm': 1.169546365737915, 'learning_rate': 4.3389830508474574e-05, 'epoch': 1.89}\n",
      "{'loss': 1.5051, 'grad_norm': 1.337491750717163, 'learning_rate': 4.271186440677966e-05, 'epoch': 1.9}\n",
      "{'loss': 1.0304, 'grad_norm': 1.661220669746399, 'learning_rate': 4.2033898305084746e-05, 'epoch': 1.9}\n",
      "{'loss': 1.3837, 'grad_norm': 2.034637689590454, 'learning_rate': 4.135593220338983e-05, 'epoch': 1.91}\n",
      "{'loss': 0.9911, 'grad_norm': 1.4352614879608154, 'learning_rate': 4.067796610169492e-05, 'epoch': 1.92}\n",
      "{'loss': 1.3726, 'grad_norm': 1.4573091268539429, 'learning_rate': 4e-05, 'epoch': 1.93}\n",
      "{'loss': 1.4303, 'grad_norm': 1.3900632858276367, 'learning_rate': 3.932203389830509e-05, 'epoch': 1.94}\n",
      "{'loss': 1.0537, 'grad_norm': 1.3344241380691528, 'learning_rate': 3.8644067796610175e-05, 'epoch': 1.94}\n",
      "{'loss': 1.0525, 'grad_norm': 1.619892954826355, 'learning_rate': 3.7966101694915254e-05, 'epoch': 1.95}\n",
      "{'loss': 1.047, 'grad_norm': 1.4634268283843994, 'learning_rate': 3.728813559322034e-05, 'epoch': 1.96}\n",
      "{'loss': 1.1131, 'grad_norm': 1.2603552341461182, 'learning_rate': 3.6610169491525426e-05, 'epoch': 1.97}\n",
      "{'loss': 1.0578, 'grad_norm': 1.2943940162658691, 'learning_rate': 3.593220338983051e-05, 'epoch': 1.98}\n",
      "{'loss': 1.0878, 'grad_norm': 1.5254689455032349, 'learning_rate': 3.52542372881356e-05, 'epoch': 1.98}\n",
      "{'loss': 1.6046, 'grad_norm': 1.5455644130706787, 'learning_rate': 3.4576271186440676e-05, 'epoch': 1.99}\n",
      "{'loss': 1.3983, 'grad_norm': 1.4871834516525269, 'learning_rate': 3.389830508474576e-05, 'epoch': 2.0}\n",
      "{'loss': 0.9782, 'grad_norm': 1.5038529634475708, 'learning_rate': 3.322033898305085e-05, 'epoch': 2.01}\n",
      "{'loss': 1.0096, 'grad_norm': 1.312536358833313, 'learning_rate': 3.2542372881355934e-05, 'epoch': 2.02}\n",
      "{'loss': 1.2062, 'grad_norm': 1.3531494140625, 'learning_rate': 3.186440677966101e-05, 'epoch': 2.02}\n",
      "{'loss': 1.0295, 'grad_norm': 1.2077211141586304, 'learning_rate': 3.1186440677966106e-05, 'epoch': 2.03}\n",
      "{'loss': 1.0439, 'grad_norm': 1.5790205001831055, 'learning_rate': 3.050847457627119e-05, 'epoch': 2.04}\n",
      "{'loss': 0.7012, 'grad_norm': 1.0127264261245728, 'learning_rate': 2.9830508474576274e-05, 'epoch': 2.05}\n",
      "{'loss': 0.5312, 'grad_norm': 1.148388147354126, 'learning_rate': 2.915254237288136e-05, 'epoch': 2.06}\n",
      "{'loss': 1.0841, 'grad_norm': 1.4107211828231812, 'learning_rate': 2.8474576271186442e-05, 'epoch': 2.06}\n",
      "{'loss': 0.9337, 'grad_norm': 1.1646870374679565, 'learning_rate': 2.7796610169491528e-05, 'epoch': 2.07}\n",
      "{'loss': 0.844, 'grad_norm': 1.1770199537277222, 'learning_rate': 2.711864406779661e-05, 'epoch': 2.08}\n",
      "{'loss': 0.5273, 'grad_norm': 1.1770840883255005, 'learning_rate': 2.6440677966101696e-05, 'epoch': 2.09}\n",
      "{'loss': 1.029, 'grad_norm': 1.611702799797058, 'learning_rate': 2.576271186440678e-05, 'epoch': 2.1}\n",
      "{'loss': 1.1248, 'grad_norm': 1.6027580499649048, 'learning_rate': 2.5084745762711865e-05, 'epoch': 2.1}\n",
      "{'loss': 0.9858, 'grad_norm': 1.6365426778793335, 'learning_rate': 2.440677966101695e-05, 'epoch': 2.11}\n",
      "{'loss': 0.7251, 'grad_norm': 1.1940827369689941, 'learning_rate': 2.3728813559322036e-05, 'epoch': 2.12}\n",
      "{'loss': 1.2281, 'grad_norm': 1.4978107213974, 'learning_rate': 2.305084745762712e-05, 'epoch': 2.13}\n",
      "{'loss': 0.8448, 'grad_norm': 1.3789520263671875, 'learning_rate': 2.2372881355932205e-05, 'epoch': 2.14}\n",
      "{'loss': 0.7036, 'grad_norm': 1.2937158346176147, 'learning_rate': 2.1694915254237287e-05, 'epoch': 2.14}\n",
      "{'loss': 0.6909, 'grad_norm': 1.4137576818466187, 'learning_rate': 2.1016949152542373e-05, 'epoch': 2.15}\n",
      "{'loss': 1.4121, 'grad_norm': 1.687817096710205, 'learning_rate': 2.033898305084746e-05, 'epoch': 2.16}\n",
      "{'loss': 0.8375, 'grad_norm': 1.6237659454345703, 'learning_rate': 1.9661016949152545e-05, 'epoch': 2.17}\n",
      "{'loss': 1.1751, 'grad_norm': 1.5536137819290161, 'learning_rate': 1.8983050847457627e-05, 'epoch': 2.18}\n",
      "{'loss': 0.9787, 'grad_norm': 1.6521198749542236, 'learning_rate': 1.8305084745762713e-05, 'epoch': 2.18}\n",
      "{'loss': 0.8918, 'grad_norm': 1.4723339080810547, 'learning_rate': 1.76271186440678e-05, 'epoch': 2.19}\n",
      "{'loss': 0.9165, 'grad_norm': 1.6614420413970947, 'learning_rate': 1.694915254237288e-05, 'epoch': 2.2}\n",
      "{'loss': 0.9092, 'grad_norm': 1.6922342777252197, 'learning_rate': 1.6271186440677967e-05, 'epoch': 2.21}\n",
      "{'loss': 0.6947, 'grad_norm': 1.5726494789123535, 'learning_rate': 1.5593220338983053e-05, 'epoch': 2.22}\n",
      "{'loss': 0.8528, 'grad_norm': 2.0240652561187744, 'learning_rate': 1.4915254237288137e-05, 'epoch': 2.22}\n",
      "{'loss': 0.8251, 'grad_norm': 1.7034975290298462, 'learning_rate': 1.4237288135593221e-05, 'epoch': 2.23}\n",
      "{'loss': 0.7287, 'grad_norm': 1.5527937412261963, 'learning_rate': 1.3559322033898305e-05, 'epoch': 2.24}\n",
      "{'loss': 0.6858, 'grad_norm': 1.4272499084472656, 'learning_rate': 1.288135593220339e-05, 'epoch': 2.25}\n",
      "{'loss': 1.4549, 'grad_norm': 2.3250112533569336, 'learning_rate': 1.2203389830508475e-05, 'epoch': 2.26}\n",
      "{'loss': 0.9297, 'grad_norm': 1.7096129655838013, 'learning_rate': 1.152542372881356e-05, 'epoch': 2.26}\n",
      "{'loss': 1.0014, 'grad_norm': 1.570917010307312, 'learning_rate': 1.0847457627118644e-05, 'epoch': 2.27}\n",
      "{'loss': 0.8778, 'grad_norm': 2.1060755252838135, 'learning_rate': 1.016949152542373e-05, 'epoch': 2.28}\n",
      "{'loss': 0.9111, 'grad_norm': 1.4649488925933838, 'learning_rate': 9.491525423728814e-06, 'epoch': 2.29}\n",
      "{'loss': 0.7445, 'grad_norm': 1.5887492895126343, 'learning_rate': 8.8135593220339e-06, 'epoch': 2.3}\n",
      "{'loss': 1.1057, 'grad_norm': 2.1804747581481934, 'learning_rate': 8.135593220338983e-06, 'epoch': 2.3}\n",
      "{'loss': 1.4964, 'grad_norm': 1.9596027135849, 'learning_rate': 7.4576271186440685e-06, 'epoch': 2.31}\n",
      "{'loss': 0.9491, 'grad_norm': 1.5228140354156494, 'learning_rate': 6.779661016949153e-06, 'epoch': 2.32}\n",
      "{'loss': 0.7548, 'grad_norm': 1.4822771549224854, 'learning_rate': 6.101694915254238e-06, 'epoch': 2.33}\n",
      "{'loss': 1.1776, 'grad_norm': 2.046403169631958, 'learning_rate': 5.423728813559322e-06, 'epoch': 2.34}\n",
      "{'loss': 1.0578, 'grad_norm': 2.086979627609253, 'learning_rate': 4.745762711864407e-06, 'epoch': 2.34}\n",
      "{'loss': 1.1549, 'grad_norm': 1.4615048170089722, 'learning_rate': 4.067796610169492e-06, 'epoch': 2.35}\n",
      "{'loss': 0.9857, 'grad_norm': 1.688761830329895, 'learning_rate': 3.3898305084745763e-06, 'epoch': 2.36}\n",
      "{'loss': 0.9146, 'grad_norm': 1.891719102859497, 'learning_rate': 2.711864406779661e-06, 'epoch': 2.37}\n",
      "{'loss': 0.7649, 'grad_norm': 1.4787254333496094, 'learning_rate': 2.033898305084746e-06, 'epoch': 2.38}\n",
      "{'loss': 0.7881, 'grad_norm': 1.7899492979049683, 'learning_rate': 1.3559322033898304e-06, 'epoch': 2.38}\n",
      "{'loss': 0.994, 'grad_norm': 2.170947551727295, 'learning_rate': 6.779661016949152e-07, 'epoch': 2.39}\n",
      "{'loss': 0.9879, 'grad_norm': 1.6584793329238892, 'learning_rate': 0.0, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimatea/.local/lib/python3.10/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f8a5c511-c05c-40d9-9845-ddd73e13fcdd)') - silently ignoring the lookup for the file config.json in unsloth/llama-3.2-3b-instruct-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/arimatea/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in unsloth/llama-3.2-3b-instruct-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1371.8185, 'train_samples_per_second': 1.75, 'train_steps_per_second': 0.219, 'train_loss': 1.3332603001594543, 'epoch': 2.4}\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show final memory and time stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1371.8185 seconds used for training.\n",
      "22.86 minutes used for training.\n",
      "Peak reserved memory = 3.549 GB.\n",
      "Peak reserved memory for training = 0.781 GB.\n",
      "Peak reserved memory % of max memory = 95.609 %.\n",
      "Peak reserved memory for training % of max memory = 21.04 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Pos training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss post-training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [01:40<00:00,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid batches: 496\n",
      "Post-training mean loss: 0.8747391153667723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure the collator and DataLoader\n",
    "collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "loader = DataLoader(\n",
    "    trainer.train_dataset, \n",
    "    batch_size=2,  # Chosen batch size\n",
    "    collate_fn=collator, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Variables to store the total loss and the number of valid batches\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculation to save memory\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Calculating loss post-training\"):\n",
    "        # Move the batch to GPU (if available)\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # Check if the loss is NaN\n",
    "        if not torch.isnan(outputs.loss):\n",
    "            total_loss += outputs.loss.item()\n",
    "            num_batches += 1\n",
    "            # print(outputs.loss.item())\n",
    "\n",
    "print(f\"Number of valid batches: {num_batches}\")\n",
    "\n",
    "# Calculate the average loss\n",
    "if num_batches > 0:\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f\"Post-training mean loss: {average_loss}\")\n",
    "else:\n",
    "    print(\"No valid batches found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which physical channel informs the UE and the RN about the number of OFDM symbols used for the PDCCHs? [3GPP Release 17]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = train_questions[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhich physical channel informs the UE and the RN about the number of OFDM symbols used for the PDCCHs? [3GPP Release 17]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe physical control format indicator channel (PCFICH) informs the UE and the RN about the number of OFDM symbols used for the PDCCHs.<|eot_id|>']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"user\", \"content\": \"How much is 1+1?\"},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Models/llama_3.2_FT_lora_1000_questions/tokenizer_config.json',\n",
       " '../Models/llama_3.2_FT_lora_1000_questions/special_tokens_map.json',\n",
       " '../Models/llama_3.2_FT_lora_1000_questions/tokenizer.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"../Models/llama_3.2_FT_lora_1000_questions\", safe_serialization=False)\n",
    "tokenizer.save_pretrained(\"../Models/llama_3.2_FT_lora_1000_questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 1.09 out of 15.31 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]We will save to Disk and not RAM now.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:07<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# model.save_pretrained_merged(\"model_3.2_lora_4bits\", tokenizer, save_method = \"merged_16bit\",)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_path = \"../Models/llama_3.2_FT_lora_1000_questions\"\n",
    "# model_path = \"model_3.2_lora_4bits\"\n",
    "\n",
    "# # Carregar o modelo e o tokenizador separadamente\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name=model_path,\n",
    "#     max_seq_length=2048,\n",
    "#     load_in_4bit=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# tokenizer = get_chat_template(\n",
    "#     tokenizer,\n",
    "#     chat_template = \"llama-3.1\",\n",
    "# )\n",
    "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": question},\n",
    "# ]\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize = True,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     return_tensors = \"pt\",\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "#                          temperature = 1.5, min_p = 0.1)\n",
    "# tokenizer.batch_decode(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
